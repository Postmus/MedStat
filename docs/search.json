[
  {
    "objectID": "SPSS_lab8.html",
    "href": "SPSS_lab8.html",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "",
    "text": "In part 1 of the lab, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav on Brightspace). The outcome of interest for today’s analysis is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\nGo to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the gender variable as the Row(s) and the stat variable as the Column(s). Click on Cells and under percentages check the box Row. Click op Continue and OK to create the table.\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. To perform this analysis in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender variable to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To perform this test in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the age variables to the Block 1 of 1 box. Press Next to create a second block. Move the gender variable to the Block 2 of 2 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\nThe output will include a table with the results of the likelihood ratio test comparing the full model (variables included in block 1 and 2) to the reduced model (variables included in block 1 only). This table can be found in the section Block 2: Method = Enter and is labeled Omnibus tests of model coefficients. The p-value for the LRT comparing the full model to the reduced model can be found in the row block.\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow to the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In SPSS, this can be achieved by checking the option Hosmer-Lemeshow goodness-of-fit in the logistic regression dialog box.\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box Hosmer-Lemeshow goodness-of-fit to include the results of the Hosmer-Lemeshow test in the output produced. Press Continue to return to the main dialog and click on OK to perform the analysis.\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab8.html#part-1-risk-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "href": "SPSS_lab8.html#part-1-risk-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "",
    "text": "In part 1 of the lab, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav on Brightspace). The outcome of interest for today’s analysis is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\nGo to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the gender variable as the Row(s) and the stat variable as the Column(s). Click on Cells and under percentages check the box Row. Click op Continue and OK to create the table.\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. To perform this analysis in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender variable to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To perform this test in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the age variables to the Block 1 of 1 box. Press Next to create a second block. Move the gender variable to the Block 2 of 2 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\nThe output will include a table with the results of the likelihood ratio test comparing the full model (variables included in block 1 and 2) to the reduced model (variables included in block 1 only). This table can be found in the section Block 2: Method = Enter and is labeled Omnibus tests of model coefficients. The p-value for the LRT comparing the full model to the reduced model can be found in the row block.\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow to the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In SPSS, this can be achieved by checking the option Hosmer-Lemeshow goodness-of-fit in the logistic regression dialog box.\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box Hosmer-Lemeshow goodness-of-fit to include the results of the Hosmer-Lemeshow test in the output produced. Press Continue to return to the main dialog and click on OK to perform the analysis.\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab8.html#part-2-unguided-exercises",
    "href": "SPSS_lab8.html#part-2-unguided-exercises",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "Part 2: unguided exercises",
    "text": "Part 2: unguided exercises\n\nExercise 1\nMultiple logistic regression was used to construct a prognostic index to predict coronary artery disease from data on 348 patients with valvular heart disease who had undergone routine coronary arteriography before valve replacement (Ramsdale et al. 1982). The estimated equation was:\n\\[logit(p) = ln(p/(1-p)) = b_{0} + 1.167 \\times x{1} + 0.0106 \\times x_{2} + \\textrm{other terms}\\]\nwhere \\(x_{1}\\) stands for the family history of ischaemic disease (0=no, 1=yes) and \\(x_{2}\\) is the estimated total number of cigarettes ever smoked in terms of thousand cigarettes, calculated as the average number smoked annually times the number of years smoking.\n\nWhat is the estimated odds ratio for having coronary artery disease for subjects with a positive family history relative to subjects with a negative family history?\nWhat total number of cigarettes ever smoked carries the same risk as a positive family history? Convert the result into years of smoking 20 cigarettes per day.\nWhat is the odds ratio for coronary artery disease for someone with a positive family history who had smoked 20 cigarettes a day for 30 years compared to a non smoker with no family history?\n\n\n\nExercise 2\nData from 37 patients receiving a non-depleted allogenic bone marrow transplant were examined to see which variables were associated with the occurrence of acute graft-versus-host disease (GvHD: 0=no, 1=yes) (Bagot et al., 1988). Possible predictors are TYPE (type of leukemia: 1=AML, acute myeloid leukaemia; 2=ALL, acute lymphocytic leukaemia; 3=CML, chronic myeloid leukemia), PREG (donor pregnancy: 0= no, 1=yes), and LOGIND (the logarithm of an index of mixed epidermal cell-lymphocyte reactions). The data are in the file GvHD.sav available on Brightspace.\n\nPerform a likelihood ratio test to determine whether there is a significant association between the type of leukemia and the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nWhat is the estimated odds ratio for the occurrence of GvHD for patients with AML compared to those with ALL?\nUse the Hosmer-Lemeshow goodness-of-fit test to evaluate the fit of the model. Based on the results, does the model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab4.html",
    "href": "SPSS_lab4.html",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "",
    "text": "Welcome to lab 4 in the advanced medical statistics course. In this lab, we will focus on the analysis of categorical data and the comparison of proportions between groups. We will also perform several statistical tests for the analysis of paired data."
  },
  {
    "objectID": "SPSS_lab4.html#smoking-and-post-surgical-complications",
    "href": "SPSS_lab4.html#smoking-and-post-surgical-complications",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Smoking and post-surgical complications",
    "text": "Smoking and post-surgical complications\nA study was conducted to investigate whether smoking is associated with an increased risk of post-surgical complications. The relationship between smoking status (smoker or non-smoker) and the occurrence of complications following surgery was examined. The outcome of interest was whether or not a complication occurred (yes or no), with smoking status serving as the explanatory variable to compare complication rates between the two groups.\nThe data from the study are summarized in the following 2x2 contingency table:\n\n\n\n\nComplication\nNo Complication\nTotal\n\n\n\n\nSmokers\n8\n12\n20\n\n\nNon-smokers\n10\n50\n60\n\n\nTotal\n18\n62\n80\n\n\n\n\nConfidence intervals and hypothesis testing for the difference in proportions using the normal approximation\n\n\n\n\n\n\nQuestion 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\nWe can also use the normal approximation to test the hypothesis that the proportion of complications is the same for smokers and non-smokers. This test is known as the two-sample Z test for equality of proportions.\nAs explained in the syllabus, the two-sample Z test uses the pooled population proportion \\(\\hat{p}\\), which is calculated as the total number of events divided by the total sample size. This pooled proportion is used under the null hypothesis, which assumes that the two groups share the same underlying proportion. The standard error of the difference in proportions is then calculated as \\(\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}\\), where \\(n_1\\) and \\(n_2\\) are the sample sizes in the two groups.\nIn contrast, the 95% confidence interval for the difference in proportions does not rely on the pooled proportion. Instead, it calculates the standard error separately for each group using the observed proportions, resulting in an unpooled standard error: \\(\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\), where \\(p_1\\) and \\(p_2\\) are the sample proportions for each group. This approach provides an interval that better reflects the variability in the observed data, independent of the null hypothesis assumption.\nTo conduct the two-sample Z test, we need to create a new dataset containing the above observations. To achieve this, go to File -&gt; New -&gt; Data to create a new data file. Navigate to the Variable View tab and create three variables: Group, Complication, and Frequency. The Group variable should have two levels: Smokers (coded as 1) and Non-smokers (coded as 2), while the Complication variable should have two levels: Complication (coded as 1) and No complication (coded as 2). The Frequency variable will contain the counts of observations for each combination of group and complication status.\nNavigate to the Data View tab and enter the data from the contingency table into the new dataset. For example, in the first row in the screenshot below, the Group variable is set to Smokers (value=1), the Complication variable is set to Complication (value=1), and the Frequency variable is set to 8 (number of individuals in that cell of the contingency table). Repeat this process for the remaining rows to enter all the data.\n\n\n\nScreenshot of the SPSS dataset\n\n\nOnce the data is entered, we need to weight the data by the Frequency variable to account for the multiple observations in each cell. To do this, go to Data -&gt; Weight Cases. Select the Frequency variable and click OK.\nFinally, we can proceed with the two-sample Z test. Go to Analyze -&gt; Compare Means -&gt; Independent-Samples proportions. Select the Group variable as the Grouping Variable and the Complication variable as the Test Variable. Under Confidence Intervals..., select Wald and Wald (Continuity Corrected) to calculate the approximate 95% confidence intervals for the difference in proportions with and without continuity correction. Under Test Type, select Wald H0 (Continuity Corrected) to conduct the two-sample Z test with continuity correction. Click OK to run the test.\n\n\n\n\n\n\nQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIn addition to the p-value, The SPSS output also provides two approximate 95% confidence intervals for the difference in proportions. How do these confidence intervals compare to the one you calculated manually?\n\n\n\n\nChecking of assumptions\nFor the use of the normal approximation to be valid, the expected number of events and non-events in each group should be at least 5.\n\n\n\n\n\n\nExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\nFisher’s exact test\nWhen the expected cell counts are small, the normal approximation may not be appropriate. In such cases, Fisher’s exact test is recommended for testing the association between two categorical variables.\nIn SPSS, we can perform Fisher’s exact test by going to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the Group variable as the Row(s) and the Complication variable as the Column(s). Click on Statistics and check the box Chi_square. Click on Exact Tests and check the box Exact. Click OK to run the test.\n\n\n\n\n\n\nQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?"
  },
  {
    "objectID": "SPSS_lab4.html#vaccine-side-effects-across-age-groups",
    "href": "SPSS_lab4.html#vaccine-side-effects-across-age-groups",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Vaccine side effects across age groups",
    "text": "Vaccine side effects across age groups\nA study was conducted to investigate whether the occurrence of vaccine side effects differs across age groups. Researchers categorized side effects into three types: none, mild, and severe. The study participants were divided into three age groups: 18–39, 40–59, and 60+, and data was collected on the type of side effect experienced by individuals in each group.\nThe research objective was to determine whether the distribution of side effects is consistent across these age groups.\nThe data is summarized in the following contingency table:\n\n\n\nAge Group\nNone\nMild\nSevere\nTotal\n\n\n\n\n18–39\n50\n30\n10\n90\n\n\n40–59\n40\n40\n20\n100\n\n\n60+\n30\n50\n40\n120\n\n\nTotal\n120\n120\n70\n310\n\n\n\n\nChi-square test of homogeneity\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new dataset in SPSS to enter the data from the contingency table.\n\n\nUsing the new dataset, we can perform a chi-square test of homogeneity to determine whether the distribution of side effects is consistent across the three age groups. Go to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the Age Group variable as the Row(s) and the Side Effect variable as the Column(s). Click on Statistics and check the box Chi_square. Click OK to run the test.\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\nChecking of assumptions\nTo use the chi-square test, the expected cell counts should be at least 5 for most cells.\n\n\n\n\n\n\nQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\n\n\nPost-hoc pairwise comparisons\nFinally, we are interested in determining which age groups have significantly different distributions of side effects. We start by comparing the first two age groups (18–39 and 40–59). Assuming the name of the age group variable is Age_Group and the three age groups are coded as 1, 2, and 3, we can filter out the third age group (60+) by going to Data -&gt; Select Cases. Select If condition is satisfied and enter the condition Age_Group &lt; 3. Click OK to filter the data.\nNow that the data is filtered, we can perform a chi-square test for the subset of the data corresponding to the first two age groups by repeating the steps described above. Manually adjust the p-value for multiple comparisons using the Bonferroni correction.\n\n\n\n\n\n\nExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?"
  },
  {
    "objectID": "SPSS_lab4.html#introduction",
    "href": "SPSS_lab4.html#introduction",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Introduction",
    "text": "Introduction\nIn this part of the lab, we will analyze paired data on pocket depth before and after an intervention. Pocket depth refers to the depth of the gum pockets around teeth, measured using a periodontal probe. It is an important indicator of periodontal health. Healthy gums typically have pocket depths less than 3 mm, while deeper pockets may indicate conditions such as gingivitis or periodontitis.\nThe dataset pockets_paired.sav, available on Brightspace, contains the following columns:\n\nsubjectID: Unique identifier for each participant\npocket_depth_before: Average pocket depth (in mm) measured before the intervention\npocket_depth_after: Average pocket depth (in mm) measured after the intervention\n\nThe objective is to determine whether the intervention significantly reduces pocket depth. We will apply three statistical methods to analyze the paired data:\n\nPaired t-test\nSign test\nWilcoxon signed-rank test\n\n\nPaired t-test\nWe start by performing a paired t-test to compare the average pocket depth before and after the intervention. To conduct this analysis in SPSS, follow these steps:\n\nPerform the Test: Go to Analyze &gt; Compare Means &gt; Paired-Samples T Test.\nSelect Variables: Move pocket_depth_before and pocket_depth_after to the Paired Variables box.\nRun the Test: Click OK to run the analysis.\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nBased on the results of the paired t-test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\nChecking of assumptions\nTo determine whether it is appropriate to apply the paired t-test to these data, we need to verify that the differences in pocket depth before and after the intervention are normally distributed. We can visually inspect the distribution of differences using a histogram.\nFirst, we need to create a new variable that calculates the difference between the two measurements. In SPSS, a new variable can be created by navigating to Transform &gt; Compute Variable. Enter a name for the new variable (e.g., diff) in the Target Variable field and enter the expression pocket_depth_after - pocket_depth_before in the Numeric Expression field. Then, click OK to create the new variable.\nNext, we can create a histogram of the differences in pocket depth to visually assess the normality of the distribution by following these steps:\n\nGo to Graphs &gt; Legacy Dialogs &gt; Histogram.\nSelect the diff variable as the Variable and click OK.\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nBased on the histogram, do the differences in pocket depth appear to be approximately normally distributed?\n\n\n\n\n\nSign test and Wilcoxon signed-rank test\nThe sign test is a non-parametric test used to compare two related samples. It is based on the signs of the differences between the pairs of observations. We will apply the sign test to the pocket depth data to determine whether the intervention has a significant effect.\nThe Wilcoxon signed-rank test is another non-parametric test used to compare two related samples. It is based on the ranks of the absolute differences between the pairs of observations. In this case, the sign test is more appropriate because the Wilcoxon signed-rank test requires the assumption of symmetry in the distribution of differences, whereas the previously constructed histogram suggests that the distribution of these differences is left-skewed.\nTo perform the sign test and the Wilcoxon signed-rank test in SPSS, follow these steps:\n\nPerform the Test: Nonparametric Tests &gt; Legacy Dialogs &gt; 2 Related Samples.\nSelect Variables: Move pocket_depth_before and pocket_depth_after to the Test Pairs box.\nRun the Test: Check the Sign box as well as the Wilcoxon box and click OK to run the analysis.\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nBased on the results of the sign test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nBased on the results of the Wilcoxon signed-rank test, can we conclude that the intervention significantly reduces pocket depth?"
  },
  {
    "objectID": "SPSS_lab2.html",
    "href": "SPSS_lab2.html",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "",
    "text": "Welcome to lab 2 in the advanced medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can download from Brightspace.\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Means",
    "text": "Point Estimates and 95% Confidence Intervals for Population Means\nWe will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nGo to Analyze =&gt; Descriptive Statistics =&gt; Frequencies. Select the variable ‘birth weight in grams’ from the list on the left. Then, uncheck ‘Display frequency tables.’ A frequency table shows how many individuals have a particular score, but due to the large number of different scores in this case, it would be quite overwhelming (you can keep it checked if you wish to see it).\nNext, click on the ‘Statistics’ button. Here you can specify which statistics you want for the selected variable(s). Select mean, standard deviation, and S.E. mean. Press ‘Continue’.\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate the corresponding 95% confidence interval based on the normal approximation.\n\n\nYou can also use SPSS to calculate the 95% confidence interval. To do this, go to Analyze =&gt; Descriptive Statistics =&gt; Explore. Click on ‘Statistics,’ where you can specify the confidence interval you want to calculate. The default is set to 95%, so no changes are needed. SPSS uses the t-distribution to calculate this confidence interval, which provides a more accurate estimate when the population standard deviation is unknown and the sample size is small. Afterward, select ‘Statistics’ under ‘Display’ to ensure the output contains only the desired descriptive statistics. Press ‘OK’ to generate the output.”\n\n\n\n\n\n\nQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?"
  },
  {
    "objectID": "SPSS_lab2.html#one-sample-t-test",
    "href": "SPSS_lab2.html#one-sample-t-test",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\nTo determine whether the population mean birth weight differs significantly from a hypothesized value of 3000 grams, we conduct a one-sample t-test. To do this, go to Analyze =&gt; Compare Means =&gt; One-Sample t-Test. Select the variable ‘birth weight in grams’ and place it under ‘Test Variables’. We want to determine if the population mean significantly differs from the threshold value of 3000 grams. Enter ‘3000’ as the test value. Press ‘OK’. Now you will see the result of the t-test in your output.\n\n\n\n\n\n\nQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\nOne of the assumptions underlying the one-sample t-test is that the data are normally distributed. We can check this assumption by creating a histogram. To do this. go to Graphs =&gt; Legacy Dialoges =&gt; Histrogram.... Select the variable ‘birth weight in grams’ and place it under ‘Variable’. Check the ‘Diplay normal curve’ checkbox and press ‘OK’.\n\n\n\n\n\n\nQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?"
  },
  {
    "objectID": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable that takes a value 1 if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of 0 otherwise.\nWe start by making a frequency table to calculate the frequency of each category of the ‘low’ variable:\n\nGo to Analyze =&gt; Descriptive Statistics =&gt; Frequencies...\nSelect the variable ‘low’ and move it to ‘Variable(s)’\nPress ‘OK’ to obtain the frequency table\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation."
  },
  {
    "objectID": "SPSS_lab2.html#binomial-test",
    "href": "SPSS_lab2.html#binomial-test",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%:\n\nGo to Analyze =&gt; Compare Means =&gt; One-Sample Proportions....\nSelect the variable ‘low’ and move it to the ‘Test Variable List’.\nUnder ‘Define Success’, select ‘Value(s)’ and enter the number 1 to indicate that having a low birth weight baby is the event of interest.\nClick on the ‘Test’ button to open the dialog box, select ‘Exact Binomial’ as the test and enter 0.3 as the test value. Press the ‘Continue’ button.\nClick ‘OK’ to run the test.\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50 % of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%.\nHint: to perform this test in SPSS, we need to create a new dataset containing the above observations. To achieve this, construct a new data file containing two variables, like this:\n\n\n\nAlcohol\nnumber\n\n\n\n\n1\n128\n\n\n0\n72\n\n\n\nThen, instruct SPSS to weight the categories (Alcohol) by “number” via Data → Weight cases.\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nDifferences in Two-Sided P-Value Calculation Between SPSS and R\n\n\n\nWhen conducting statistical tests, it is important to understand that different software packages can calculate two-sided p-values in slightly different ways, which may lead to variations in results. A key difference exists between how SPSS and base R handle this calculation:\n\nSPSS often calculates two-sided p-values by doubling the one-sided p-value. Specifically, SPSS determines the probability of the observed outcome in one direction (greater or less than a given value) and then multiplies this value by 2. This approach assumes that the distribution of the test statistic is symmetric under the null hypothesis. While this method is straightforward, it can be misleading if the distribution is skewed or the sample size is small, as it may not fully account for the asymmetry in the data.\nBase R (e.g., the binom.test() function) uses a more exact method for calculating two-sided p-values. R’s approach sums the probabilities of observing outcomes that are as extreme as, or more extreme than, the observed value in both directions (both tails of the distribution). This method does not assume symmetry and provides a more accurate p-value, particularly for small samples or skewed distributions."
  },
  {
    "objectID": "R_lab8.html",
    "href": "R_lab8.html",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "",
    "text": "In part 1 of the lab, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav on Brightspace). The outcome of interest for today’s analysis is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(DescTools)  # for performing Hosmer-Lemeshow Goodness of Fit Tests\n\n# Load the dataset\nwhas500 &lt;- read_sav(\"datasets/whas500.sav\")\n\n# Convert labeled variables to factors\nwhas500 &lt;- whas500 %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\n\ncontingencyTable &lt;- table(whas500$gender, whas500$dstat) # create 2 x 2 table\ncontingencyTable\n\nprop.table(contingencyTable, margin = 1) # Calulate the row proportions (margin = 1)\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. In R, this can be achieved using the glm() function:\n\n# Create a 0/1 numeric version of the dependent variable, where a value of 1 means \"success\" (i.e., occurrence of the event of interest)\nwhas500$dstat_numeric &lt;- ifelse(whas500$dstat==\"dead\", 1, 0)\n\n# Fit logistic regression model \nmodel.sex &lt;- glm(dstat_numeric ~ gender, family = binomial, data = whas500)\nsummary(model.sex)\n\nExplanation: glm stands for “generalized linear model,” an extension of linear regression that accommodates different types of outcome distributions and includes a link function to model the relationship between predictors and the outcome. The argument family = binomial specifies that the outcome distribution is Bernoulli/binomial. By default, the link function for this model is “logit,” which corresponds to logistic regression.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\n\n# Fit logistic regression model \nmodel.sex.age &lt;- glm(dstat_numeric ~ gender + age, family = binomial, data = whas500)\nsummary(model.sex.age)\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To achieve this, we first need to fit the reduced model that only includes age and then perform the LRT using the anova() function:\n\n# Fit reduced model\nmodel.age &lt;- glm(dstat_numeric ~ age, family = binomial, data = whas500)\n\n# Perform likelihood ratio test\nanova(model.age, model.sex.age, test=\"LRT\")\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow to the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In R, this test can be performed using the HosmerLemeshowTest() function from the DescTools package:\n\nHosmerLemeshowTest(fit=fitted(model.sex.age), obs=model.sex.age$y, ngr = 10, verbose = TRUE)\n\nExplanation:\n\nfitted(model.sex.age) returns the caclulated in-hospital death probabilities based on the fitted logistic regression model for each patient in the dataset. These are the\nmodel.sex.age$y retrieves the outcome variable from the fitted logistic regression model.\nngr = 10 specifies the number of groups to be used in the test, which is set to 10 by default.\nverbose = TRUE not only prints the test-statistic, degrees of freedom, and p-value but also the groups used to perform the test.\n\nThe Hosmer-Lemeshow goodness-of-fit test has two variations: the C statistic and the H statistic, which differ in how they group predicted probabilities for comparison. The C statistic groups predicted probabilities into deciles (10 equal-sized groups based on the range of probabilities), while the H statistic uses fixed cutoffs (e.g., evenly spaced intervals between 0 and 1). The C statistic is the most commonly used version as it adapts to the data distribution, ensuring well-populated groups, making it suitable for general model fit evaluation. In this lab, we therefore focus on this latter statistics (with matches the explanation in the syllabus).\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab8.html#part-1-risk-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "href": "R_lab8.html#part-1-risk-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "",
    "text": "In part 1 of the lab, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav on Brightspace). The outcome of interest for today’s analysis is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(DescTools)  # for performing Hosmer-Lemeshow Goodness of Fit Tests\n\n# Load the dataset\nwhas500 &lt;- read_sav(\"datasets/whas500.sav\")\n\n# Convert labeled variables to factors\nwhas500 &lt;- whas500 %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\n\ncontingencyTable &lt;- table(whas500$gender, whas500$dstat) # create 2 x 2 table\ncontingencyTable\n\nprop.table(contingencyTable, margin = 1) # Calulate the row proportions (margin = 1)\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. In R, this can be achieved using the glm() function:\n\n# Create a 0/1 numeric version of the dependent variable, where a value of 1 means \"success\" (i.e., occurrence of the event of interest)\nwhas500$dstat_numeric &lt;- ifelse(whas500$dstat==\"dead\", 1, 0)\n\n# Fit logistic regression model \nmodel.sex &lt;- glm(dstat_numeric ~ gender, family = binomial, data = whas500)\nsummary(model.sex)\n\nExplanation: glm stands for “generalized linear model,” an extension of linear regression that accommodates different types of outcome distributions and includes a link function to model the relationship between predictors and the outcome. The argument family = binomial specifies that the outcome distribution is Bernoulli/binomial. By default, the link function for this model is “logit,” which corresponds to logistic regression.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\n\n# Fit logistic regression model \nmodel.sex.age &lt;- glm(dstat_numeric ~ gender + age, family = binomial, data = whas500)\nsummary(model.sex.age)\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To achieve this, we first need to fit the reduced model that only includes age and then perform the LRT using the anova() function:\n\n# Fit reduced model\nmodel.age &lt;- glm(dstat_numeric ~ age, family = binomial, data = whas500)\n\n# Perform likelihood ratio test\nanova(model.age, model.sex.age, test=\"LRT\")\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow to the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In R, this test can be performed using the HosmerLemeshowTest() function from the DescTools package:\n\nHosmerLemeshowTest(fit=fitted(model.sex.age), obs=model.sex.age$y, ngr = 10, verbose = TRUE)\n\nExplanation:\n\nfitted(model.sex.age) returns the caclulated in-hospital death probabilities based on the fitted logistic regression model for each patient in the dataset. These are the\nmodel.sex.age$y retrieves the outcome variable from the fitted logistic regression model.\nngr = 10 specifies the number of groups to be used in the test, which is set to 10 by default.\nverbose = TRUE not only prints the test-statistic, degrees of freedom, and p-value but also the groups used to perform the test.\n\nThe Hosmer-Lemeshow goodness-of-fit test has two variations: the C statistic and the H statistic, which differ in how they group predicted probabilities for comparison. The C statistic groups predicted probabilities into deciles (10 equal-sized groups based on the range of probabilities), while the H statistic uses fixed cutoffs (e.g., evenly spaced intervals between 0 and 1). The C statistic is the most commonly used version as it adapts to the data distribution, ensuring well-populated groups, making it suitable for general model fit evaluation. In this lab, we therefore focus on this latter statistics (with matches the explanation in the syllabus).\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab8.html#part-2-unguided-exercises",
    "href": "R_lab8.html#part-2-unguided-exercises",
    "title": "Advanced Medical Statistics – Lab 8",
    "section": "Part 2: unguided exercises",
    "text": "Part 2: unguided exercises\n\nExercise 1\nMultiple logistic regression was used to construct a prognostic index to predict coronary artery disease from data on 348 patients with valvular heart disease who had undergone routine coronary arteriography before valve replacement (Ramsdale et al. 1982). The estimated equation was:\n\\[logit(p) = ln(p/(1-p)) = b_{0} + 1.167 \\times x{1} + 0.0106 \\times x_{2} + \\textrm{other terms}\\]\nwhere \\(x_{1}\\) stands for the family history of ischaemic disease (0=no, 1=yes) and \\(x_{2}\\) is the estimated total number of cigarettes ever smoked in terms of thousand cigarettes, calculated as the average number smoked annually times the number of years smoking.\n\nWhat is the estimated odds ratio for having coronary artery disease for subjects with a positive family history relative to subjects with a negative family history?\nWhat total number of cigarettes ever smoked carries the same risk as a positive family history? Convert the result into years of smoking 20 cigarettes per day.\nWhat is the odds ratio for coronary artery disease for someone with a positive family history who had smoked 20 cigarettes a day for 30 years compared to a non smoker with no family history?\n\n\n\nExercise 2\nData from 37 patients receiving a non-depleted allogenic bone marrow transplant were examined to see which variables were associated with the occurrence of acute graft-versus-host disease (GvHD: 0=no, 1=yes) (Bagot et al., 1988). Possible predictors are TYPE (type of leukemia: 1=AML, acute myeloid leukaemia; 2=ALL, acute lymphocytic leukaemia; 3=CML, chronic myeloid leukemia), PREG (donor pregnancy: 0= no, 1=yes), and LOGIND (the logarithm of an index of mixed epidermal cell-lymphocyte reactions). The data are in the file GvHD.sav available on Brightspace.\n\nPerform a likelihood ratio test to determine whether there is a significant association between the type of leukemia and the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nWhat is the estimated odds ratio for the occurrence of GvHD for patients with AML compared to those with ALL?\nUse the Hosmer-Lemeshow goodness-of-fit test to evaluate the fit of the model. Based on the results, does the model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab4.html",
    "href": "R_lab4.html",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "",
    "text": "Welcome to lab 4 in the advanced medical statistics course. In this lab, we will focus on the analysis of categorical data and the comparison of proportions between groups. We will also perform several statistical tests for the analysis of paired data."
  },
  {
    "objectID": "R_lab4.html#smoking-and-post-surgical-complications",
    "href": "R_lab4.html#smoking-and-post-surgical-complications",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Smoking and post-surgical complications",
    "text": "Smoking and post-surgical complications\nA study was conducted to investigate whether smoking is associated with an increased risk of post-surgical complications. The relationship between smoking status (smoker or non-smoker) and the occurrence of complications following surgery was examined. The outcome of interest was whether or not a complication occurred (yes or no), with smoking status serving as the explanatory variable to compare complication rates between the two groups.\nThe data from the study are summarized in the following 2x2 contingency table:\n\n\n\n\nComplication\nNo Complication\nTotal\n\n\n\n\nSmokers\n8\n12\n20\n\n\nNon-smokers\n10\n50\n60\n\n\nTotal\n18\n62\n80\n\n\n\n\nConfidence intervals and hypothesis testing for the difference in proportions using the normal approximation\n\n\n\n\n\n\nQuestion 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\nWe can also use the normal approximation to test the hypothesis that the proportion of complications is the same for smokers and non-smokers. This test is known as the two-sample Z test for equality of proportions.\nAs explained in the syllabus, the two-sample Z test uses the pooled population proportion \\(\\hat{p}\\), which is calculated as the total number of events divided by the total sample size. This pooled proportion is used under the null hypothesis, which assumes that the two groups share the same underlying proportion. The standard error of the difference in proportions is then calculated as \\(\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}\\), where \\(n_1\\) and \\(n_2\\) are the sample sizes in the two groups.\nIn contrast, the 95% confidence interval for the difference in proportions does not rely on the pooled proportion. Instead, it calculates the standard error separately for each group using the observed proportions, resulting in an unpooled standard error: \\(\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\), where \\(p_1\\) and \\(p_2\\) are the sample proportions for each group. This approach provides an interval that better reflects the variability in the observed data, independent of the null hypothesis assumption.\nTo conduct the two-sample Z test, we start by creating a contingency table:\n\n# Create a contingency table\ncomplications &lt;- matrix(c(8, 12, 10, 50), nrow = 2, byrow = TRUE)\ncolnames(complications) &lt;- c(\"Complication\", \"No Complication\")\nrownames(complications) &lt;- c(\"Smokers\", \"Non-smokers\")\ncomplications &lt;- as.table(complications)\ncomplications\n\nNext, we supply this table to the prop.test() function to calculate the test statistic and p-value:\n\nprop.test(complications)\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIn addition to the p-value, output of the prop.test() function also provides an approximate 95% confidence interval for the difference in proportions. How does this confidence interval compare to the one you calculated manually?\n\n\n\n\nChecking of assumptions\nFor the use of the normal approximation to be valid, the expected number of events and non-events in each group should be at least 5.\n\n\n\n\n\n\nExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\nFisher’s exact test\nWhen the expected cell counts are small, the normal approximation may not be appropriate. In such cases, Fisher’s exact test is recommended for testing the association between two categorical variables.\nIn R, we can perform Fisher’s exact test using the fisher.test() function. The test is based on the hypergeometric distribution and provides an exact p-value for the association between the two variables:\n\nfisher.test(complications)\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?"
  },
  {
    "objectID": "R_lab4.html#vaccine-side-effects-across-age-groups",
    "href": "R_lab4.html#vaccine-side-effects-across-age-groups",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Vaccine side effects across age groups",
    "text": "Vaccine side effects across age groups\nA study was conducted to investigate whether the occurrence of vaccine side effects differs across age groups. Researchers categorized side effects into three types: none, mild, and severe. The study participants were divided into three age groups: 18–39, 40–59, and 60+, and data was collected on the type of side effect experienced by individuals in each group.\nThe research objective was to determine whether the distribution of side effects is consistent across these age groups.\nThe data is summarized in the following contingency table:\n\n\n\nAge Group\nNone\nMild\nSevere\nTotal\n\n\n\n\n18–39\n50\n30\n10\n90\n\n\n40–59\n40\n40\n20\n100\n\n\n60+\n30\n50\n40\n120\n\n\nTotal\n120\n120\n70\n310\n\n\n\n\nChi-square test of homogeneity\nTo conduct a chi-square test of homogeneity, we start by creating a contingency table:\n\n# Create a contingency table\nside_effects &lt;- matrix(c(50, 30, 10, 40, 40, 20, 30, 50, 40), nrow = 3, byrow = TRUE)\ncolnames(side_effects) &lt;- c(\"None\", \"Mild\", \"Severe\")\nrownames(side_effects) &lt;- c(\"18–39\", \"40–59\", \"60+\")\nside_effects &lt;- as.table(side_effects)\nside_effects\n\nNext, we supply this table to the chisq.test() function to calculate the test statistic and p-value:\n\nchisq_test_overall &lt;- chisq.test(side_effects)\nprint(chisq_test_overall)\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\nChecking of assumptions\nTo use the chi-square test, the expected cell counts should be at least 5 for most cells. To check this assumption, we retrieve the table of expected counts from the output of the chisq.test() function, which we stored in the chisq_test_overall object:\n\n# Retrieve the table of expected counts\nchisq_test_overall$expected\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\n\n\nPost-hoc pairwise comparisons\nFinally, we are interested in determining which age groups have significantly different distributions of side effects. For a \\(n \\times 2\\) table, this can be achieved easily using the pairwise.prop.test() function, which performs pairwise comparisons of proportions between groups with adjustments for multiple testing. However, in our case, we are working with a \\(3 \\times 3\\) contingency table, where the outcomes have more than two categories (None, Mild, Severe). This complexity requires a manual approach to perform pairwise comparisons.\nWe start by comparing the first two age groups (18–39 and 40–59). First, we set up the contingency table for these two groups:\n\ntable_12 &lt;- side_effects[c(\"18–39\", \"40–59\"), ]\ntable_12\n\nNext, we perform the chi-square test for this subset of the data and adjust the p-value by applying the Bonferroni correction:\n\n# Perform chi-square test for the subset of data\nchisq_test_12 &lt;- chisq.test(table_12)\nprint(chisq_test_12)\n\n# Adjust the p-value for multiple testing\np_adjusted_12 &lt;- 3*chisq_test_12$p.value\np_adjusted_12\n\n\n\n\n\n\n\nExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?"
  },
  {
    "objectID": "R_lab4.html#introduction",
    "href": "R_lab4.html#introduction",
    "title": "Advanced Medical Statistics – Lab 4",
    "section": "Introduction",
    "text": "Introduction\nIn this part of the lab, we will analyze paired data on pocket depth before and after an intervention. Pocket depth refers to the depth of the gum pockets around teeth, measured using a periodontal probe. It is an important indicator of periodontal health. Healthy gums typically have pocket depths less than 3 mm, while deeper pockets may indicate conditions such as gingivitis or periodontitis.\nThe dataset pockets_paired.sav, available on Brightspace, contains the following columns:\n\nsubjectID: Unique identifier for each participant\npocket_depth_before: Average pocket depth (in mm) measured before the intervention\npocket_depth_after: Average pocket depth (in mm) measured after the intervention\n\nThe objective is to determine whether the intervention significantly reduces pocket depth. We will apply three statistical methods to analyze the paired data:\n\nPaired t-test\nSign test\nWilcoxon signed-rank test\n\n\nPaired t-test\nFirst, we load the data from the provided SPSS file pockets_paired.sav:\n\nlibrary(haven)\npockets &lt;- read_sav(\"pockets_paired.sav\")\nhead(pockets)\n\nNext, we perform a paired t-test to compare the average pocket depth before and after the intervention:\n\n# Perform paired t-test\nt_test &lt;- t.test(pockets$pocket_depth_before, pockets$pocket_depth_after, paired = TRUE)\nprint(t_test)\n\nThe t.test() function with the argument paired = TRUE conducts a paired t-test; setting paired = FALSE (default) would perform an independent samples t-test. The output includes the test statistic, degrees of freedom, and the p-value.\n\n\n\n\n\n\nQuestion 10\n\n\n\nBased on the results of the paired t-test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\nChecking of assumptions\nTo determine whether it is appropriate to apply the paired t-test to these data, we need to verify that the differences in pocket depth before and after the intervention are normally distributed. We can visually inspect the distribution of differences using a histogram:\n\n# Calculate the differences in pocket depth\npockets$diff &lt;- pockets$pocket_depth_after - pockets$pocket_depth_before\n\n# Create a histogram of the differences\nlibrary(ggplot2)\nggplot(pockets, aes(x = diff)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of differences in pocket depth\",\n       x = \"Difference in pocket depth (mm)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nBased on the histogram, do the differences in pocket depth appear to be approximately normally distributed?\n\n\n\n\n\nSign test\nThe sign test is a non-parametric test used to compare two related samples. It is based on the signs of the differences between the pairs of observations. We will apply the sign test to the pocket depth data to determine whether the intervention has a significant effect.\nFirst, we calculate the total number of positive and negative signs in the differences:\n\n# Calculate the number of positive and negative signs\nsigns_positive &lt;- sum(pockets$diff &gt; 0)\nsigns_negative &lt;- sum(pockets$diff &lt; 0)\n\nNext, we perform the sign test using the binom.test() function, which calculates the exact p-value for the sign test:\n\n# Perform the sign test\n# There are 7 positive signs out of 96 pairs with either a positive or negative sign\nsign_test &lt;- binom.test(signs_positive, (signs_positive + signs_negative), p = 0.5)\nprint(sign_test)\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nBased on the results of the sign test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\n\nWilcoxon signed-rank test\nThe Wilcoxon signed-rank test is another non-parametric test used to compare two related samples. It is based on the ranks of the absolute differences between the pairs of observations. In this case, the sign test is more appropriate because the Wilcoxon signed-rank test requires the assumption of symmetry in the distribution of differences, whereas the previously constructed histogram suggests that the distribution of these differences is left-skewed.\nTo perform the Wilcoxon signed-rank test, we use the wilcox.test() function:\n\n# Perform the Wilcoxon signed-rank test\nwilcox_test &lt;- wilcox.test(pockets$pocket_depth_before, pockets$pocket_depth_after, paired = TRUE)\nprint(wilcox_test)\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nBased on the results of the Wilcoxon signed-rank test, can we conclude that the intervention significantly reduces pocket depth?"
  },
  {
    "objectID": "R_lab2.html",
    "href": "R_lab2.html",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "",
    "text": "Welcome to lab 2 in the advanced medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can download from Brightspace.\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the dataset\nlowbwt &lt;- read_sav(\"lowbwt.sav\")\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Means",
    "text": "Point Estimates and 95% Confidence Intervals for Population Means\nWe will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nTo calculate the mean, standard deviation, and standard error of the mean, you can use the following R code:\n\nmean_bwt &lt;- mean(lowbwt$bwt, na.rm = TRUE)\nsd_bwt &lt;- sd(lowbwt$bwt, na.rm = TRUE)\nse_bwt &lt;- sd_bwt / sqrt(sum(!is.na(lowbwt$bwt)))\n\nmean_bwt\nsd_bwt\nse_bwt\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate the corresponding 95% confidence interval based on the normal approximation.\n\n\nYou can also use R to calculate the 95% confidence interval by using the qt() function to determine the appropriate t-value. This approach uses the t-distribution, which provides a more accurate confidence interval when the population standard deviation is unknown and the sample size is small.\n\nn &lt;- sum(!is.na(lowbwt$bwt))\nt_value &lt;- qt(0.975, df = n - 1)\nlower_ci &lt;- mean_bwt - t_value * se_bwt\nupper_ci &lt;- mean_bwt + t_value * se_bwt\n\nc(lower_ci, upper_ci)\n\nExplanation:\nThe qt() function in R is used to obtain the critical value from the t-distribution. In this case, we use qt(0.975, df = n - 1) to get the t-value for a 95% confidence interval, where 0.975 corresponds to the upper tail probability for a two-sided confidence level of 95%, and df = n - 1 represents the degrees of freedom (sample size minus one).\n\n\n\n\n\n\nQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?"
  },
  {
    "objectID": "R_lab2.html#one-sample-t-test",
    "href": "R_lab2.html#one-sample-t-test",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\nTo determine whether the population mean birth weight differs significantly from a hypothesized value of 3000 grams, we use the t.test() function to conduct a one-sample t-test:\n\nt_test_result &lt;- t.test(lowbwt$bwt, mu = 3000)\nt_test_result\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\nOne of the assumptions underlying the one-sample t-test is that the data are normally distributed. We can check this assumption by creating a histogram:\n\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Birth Weights\", x = \"Birth Weight (grams)\", y = \"Frequency\")\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?"
  },
  {
    "objectID": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable that takes a value 1 if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of 0 otherwise.\nWe start by using the table() function to calculate the frequency of each category of the ‘low’ variable:\n\n# Calculate the frequency of each category of the 'low' variable\nfreq_table &lt;- table(lowbwt$low)\nfreq_table\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation."
  },
  {
    "objectID": "R_lab2.html#binomial-test",
    "href": "R_lab2.html#binomial-test",
    "title": "Advanced medical Statistics – Lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%:\n\nbinom_test_result &lt;- binom.test(sum(lowbwt$low == 1, na.rm = TRUE),\n                                length(na.omit(lowbwt$low)),\n                                p = 0.30)\nbinom_test_result\n\nExplanation:\n\nbinom.test() is the function used to perform the binomial test.\nsum(lowbwt$low == 1, na.rm = TRUE) calculates the number of babies in the dataset with low birth weight (where low is 1). The na.rm = TRUE argument tells R to ignore missing values.\nlength(na.omit(lowbwt$lbw)) gives the total number of non-missing observations in the low variable.\np = 0.30 specifies the hypothesized population proportion (30%).\n\nThe output displays, among other statistics, the two-sided p-value and an exact 95% confidence interval calculated using the Clopper and Pearson procedure.\n\n\n\n\n\n\nQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50 % of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%.\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nDifferences in Two-Sided P-Value Calculation Between SPSS and R\n\n\n\nWhen conducting statistical tests, it is important to understand that different software packages can calculate two-sided p-values in slightly different ways, which may lead to variations in results. A key difference exists between how SPSS and base R handle this calculation:\n\nSPSS often calculates two-sided p-values by doubling the one-sided p-value. Specifically, SPSS determines the probability of the observed outcome in one direction (greater or less than a given value) and then multiplies this value by 2. This approach assumes that the distribution of the test statistic is symmetric under the null hypothesis. While this method is straightforward, it can be misleading if the distribution is skewed or the sample size is small, as it may not fully account for the asymmetry in the data.\nBase R (e.g., the binom.test() function) uses a more exact method for calculating two-sided p-values. R’s approach sums the probabilities of observing outcomes that are as extreme as, or more extreme than, the observed value in both directions (both tails of the distribution). This method does not assume symmetry and provides a more accurate p-value, particularly for small samples or skewed distributions."
  },
  {
    "objectID": "lab7_answers.html",
    "href": "lab7_answers.html",
    "title": "Advanced Medical Statistics – Answers lab 7",
    "section": "",
    "text": "Create an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\n\n\n\nCall:\nlm(formula = los ~ age + gender + hr + sysbp + diasbp + bmi + \n    cvd + sho, data = whas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.335 -2.653 -1.071  1.200 40.073 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2648144  2.1394896   1.526 0.127659    \nage           0.0031994  0.0168850   0.189 0.849792    \ngenderfemale  0.8575246  0.4489334   1.910 0.056698 .  \nhr            0.0190577  0.0090828   2.098 0.036396 *  \nsysbp        -0.0008358  0.0085656  -0.098 0.922304    \ndiasbp        0.0141799  0.0128884   1.100 0.271780    \nbmi          -0.0304532  0.0427613  -0.712 0.476700    \ncvdyes        0.3903925  0.4981468   0.784 0.433600    \nshoyes        3.5265170  1.0329265   3.414 0.000693 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.632 on 491 degrees of freedom\nMultiple R-squared:  0.04991,   Adjusted R-squared:  0.03443 \nF-statistic: 3.224 on 8 and 491 DF,  p-value: 0.001388\n\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: los\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)    50.0   1  2.3286 0.1276592    \nage             0.8   1  0.0359 0.8497924    \ngender         78.3   1  3.6486 0.0566978 .  \nhr             94.5   1  4.4025 0.0363962 *  \nsysbp           0.2   1  0.0095 0.9223042    \ndiasbp         26.0   1  1.2105 0.2717799    \nbmi            10.9   1  0.5072 0.4766997    \ncvd            13.2   1  0.6142 0.4336001    \nsho           250.1   1 11.6561 0.0006929 ***\nResiduals   10535.8 491                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that the predictor with the highest p-value is sysbp (\\(p = 0.92\\)). Systolic blood pressure is the least significant predictor and should be removed from the model.\n\n\n\nThe following variables are sequentially removed from the model (after initially removing sysbp):\n\nage: p-value = 0.86\ncvd: p-value = 0.41\nbmi: p-value = 0.44\ndiasbp: p-value = 0.22\n\n\n\n\n\n\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe overall fit of the model appears reasonable, as the residuals are generally centered around zero with no major patterns suggesting severe violations of linearity. However, there are some outliers with very long lengths of stay (LOS) that are not adequately captured by the model. These outliers lead to a right skew in the residual distribution, as seen in the histogram, influencing model fit. While the current model seems to work reasonably well for most observations, further steps (e.g., transformations or robust regression techniques) could be considered to better account for these extreme cases."
  },
  {
    "objectID": "lab7_answers.html#part-1-building-prediction-models-using-backward-elimination",
    "href": "lab7_answers.html#part-1-building-prediction-models-using-backward-elimination",
    "title": "Advanced Medical Statistics – Answers lab 7",
    "section": "",
    "text": "Create an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\n\n\n\nCall:\nlm(formula = los ~ age + gender + hr + sysbp + diasbp + bmi + \n    cvd + sho, data = whas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.335 -2.653 -1.071  1.200 40.073 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2648144  2.1394896   1.526 0.127659    \nage           0.0031994  0.0168850   0.189 0.849792    \ngenderfemale  0.8575246  0.4489334   1.910 0.056698 .  \nhr            0.0190577  0.0090828   2.098 0.036396 *  \nsysbp        -0.0008358  0.0085656  -0.098 0.922304    \ndiasbp        0.0141799  0.0128884   1.100 0.271780    \nbmi          -0.0304532  0.0427613  -0.712 0.476700    \ncvdyes        0.3903925  0.4981468   0.784 0.433600    \nshoyes        3.5265170  1.0329265   3.414 0.000693 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.632 on 491 degrees of freedom\nMultiple R-squared:  0.04991,   Adjusted R-squared:  0.03443 \nF-statistic: 3.224 on 8 and 491 DF,  p-value: 0.001388\n\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: los\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)    50.0   1  2.3286 0.1276592    \nage             0.8   1  0.0359 0.8497924    \ngender         78.3   1  3.6486 0.0566978 .  \nhr             94.5   1  4.4025 0.0363962 *  \nsysbp           0.2   1  0.0095 0.9223042    \ndiasbp         26.0   1  1.2105 0.2717799    \nbmi            10.9   1  0.5072 0.4766997    \ncvd            13.2   1  0.6142 0.4336001    \nsho           250.1   1 11.6561 0.0006929 ***\nResiduals   10535.8 491                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that the predictor with the highest p-value is sysbp (\\(p = 0.92\\)). Systolic blood pressure is the least significant predictor and should be removed from the model.\n\n\n\nThe following variables are sequentially removed from the model (after initially removing sysbp):\n\nage: p-value = 0.86\ncvd: p-value = 0.41\nbmi: p-value = 0.44\ndiasbp: p-value = 0.22\n\n\n\n\n\n\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe overall fit of the model appears reasonable, as the residuals are generally centered around zero with no major patterns suggesting severe violations of linearity. However, there are some outliers with very long lengths of stay (LOS) that are not adequately captured by the model. These outliers lead to a right skew in the residual distribution, as seen in the histogram, influencing model fit. While the current model seems to work reasonably well for most observations, further steps (e.g., transformations or robust regression techniques) could be considered to better account for these extreme cases."
  },
  {
    "objectID": "lab7_answers.html#part-2-automated-procedures-for-building-prediction-models",
    "href": "lab7_answers.html#part-2-automated-procedures-for-building-prediction-models",
    "title": "Advanced Medical Statistics – Answers lab 7",
    "section": "Part 2: Automated procedures for building prediction models",
    "text": "Part 2: Automated procedures for building prediction models\n\nExercise: Automated procedures vs manual model\n\nR\n\nlibrary(MASS)\nfit &lt;- lm(los ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho, data = whas)\nstep_model &lt;- stepAIC(fit, direction = \"backward\")\n\nStart:  AIC=1541.96\nlos ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho\n\n         Df Sum of Sq   RSS    AIC\n- sysbp   1     0.204 10536 1540.0\n- age     1     0.770 10537 1540.0\n- bmi     1    10.883 10547 1540.5\n- cvd     1    13.179 10549 1540.6\n- diasbp  1    25.974 10562 1541.2\n&lt;none&gt;                10536 1542.0\n- gender  1    78.292 10614 1543.7\n- hr      1    94.469 10630 1544.4\n- sho     1   250.115 10786 1551.7\n\nStep:  AIC=1539.97\nlos ~ age + gender + hr + diasbp + bmi + cvd + sho\n\n         Df Sum of Sq   RSS    AIC\n- age     1     0.682 10537 1538.0\n- bmi     1    11.109 10547 1538.5\n- cvd     1    12.975 10549 1538.6\n- diasbp  1    38.551 10575 1539.8\n&lt;none&gt;                10536 1540.0\n- gender  1    78.719 10615 1541.7\n- hr      1    96.991 10633 1542.6\n- sho     1   260.543 10797 1550.2\n\nStep:  AIC=1538.01\nlos ~ gender + hr + diasbp + bmi + cvd + sho\n\n         Df Sum of Sq   RSS    AIC\n- cvd     1    14.624 10551 1536.7\n- bmi     1    15.372 10552 1536.7\n- diasbp  1    37.888 10575 1537.8\n&lt;none&gt;                10537 1538.0\n- gender  1    84.736 10621 1540.0\n- hr      1   101.448 10638 1540.8\n- sho     1   264.134 10801 1548.4\n\nStep:  AIC=1536.7\nlos ~ gender + hr + diasbp + bmi + sho\n\n         Df Sum of Sq   RSS    AIC\n- bmi     1    12.883 10564 1535.3\n- diasbp  1    38.599 10590 1536.5\n&lt;none&gt;                10551 1536.7\n- gender  1    97.518 10649 1539.3\n- hr      1    99.732 10651 1539.4\n- sho     1   266.183 10818 1547.2\n\nStep:  AIC=1535.31\nlos ~ gender + hr + diasbp + sho\n\n         Df Sum of Sq   RSS    AIC\n- diasbp  1    32.352 10597 1534.8\n&lt;none&gt;                10564 1535.3\n- hr      1   104.369 10669 1538.2\n- gender  1   108.564 10673 1538.4\n- sho     1   273.160 10837 1546.1\n\nStep:  AIC=1534.84\nlos ~ gender + hr + sho\n\n         Df Sum of Sq   RSS    AIC\n&lt;none&gt;                10597 1534.8\n- gender  1     98.11 10695 1537.5\n- hr      1    116.84 10713 1538.3\n- sho     1    264.48 10861 1545.2\n\nsummary(step_model)\n\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n\n\nThe final model obtained from the automated procedure is identical to the manually created model in Part 1.\n\n\nSPSS\n\n\n\nSPSS results of the backward elimination procedure\n\n\nThe table above shows the variables eliminated at each step of the backward elimination procedure in SPSS. The final model obtained from the automated procedure is identical to the manually created model in Part 1."
  },
  {
    "objectID": "lab7_answers.html#part-3-causal-diagrams",
    "href": "lab7_answers.html#part-3-causal-diagrams",
    "title": "Advanced Medical Statistics – Answers lab 7",
    "section": "Part 3: Causal diagrams",
    "text": "Part 3: Causal diagrams\nFor each of the exercises below:\n\nTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\nCheck your answer using the DAGitty webtool\n\n\nExercise 1\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n\n\nDAG exercise 1\n\n\nAnswer:\nFollowing the recipe: after removing all arrows leaving E, there are several unblocked paths leading from E to O. Just like in the lecture, adjusting for v2 opens a backdoor path (E – v1 – v3 – O) This newly opened backdoor path needs to be closed by also conditioning on v1 or v3, or both. Hence, there are 3 options: (v1, v2, v3) ; (v1, v2) ; and finally, (v2, v3).\n\n\nExercise 2\nIn the graph depicted below, what happens when you additionally adjust for v5?\n\n\n\nDAG exercise 2\n\n\nAnswer: When adjusting for v5, we are blocking the effect through this indirect path from E to O (v5 is a mediator between E and O). Instead of the total effect of E on O, we will be estimating the direct effect.\nIn DAGitty, when you set v5 to ‘adjusted’, the algorithm will say the following: “The total effect cannot be estimated due to adjustment for an intermediate or a descendant of an intermediate.”\n\n\nExercise 3\nThis diagram is slightly different: v1 now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of v1 on O?\n\n\n\nDAG exercise 3\n\n\nAnswer: No adjustment is needed: there are no backdoor paths (removing all arrows leaving v1 reveals no remaining unblocked path from v1 to O).\n\n\nExercise 4\nNow, v2 is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of v2 on O?\n\n\n\nDAG exercise 4\n\n\nAnswer: Following the recipe, there are three unblocked paths left after removing the arrows leaving v2:\n\nv2 – v3 – O and\nv2 – v1 – E – O\nv2 - v1 – E -v5 - O\n\nBackdoor path a) can be closed by conditioning on v3.\nBackdoor path b) can be closed by conditioning on v1 (but not by conditioning on E, as you would no longer be estimating the total effect by blocking the paths from v2 to O mediated by E).\nIn this case, you should therefore condition on v1 and v3.\n\n\nExercise 5\nBack to the first DAG. However, v2 is now unmeasured. Can we still obtain an unconfounded estimate of the effect of E on O?\n\n\n\nDAG exercise 5\n\n\nAnswer: No, we cannot close the backdoor path between E and O since v2 is unmeasured and cannot be corrected for.\n\n\nExercise 6\nSee the DAG below: you adjusted for v5. What would be the consequence of this action?\n\n\n\nDAG exercise 6\n\n\nAnswer: There is no consequence: conditioning on v5 cannot alter any of the estimated effects in the DAG (it is neither a confounder, collider, nor a mediator in the E-O relationship)."
  },
  {
    "objectID": "lab6_answers.html",
    "href": "lab6_answers.html",
    "title": "Advanced medical Statistics – Answers lab 6",
    "section": "",
    "text": "Question 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nThe scatterplot suggests that there is a positive linear association between age and pocketdepth. As age increases, pocket depth tends to increase.\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  pockets$age and pockets$pocketdepth\nt = 3.9647, df = 98, p-value = 0.0001398\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1891831 0.5295346\nsample estimates:\n     cor \n0.371786 \n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nThe correlation coefficient is approximately 0.37, indicating a moderate positive linear relationship between age and pocketdepth. This aligns with the interpretation of the scatterplot.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nThe p-value for the correlation coefficient test is approximately 0.00014, indicating that we have sufficient evidence to reject the null hypothesis of no correlation between age and pocketdepth.\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pocketdepth ~ age, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.49289 -0.44376 -0.07903  0.49597  2.13335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.85872    0.22862  16.878  &lt; 2e-16 ***\nage          0.02054    0.00518   3.965  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7535 on 98 degrees of freedom\nMultiple R-squared:  0.1382,    Adjusted R-squared:  0.1294 \nF-statistic: 15.72 on 1 and 98 DF,  p-value: 0.0001398\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\nThe estimated coefficient for age (0.021) is statistically significant with a p-value of 0.00014, indicating that the relationship between age and pocketdepth is statistically significant at α = 0.05.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe p-value for age in the regression output (0.00014) and the p-value for the correlation coefficient test (0.00014) are consistent with each other. They both indicate a statistically significant relationship between age and pocketdepth.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nThe intercept (3.859) represents the estimated pocket depth when age is 0, which may not have a meaningful interpretation in this context. The coefficient for age (0.021) represents the estimated change in pocket depth for each additional year of age.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe expected pocket depth for a person who is 40 years old can be calculated using the intercept and coefficient from the regression output: 3.859 + 0.021 * 40 = 4.699 mm\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nThe R-squared value of 0.138 indicates that approximately 14% of the variation in pocket depth is explained by age in this model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nThe two plots suggest that the residuals are approximately normally distributed, with the histogram showing a roughly symmetric shape and the Q-Q plot showing the residuals closely following the diagonal line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\n\n\n\n\n\n\nAnswer question 10\n\n\n\nThe residual-versus-fitted plot suggests that the variance of the residuals is relatively constant across the range of fitted values, indicating homoscedasticity.\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?\n\n\n\n\n\n\n\n\nAnswer question 11\n\n\n\nThere is no obvious pattern or curvature in the residual-versus-fitted plot, indicating that the linearity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "lab6_answers.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Advanced medical Statistics – Answers lab 6",
    "section": "",
    "text": "Question 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nThe scatterplot suggests that there is a positive linear association between age and pocketdepth. As age increases, pocket depth tends to increase.\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  pockets$age and pockets$pocketdepth\nt = 3.9647, df = 98, p-value = 0.0001398\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1891831 0.5295346\nsample estimates:\n     cor \n0.371786 \n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nThe correlation coefficient is approximately 0.37, indicating a moderate positive linear relationship between age and pocketdepth. This aligns with the interpretation of the scatterplot.\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nThe p-value for the correlation coefficient test is approximately 0.00014, indicating that we have sufficient evidence to reject the null hypothesis of no correlation between age and pocketdepth.\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pocketdepth ~ age, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.49289 -0.44376 -0.07903  0.49597  2.13335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.85872    0.22862  16.878  &lt; 2e-16 ***\nage          0.02054    0.00518   3.965  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7535 on 98 degrees of freedom\nMultiple R-squared:  0.1382,    Adjusted R-squared:  0.1294 \nF-statistic: 15.72 on 1 and 98 DF,  p-value: 0.0001398\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\nThe estimated coefficient for age (0.021) is statistically significant with a p-value of 0.00014, indicating that the relationship between age and pocketdepth is statistically significant at α = 0.05.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe p-value for age in the regression output (0.00014) and the p-value for the correlation coefficient test (0.00014) are consistent with each other. They both indicate a statistically significant relationship between age and pocketdepth.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nThe intercept (3.859) represents the estimated pocket depth when age is 0, which may not have a meaningful interpretation in this context. The coefficient for age (0.021) represents the estimated change in pocket depth for each additional year of age.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe expected pocket depth for a person who is 40 years old can be calculated using the intercept and coefficient from the regression output: 3.859 + 0.021 * 40 = 4.699 mm\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nThe R-squared value of 0.138 indicates that approximately 14% of the variation in pocket depth is explained by age in this model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nThe two plots suggest that the residuals are approximately normally distributed, with the histogram showing a roughly symmetric shape and the Q-Q plot showing the residuals closely following the diagonal line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\n\n\n\n\n\n\nAnswer question 10\n\n\n\nThe residual-versus-fitted plot suggests that the variance of the residuals is relatively constant across the range of fitted values, indicating homoscedasticity.\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?\n\n\n\n\n\n\n\n\nAnswer question 11\n\n\n\nThere is no obvious pattern or curvature in the residual-versus-fitted plot, indicating that the linearity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-2-ancova-analysis-of-covariance",
    "href": "lab6_answers.html#part-2-ancova-analysis-of-covariance",
    "title": "Advanced medical Statistics – Answers lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\n\n\n\n\nAnswer question 12\n\n\n\nAs discussed previously, the scatterplot suggests a positive relationship between age and pocketdepth, with higher pocket depths observed for older individuals. It also suggests that there is a U-shaped relationship between alcohol consumption and pocketdepth, with individuals consuming “1-2 glasses/day” having lower pocket depths compared to those consuming “None” or “&gt;2 glasses/day”.\n\n\n\n\nFitting the ANCOVA Model\n\nR output\n\n\n\nCall:\nlm(formula = pocketdepth ~ age + alcohol, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35037 -0.45299 -0.06626  0.39746  1.76573 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             4.165461   0.270688  15.388  &lt; 2e-16 ***\nage                     0.014849   0.005248   2.829  0.00568 ** \nalcohol1–2 glasses/day -0.394551   0.172383  -2.289  0.02428 *  \nalcohol&gt;2 glasses/day   0.364565   0.188347   1.936  0.05586 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7069 on 96 degrees of freedom\nMultiple R-squared:  0.257, Adjusted R-squared:  0.2338 \nF-statistic: 11.07 on 3 and 96 DF,  p-value: 2.638e-06\n\n\n\n\nSPSS output\n\n\n\nScreenshot of the SPSS output tables\n\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nAnswer question 13\n\n\n\nBy default, the reference level for the alcohol variable is set to \"None\" in R and to \"&gt;2 glasses/day\" in SPSS.\nLooking at the R output, the coefficient for \"&gt;2 glasses/day\" is 0.365, which means that individuals who consume \"&gt;2 glasses/day\" are expected to have a pocket depth that is 0.365 mm higher than those who consume \"None\", while controlling for age.\nLooking at the SPSS output, the coefficient for \"None\" is -0.365, which means that individuals who consume \"None\" are expected to have a pocket depth that is 0.365 mm lower than those who consume \"&gt;2 glasses/day\", while controlling for age.\nTherefore, the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\" is 0.365 mm, independent of the reference level chosen.\n\n\n\n\n\n\n\n\nQuestion 14\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nAnswer question 14\n\n\n\nBy default, the reference level for the alcohol variable is set to \"None\" in R and to \"&gt;2 glasses/day\" in SPSS.\nLooking at the R output, the coefficient for \"&gt;2 glasses/day\" is 0.365 and the coefficient for \"1-2 glasses/day\" is -0.395. This means that individuals who consume \"&gt;2 glasses/day\" are expected to have a pocket depth that is 0.365 - -0.395 = 0.76 mm higher than those who consume \"1-2 glasses/day\", while controlling for age.\nLooking at the SPSS output, the coefficient for \"1-2 glasses/day\" is -0.759, which means that individuals who consume \"1-2 glasses/day\" are expected to have a pocket depth that is 0.759 mm lower than those who consume \"&gt;2 glasses/day\", while controlling for age.\nTherefore, the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\" is 0.76 mm, independent of the reference level chosen.\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 118.337  1 236.8042 &lt; 2.2e-16 ***\nage           4.001  1   8.0055 0.0056789 ** \nalcohol       7.669  2   7.6729 0.0008104 ***\nResiduals    47.974 96                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nQuestion 15\n\n\n\nBased on the ANOVA table, is the alcohol variable significantly associated with pocketdepth after accounting for age?\n\n\n\n\n\n\n\n\nAnswer question 15\n\n\n\nThe p-value for the alcohol variable in the ANOVA table is less than 0.05, indicating that the alcohol variable is significantly associated with pocketdepth after accounting for age.\n\n\n\n\n\nModel Diagnostics\n\n\n\n\n\n\nExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model.\n\n\n\n\n\n\n\n\nAnswer question 15\n\n\n\n\nNormality of Residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histogram and normal Q-Q plot of residuals shows that the residuals are approximately normally distributed, indicating that the normality assumption is not violated.\n\n\nHomoscedasticity\n\n\n\n\n\n\n\n\n\nThe residuals vs. fitted plot shows that the residuals are randomly scattered around zero, indicating that the homoscedasticity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-3-interactions-in-ancova",
    "href": "lab6_answers.html#part-3-interactions-in-ancova",
    "title": "Advanced medical Statistics – Answers lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\n\nFitting the Interaction Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age * alcohol, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35020 -0.45321 -0.06748  0.39755  1.76684 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 4.161e+00  3.811e-01  10.921   &lt;2e-16 ***\nage                         1.494e-02  7.747e-03   1.928   0.0569 .  \nalcohol1–2 glasses/day     -3.862e-01  5.142e-01  -0.751   0.4544    \nalcohol&gt;2 glasses/day       3.690e-01  6.538e-01   0.564   0.5738    \nage:alcohol1–2 glasses/day -2.082e-04  1.214e-02  -0.017   0.9863    \nage:alcohol&gt;2 glasses/day  -9.585e-05  1.395e-02  -0.007   0.9945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7144 on 94 degrees of freedom\nMultiple R-squared:  0.257, Adjusted R-squared:  0.2175 \nF-statistic: 6.503 on 5 and 94 DF,  p-value: 3.102e-05\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n            Sum Sq Df  F value  Pr(&gt;F)    \n(Intercept) 60.865  1 119.2592 &lt; 2e-16 ***\nage          1.897  1   3.7169 0.05688 .  \nalcohol      0.784  2   0.7678 0.46691    \nage:alcohol  0.000  2   0.0001 0.99985    \nResiduals   47.974 94                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?\n\n\n\n\n\n\n\n\nAnswer question 16\n\n\n\nThe p-value for the interaction term age:alcohol in the ANOVA table is approximately equal to 1, indicating that there is no interaction between age and alcohol in predicting pocketdepth."
  },
  {
    "objectID": "lab6_answers.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "lab6_answers.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Advanced medical Statistics – Answers lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions.\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nFitting the ANCOVA Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age + smoking, data = pockets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3995 -0.5308 -0.1453  0.5331  2.2462 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.89031    0.22660  17.168  &lt; 2e-16 ***\nage            0.01776    0.00534   3.326  0.00125 ** \nsmokingSmoker  0.32371    0.17709   1.828  0.07063 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7447 on 97 degrees of freedom\nMultiple R-squared:  0.1669,    Adjusted R-squared:  0.1497 \nF-statistic: 9.718 on 2 and 97 DF,  p-value: 0.0001423\n\n\nThe p-value for the smoking variable in the ANOVA table is 0.071, indicating that the smoking variable is not significantly associated with pocketdepth after accounting for age.\n\n\nModel Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots do not show any violations of the normality or homoscedasticity assumptions.\n\n\nFitting the Interaction Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age * smoking, data = pockets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4160 -0.5382 -0.1353  0.5514  2.2096 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.778052   0.264041  14.309  &lt; 2e-16 ***\nage                0.020622   0.006360   3.243  0.00163 ** \nsmokingSmoker      0.772509   0.567866   1.360  0.17690    \nage:smokingSmoker -0.009779   0.011755  -0.832  0.40751    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7459 on 96 degrees of freedom\nMultiple R-squared:  0.1729,    Adjusted R-squared:  0.147 \nF-statistic: 6.689 on 3 and 96 DF,  p-value: 0.0003787\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 113.894  1 204.7350 &lt; 2.2e-16 ***\nage           5.849  1  10.5145  0.001629 ** \nsmoking       1.029  1   1.8506  0.176898    \nage:smoking   0.385  1   0.6921  0.407507    \nResiduals    53.405 96                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for the interaction term age:smoking is 0.407, indicating that there is no significant interaction between age and smoking in predicting pocketdepth."
  },
  {
    "objectID": "lab3_answers.html",
    "href": "lab3_answers.html",
    "title": "Advanced medical Statistics – Answers lab 3",
    "section": "",
    "text": "# Create boxplot comparing birth weights for mothers who smoked and those who did not\nggplot(lowbwt, aes(x = smoke, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Smoking Status\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nBased on the boxplot, it appears that mothers who smoked have lower birth weights on average compared to those who did not smoke. This suggests that smoking status may indeed have an effect on birth weight.\n\n\n\n\n\n\n# Perform independent samples t-test\nindependent_t_test &lt;- t.test(bwt ~ smoke, data = lowbwt, var.equal = TRUE)\n\n# Print results\nprint(independent_t_test)\n\n\n    Two Sample t-test\n\ndata:  bwt by smoke\nt = 2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n  70.69274 492.73382\nsample estimates:\n mean in group no mean in group yes \n         3054.957          2773.243 \n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nThe p-value of the independent samples t-test (p=0.009) is less than 0.05, which indicates that there is a statistically significant difference in the mean birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n# Create histograms of birth weight by smoking status\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(binwidth = 200, fill = \"blue\", colour=\"black\", alpha = 0.7) +\n  facet_wrap(~smoke) +\n  labs(x = \"Birth Weight (grams)\", title = \"Histograms of Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nBased on the histograms, the distributions in both groups appear reasonably normal for the purposes of conducting an independent samples t-test.\n\n\n\n# Perform Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(bwt ~ smoke, data = lowbwt)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.3901 0.2399\n      187               \n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the Levene test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\nThe p-value of the Levene test (p = 0.2399) is greater than 0.05, indicating insufficient evidence to reject the null hypothesis of equal variances. Therefore, we can assume that the assumption of equal variances holds.\n\n\n\n\n\n\n# Calculate summary statistics by smoking status\nsummary_stats &lt;- lowbwt %&gt;%\n  group_by(smoke) %&gt;%\n  summarise(mean = mean(bwt), sd = sd(bwt), n = n())\nsummary_stats\n\n# A tibble: 2 × 4\n  smoke  mean    sd     n\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 no    3055.  752.   115\n2 yes   2773.  660.    74\n\n\n\n\n\n\n\n\nExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not. You may simplify the calculation by using the 97.5th percentile from the standard normal distribution (1.96) rather than the corresponding percentile from the t-distribution.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nPooled Standard Deviation\nThe pooled standard deviation is calculated using the formula:\n\\[\ns_p = \\sqrt{\\frac{(n_{no} - 1) \\cdot s_{no}^2 + (n_{yes} - 1) \\cdot s_{yes}^2}{n_{no} + n_{yes} - 2}}\n\\] Substituting the values:\n\\[\ns_p = \\sqrt{\\frac{(115 - 1) \\cdot 752^2 + (74 - 1) \\cdot 660^2}{115 + 74 - 2}} = 717.49\n\\]\nStandard Error of the Mean Difference\nThe standard error (SE) of the mean difference is calculated as:\n\\[\nSE = s_p \\cdot \\sqrt{\\frac{1}{n_{no}} + \\frac{1}{n_{yes}}}\n\\] Substituting the values:\n\\[\nSE = 717.49 \\cdot \\sqrt{\\frac{1}{115} + \\frac{1}{74}} = 106.93\n\\]\nMean Difference\nThe difference in means between the groups is:\n\\[\n\\text{Mean Difference} = \\bar{X}_{no} - \\bar{X}_{yes} = 3055 - 2773 = 282\n\\]\n95% Confidence Interval\nThe confidence interval (CI) is calculated as:\n\\[\n\\text{CI} = \\text{Mean Difference} \\pm 1.96 \\cdot SE\n\\] Substituting the values:\n\\[\n\\text{CI} = 282 \\pm 1.96 \\cdot 106.93 = (72.43, 491.57)\n\\]\n\n\n\n\n\nPooled Standard Deviation: 717.49\n\nStandard Error of the Mean Difference: 106.93\n\n95% Confidence Interval for the Mean Difference: (72.43, 491.57)\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe manually calculated 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not is (72.43, 491.57), is approximately equal to the one provided in the output of the t.test() function, wich is (70.69, 492.73). The latter is slightly wider due to the use of the 97.5 the percentile of the t-distribution with 187 degrees of freedom (1.97) rather than the standard normal distribution (1.96).\n\n\n\n\n\n\n# Perform Mann-Whitney U test\nwilcoxon_test &lt;- wilcox.test(bwt ~ smoke, data = lowbwt)\nwilcoxon_test\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by smoke\nW = 5243.5, p-value = 0.007109\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nThe null hypothesis for the Mann-Whitney U test is that there is no difference in the median birth weight between mothers who smoked and those who did not. The alternative hypothesis is that there is a difference in median birth weight between the two groups. The p-value of the test is 0.0071, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that there is a statistically significant difference in median birth weight between mothers who smoked and those who did not."
  },
  {
    "objectID": "lab3_answers.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "lab3_answers.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Advanced medical Statistics – Answers lab 3",
    "section": "",
    "text": "# Create boxplot comparing birth weights for mothers who smoked and those who did not\nggplot(lowbwt, aes(x = smoke, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Smoking Status\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nBased on the boxplot, it appears that mothers who smoked have lower birth weights on average compared to those who did not smoke. This suggests that smoking status may indeed have an effect on birth weight.\n\n\n\n\n\n\n# Perform independent samples t-test\nindependent_t_test &lt;- t.test(bwt ~ smoke, data = lowbwt, var.equal = TRUE)\n\n# Print results\nprint(independent_t_test)\n\n\n    Two Sample t-test\n\ndata:  bwt by smoke\nt = 2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n  70.69274 492.73382\nsample estimates:\n mean in group no mean in group yes \n         3054.957          2773.243 \n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nThe p-value of the independent samples t-test (p=0.009) is less than 0.05, which indicates that there is a statistically significant difference in the mean birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n# Create histograms of birth weight by smoking status\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(binwidth = 200, fill = \"blue\", colour=\"black\", alpha = 0.7) +\n  facet_wrap(~smoke) +\n  labs(x = \"Birth Weight (grams)\", title = \"Histograms of Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nBased on the histograms, the distributions in both groups appear reasonably normal for the purposes of conducting an independent samples t-test.\n\n\n\n# Perform Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(bwt ~ smoke, data = lowbwt)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.3901 0.2399\n      187               \n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the Levene test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\nThe p-value of the Levene test (p = 0.2399) is greater than 0.05, indicating insufficient evidence to reject the null hypothesis of equal variances. Therefore, we can assume that the assumption of equal variances holds.\n\n\n\n\n\n\n# Calculate summary statistics by smoking status\nsummary_stats &lt;- lowbwt %&gt;%\n  group_by(smoke) %&gt;%\n  summarise(mean = mean(bwt), sd = sd(bwt), n = n())\nsummary_stats\n\n# A tibble: 2 × 4\n  smoke  mean    sd     n\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 no    3055.  752.   115\n2 yes   2773.  660.    74\n\n\n\n\n\n\n\n\nExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not. You may simplify the calculation by using the 97.5th percentile from the standard normal distribution (1.96) rather than the corresponding percentile from the t-distribution.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nPooled Standard Deviation\nThe pooled standard deviation is calculated using the formula:\n\\[\ns_p = \\sqrt{\\frac{(n_{no} - 1) \\cdot s_{no}^2 + (n_{yes} - 1) \\cdot s_{yes}^2}{n_{no} + n_{yes} - 2}}\n\\] Substituting the values:\n\\[\ns_p = \\sqrt{\\frac{(115 - 1) \\cdot 752^2 + (74 - 1) \\cdot 660^2}{115 + 74 - 2}} = 717.49\n\\]\nStandard Error of the Mean Difference\nThe standard error (SE) of the mean difference is calculated as:\n\\[\nSE = s_p \\cdot \\sqrt{\\frac{1}{n_{no}} + \\frac{1}{n_{yes}}}\n\\] Substituting the values:\n\\[\nSE = 717.49 \\cdot \\sqrt{\\frac{1}{115} + \\frac{1}{74}} = 106.93\n\\]\nMean Difference\nThe difference in means between the groups is:\n\\[\n\\text{Mean Difference} = \\bar{X}_{no} - \\bar{X}_{yes} = 3055 - 2773 = 282\n\\]\n95% Confidence Interval\nThe confidence interval (CI) is calculated as:\n\\[\n\\text{CI} = \\text{Mean Difference} \\pm 1.96 \\cdot SE\n\\] Substituting the values:\n\\[\n\\text{CI} = 282 \\pm 1.96 \\cdot 106.93 = (72.43, 491.57)\n\\]\n\n\n\n\n\nPooled Standard Deviation: 717.49\n\nStandard Error of the Mean Difference: 106.93\n\n95% Confidence Interval for the Mean Difference: (72.43, 491.57)\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe manually calculated 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not is (72.43, 491.57), is approximately equal to the one provided in the output of the t.test() function, wich is (70.69, 492.73). The latter is slightly wider due to the use of the 97.5 the percentile of the t-distribution with 187 degrees of freedom (1.97) rather than the standard normal distribution (1.96).\n\n\n\n\n\n\n# Perform Mann-Whitney U test\nwilcoxon_test &lt;- wilcox.test(bwt ~ smoke, data = lowbwt)\nwilcoxon_test\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by smoke\nW = 5243.5, p-value = 0.007109\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nThe null hypothesis for the Mann-Whitney U test is that there is no difference in the median birth weight between mothers who smoked and those who did not. The alternative hypothesis is that there is a difference in median birth weight between the two groups. The p-value of the test is 0.0071, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that there is a statistically significant difference in median birth weight between mothers who smoked and those who did not."
  },
  {
    "objectID": "lab3_answers.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "lab3_answers.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Advanced medical Statistics – Answers lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\n\nExplaratory data analysis\n\n# Create boxplot comparing birth weights across ethnic groups\nggplot(lowbwt, aes(x = ethnicity, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Ethnicity\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Ethnicity\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe boxplot suggests that there may be differences in birth weight across the three ehtnic groups. More specifically, The mean birth weight appears to be higher for the Causasian group compared to the Afro-American and Asian groups.\n\n\n\n\nOne-way ANOVA\n\n# Perform One-Way ANOVA\nanova_result &lt;- aov(bwt ~ ethnicity, data = lowbwt)\n\n# Print summary of ANOVA\nsummary(anova_result)\n\n             Df   Sum Sq Mean Sq F value  Pr(&gt;F)   \nethnicity     2  5070608 2535304   4.972 0.00788 **\nResiduals   186 94846445  509927                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nThe p-value from the one-way ANOVA (p=0.0079) is less than 0.05, indicating that there is at least one ethic group with a significantly different birth weight.\n\n\n\n\nPost-hoc tests\n\n# Perform pairwise comparisons with Bonferroni correction\nposthoc &lt;- pairwise.t.test(lowbwt$bwt, lowbwt$ethnicity, p.adjust.method = \"bonferroni\")\nposthoc\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  lowbwt$bwt and lowbwt$ethnicity \n\n              Caucasian Afro-American\nAfro-American 0.048     -            \nAsian         0.027     1.000        \n\nP value adjustment method: bonferroni \n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nWhat conclusions can be drawn from the post-hoc comparisons?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nIt follows from the post-hoc comparisons with the Bonferroni-adjusted p-values that the mean birth weight of Caucasian infants is significantly different from that of Afro-American infants (p=0.048) and Asian infants (p=0.027). There is no significant difference in mean birth weight between Afro-American and Asian infants (p=1).\n\n\n\n\nKruskal-Wallis Test\nIf the assumptions of the one-way ANOVA are violated, we can use the Kruskal-Wallis test as a non-parametric alternative. The test can be performed in R using the kruskal.test() function:\n\n# Perform Kruskal-Wallis test\nkruskal_test &lt;- kruskal.test(bwt ~ ethnicity, data = lowbwt)\nkruskal_test\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  bwt by ethnicity\nKruskal-Wallis chi-squared = 8.5909, df = 2, p-value = 0.01363\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?\n\n\n\n\n\n\n\n\nAnswer question 10\n\n\n\nThe p-value from the Kruskal-Wallis test (p=0.014) is consistent with the one-way ANOVA results, indicating that there is at least one ethnic group with a significantly different median birth weight."
  },
  {
    "objectID": "lab3_answers.html#part-3-unguided-exercises",
    "href": "lab3_answers.html#part-3-unguided-exercises",
    "title": "Advanced medical Statistics – Answers lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nExample solution\n\n# Create boxplot comparing birth weights by history of hypertension\nggplot(lowbwt, aes(x = ht, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"History of Hypertension\", y = \"Birth Weight (grams)\", title = \"Birth Weight by History of Hypertension\")\n\n\n\n\n\n\n\n# Perform independent samples t-test\nt_test &lt;- t.test(bwt ~ ht, data = lowbwt, equal.var = TRUE)\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  bwt by ht\nt = 1.6124, df = 11.908, p-value = 0.133\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -153.4955 1024.6170\nsample estimates:\n mean in group no mean in group yes \n         2972.311          2536.750 \n\n# Create histograms to check normality\nggplot(lowbwt, aes(x = bwt, fill = ht)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 20) +\n  facet_wrap(~ht) +\n  labs(x = \"Birth Weight (grams)\", y = \"Frequency\", title = \"Distribution of Birth Weight by History of Hypertension\")\n\n\n\n\n\n\n\n# Perform Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(bwt ~ ht, data = lowbwt)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.2851 0.2584\n      187               \n\n\nEvaluation of the results: Although the p-value from the independent samples t-test suggests that there is no statistically significant difference in birth weight between mothers with and without a history of hypertension, the group sizes are highly imbalanced (177 vs. 12). This imbalance reduces the reliability of the t-test results and may limit its power to detect true differences. Additionally, the small sample size in the hypertension group makes it harder to assess assumptions such as normality. To check the robustness of the findings, we also conduct the Mann-Whitney U test as a non-parametric alternative.\n\n# Perform Mann-Whitney U test\nwilcox_test &lt;- wilcox.test(bwt ~ ht, data = lowbwt)\nwilcox_test\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by ht\nW = 1350, p-value = 0.1169\nalternative hypothesis: true location shift is not equal to 0\n\n\nOverall conclusion: Both the independent samples t-test and the Mann-Whitney U test suggest that there is no statistically significant difference in birth weight between mothers with and without a history of hypertension. This consistency across methods supports the robustness of the findings, despite the imbalance in group sizes.\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav (available on Brightspace) contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?\n\n\n\n\n\nExample solution\n\nExploratory data analysis\n\n\n\n\n\n\n\n\n\nThe groups seem to be different with respect to their means, but also with respect to their variation (range has different length)\n\n\nPerform a one way ANOVA, and interpret the results. Are the conditions satisfied?\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  15516    7758   3.711 0.0436 *\nResiduals   19  39716    2090                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  2  3.6413 0.04585 *\n      19                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA reveals significant results P= 0.044; this means that the null hypothesis of equal means in the three groups is rejected. However, according to the test of homogeneity of variances, the assumption of equal variances is violated (P-value = 0.044).\n\n\nTry a log transformation on the data, and perform again a one-way ANOVA. Are now the assumptions satisfied?\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2 0.1784 0.08922   3.539 0.0494 *\nResiduals   19 0.4790 0.02521                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   1.889 0.1785\n      19               \n\n\nThe test of equal variances for the log-transformed data reveals that the assumption of equal variances can be made. Test results for the ANOVA: P-value = 0.049. This is on the boundary of significance. There is indication that at least one pair of groups have different means.\n\n\nWhich means do differ according to you? Why?\n\n# Untransformed data\npairwise.t.test(df$rcfl, df$group, p.adj = \"bonf\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$rcfl and df$group \n\n        Group 1 Group 2\nGroup 2 0.042   -      \nGroup 3 0.464   1.000  \n\nP value adjustment method: bonferroni \n\n# Log-transformed data\npairwise.t.test(df$log_rcfl, df$group, p.adj = \"bonf\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$log_rcfl and df$group \n\n        Group 1 Group 2\nGroup 2 0.047   -      \nGroup 3 0.597   1.000  \n\nP value adjustment method: bonferroni \n\n\nFrom the paired comparisons with Bonferroni-corrections it appears that group1 and group 2 differ significantly with respect to their means (but on the boundary, P-value = 0.047).\n\nTry a non-parametric approach on these data. What are now your conclusions?\n\n\nkruskal.test(rcfl ~ group, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rcfl by group\nKruskal-Wallis chi-squared = 4.1852, df = 2, p-value = 0.1234\n\n\nThe Kruskal-Wallis test gives P-value = 0.123, indicating that there are no significant differences between the three groups.\n\n\nCaveat\nIn this example, the sample size is very limited, making it difficult — if not impossible — to verify whether the assumptions underlying the one-way ANOVA are met. The Kruskal-Wallis test is a good alternative in such cases."
  },
  {
    "objectID": "lab1_answers.html",
    "href": "lab1_answers.html",
    "title": "Advanced medical Statistics – Answers lab 1",
    "section": "",
    "text": "Part 1 - Descriptive statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nFrom the histogram, it follows that the distribution of age is right-skewed. In such a situation, we would generally prefer reporting median [IQR] or median [Q1, Q3] over mean (SD).\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, and IQR for the variable lwt. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report.\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nWe start by making a histogram of the variable lwt:\n\n\n\n\n\n\n\n\n\nFrom the histogram, it follows that the distribution of lwt is right-skewed. We therefore prefer reporting median [IQR] or median [Q1, Q3], depending on the style conventions of the journal (typically the latter in the medical field). The median [IQR] is equal to 121 [30]. The first and third quartiles, calculated using Analyze -&gt; Explore in SPSS or quantile(lowbwt$lwt, probs = c(0.25, 0.75), na.rm = TRUE) in R, are equal to [110, 140].\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable history of hypertension (ht)\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\n\n\n\nht\nfrequency\npercentage\n\n\n\n\n0\n177\n93.7%\n\n\n1\n12\n6.3%\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension.\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 - Probability calculations for random variables\n\n\n\n\n\n\nQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nIf we let the random variable X denote the number of blood donors of blood group B, it follows from the text in the exercise that X ~ Bin(0.08, 100). Fewer than 1,500 ml of group B blood will be obtained if X is equal to or smaller than 2. We can therefore answer this question by calculating \\(P(X \\leq 2)\\) using the cumulative distribution function of the binomial distribution, which gives a probability of 0.01 or 1%.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution).\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nIf we let the random variable X denote the number of individuals in treatment group A, we get a difference between the number of patients in the two treatment groups of over 20 if \\(X &lt; 40\\) or if \\(X &gt; 60\\). Using the cumulative distribution function of the binomial distribution, these probabilities are equal to \\(P(X &lt; 40) = 0.0176\\) and \\(P(X &gt; 60) = 1 - P(X \\leq 60) = 0.0176\\). The probability that the difference between the number of patients in the two treatment groups exceeds 20 is therefore equal to 2*0.0176 = 0.035 or 3.5%\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nAt the start, the minimum height was \\((172 - 175.8) / 5.84 = -0.65\\) standard deviations from the mean. From the cumulative distribution function of the standard normal distribution, the proportion of men smaller than this would be about \\(26\\%\\). At the end of the \\(25\\)-year period, the minimum height was \\((172 - 179.1) / 5.84 = -1.22\\) standard deviations from the mean. From the cumulative distribution function of the standard normal distribution, the proportion of men below the minimum is now about \\(11\\%\\). Thus, the proportion of ineligible men has more than halved.\n\n\n\n\nPart 3 - Some conceptual questions\n\n\n\n\n\n\nQuestion 8\n\n\n\nImagine we have some observations on blood pressure and calculate the mean, standard deviation, median and IQR. How do these measures change if all observations are\n\nincreased by 10\ndecreased by their mean\nmultiplied by 10\ndivided by their standard deviation\n\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\n\nStandard deviation and IQR remain the same, mean and median increase by 10.\nStandard deviation and IQR remain the same, mean and median decrease by the mean (the mean becomes zero).\nAll are multiplied by 10.\nAll are divided by standard deviation; the standard deviation becomes one.\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nTwo fair dice are rolled, the results are X1 and X2.\n\nWhat is the probability Prob(X1=X2)?\nWhat is the expected value E(X1), and the standard deviation SD(X1)?\nGive the expected value and the variance of X1+X2 and of X1-X2.\n\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nQuestion (a)\nAssume the dice have different colors, red (R) and blue (B). \\(P(R = 1) = \\frac{1}{6}\\), \\(P(R = 2) = \\frac{1}{6}\\), etc. The outcomes of Red and Blue are independent of each other. So for example: \\(P(R = 1 \\text{ and } B = 4) = P(R = 1) \\times P(B = 4) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}\\).\nThis is true for all possible combinations of outcomes. There are only \\(6\\) possible outcomes with equal results: \\(R = 1\\) and \\(B = 1\\), \\(R = 2\\) and \\(B = 2\\), and so on. In total: \\(\\text{Prob} = 6 \\cdot (\\frac{1}{6} \\cdot \\frac{1}{6}) = 6 \\times \\frac{1}{36} = \\frac{1}{6}\\).\nQuestion (b)\nUsing the formula on page 3.3 of the syllabus:\n\\(E(X_1) = \\frac{1}{6} \\cdot (1 + 2 + 3 + 4 + 5 + 6) = 3.5\\)\n\\(\\text{Var}(X_1) = \\frac{1}{6} \\cdot [(1 - 3.5)^2 + (2 - 3.5)^2 + \\ldots] = 2.92\\)\n\\(\\text{SD} = \\sqrt{2.92} = 1.71\\)\nQuestion (c) Using the calculation rules on page 3.5 of the syllabus:\n\\(E(X_1 + X_2) = E(X_1) + E(X_2) = 3.5 + 3.5 = 7\\)\n\\(\\text{Var}(X_1 + X_2) = 2.92 + 2.92 = 5.84\\)\n\\(E(X_1 - X_2) = 0\\)\n\\(\\text{Var}(X_1 - X_2) = 2.92 + 2.92 = 5.84\\)\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nThere are two hospitals in town. On average 45 deliveries take place each day in the larger hospital, and 15 in the smaller. The probability of a baby being a boy is about 0.52, and the probability of twins is about 0.012. On any day, which hospital is more likely\n\nto have a set of twins delivered\nto have more than 60 % of babies being boys?\n\n\n\n\n\n\n\n\n\nAnswer question 10\n\n\n\n\nThe larger hospital, simply because there are more births. (This question is about an absolute number: a set of twins).\nThe day-to-day variation in the proportion of boys will be greater in the smaller hospital, so it is more likely to have more than 60% of babies being boys on any day. (This question is about a relative number: 60% of the babies).\n\nTo make the differences more clear: consider a very small hospital (only one delivery per day) and a very large hospital (1000 deliveries a day).\n\nThe probability that the delivery in the small hospital will concern twins equals 0.012. The same is true for each delivery in the larger hospital, but since we have 1000 deliveries, we expect 12 of them to concern twins.\nIn the small hospital, the probability that the child is a boy is 0.52. If a boy is born, then 100% of the babies that day are boys. This will happen in 52% of the days. For the larger hospital, we expect to see 520 boys and 480 girls on an arbitrary day. Some days there will be more than 520 boys, and other days there will be fewer. However, the probability that more than 60% of the children (more than 600 out of 1000) will be boys is very small—for sure less than 0.52.\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nThe probability of a baby being a boy is 0.52. For six women delivering consecutively in the same labour ward on one day, which of the following exact sequences of boys and girls is most likely and which least likely?\nGBGBGB\nBBBGGG\nGBBBBB\n\n\n\n\n\n\n\n\nAnswer question 11\n\n\n\nAs the probability of a boy is slightly greater than the probability of a girl, the sequence with the most boys is most likely, which is the last sequence. The other two sequences are equally likely."
  },
  {
    "objectID": "Assignment_1.html",
    "href": "Assignment_1.html",
    "title": "Advanced Medical Statistics",
    "section": "",
    "text": "In this assignment, you will work with an individualized dataset derived from the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "Assignment_1.html#introduction",
    "href": "Assignment_1.html#introduction",
    "title": "Advanced Medical Statistics",
    "section": "",
    "text": "In this assignment, you will work with an individualized dataset derived from the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "Assignment_1.html#dataset-description",
    "href": "Assignment_1.html#dataset-description",
    "title": "Advanced Medical Statistics",
    "section": "Dataset description",
    "text": "Dataset description\nThe Cleveland heart disease dataset originates from the Cleveland Clinic Foundation and focuses on heart disease diagnosis. It includes data from 303 patients on the following variables:\n\nage: Age in years\nsex: Sex (1 = female; 2 = male)\ncp: Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\ntrestbps: Resting blood pressure (mm Hg at hospital admission)\nchol: Serum cholesterol in mg/dl\nfbs: Fasting blood sugar &gt; 120 mg/dl\nrestecg: Resting electrocardiographic results (1 = normal; 2 = ST-T wave abnormality; 3 = left ventricular hypertrophy)\nthalach: Maximum heart rate achieved\nexang: Exercise-induced angina (1 = no; 2 = yes)\noldpeak: ST depression induced by exercise relative to rest\nslope: Slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\nca: Number of major vessels (0-3) colored by fluoroscopy\nthal: Thallium heart scan results (1 = normal; 2 = fixed defect; 3 = reversible defect)\ntarget: Diagnosis of heart disease (1 = heart disease; 2 = no heart disease)"
  },
  {
    "objectID": "Assignment_1.html#objectives",
    "href": "Assignment_1.html#objectives",
    "title": "Advanced Medical Statistics",
    "section": "Objectives",
    "text": "Objectives\nIn this assignment, you will explore the Cleveland heart disease dataset to answer the following research questions:\n\nDoes maximum heart rate achieved differ across chest pain types?\nIs the presence of heart disease associated with sex or fasting blood sugar?"
  },
  {
    "objectID": "Assignment_1.html#steps-to-complete-the-assignment",
    "href": "Assignment_1.html#steps-to-complete-the-assignment",
    "title": "Advanced Medical Statistics",
    "section": "Steps to complete the assignment",
    "text": "Steps to complete the assignment\n\nStep 1: Create an individualized dataset\nDownload the Cleveland heart disease dataset from Brightspace. After loading the dataset into R or SPSS, follow the steps below to create an individualized dataset with 200 patients:\n\n\n\n\n\n\nImportant\n\n\n\nYou must use your individualized dataset for all analyses in this assignment. Each dataset is uniquely sampled based on your student or staff number, ensuring that every student works independently with a unique dataset. This approach also ensures that results remain reproducible and can be individually verified by the instructor.\n\n\n\nInstructions for R users\n\nLoad the dataset into R\nSet a random seed using your student or staff number\n\nRemove any letters (e.g., S or P) and use the numeric part\nExample: set.seed(123456) for student number S123456\n\nRandomly sample 200 patients from the dataset\n\nUse: heart_data &lt;- heart_data[sample(nrow(heart_data), 200), ]\n\nVerify the dataset contains exactly 200 rows\n\nExample: nrow(heart_data) should return 200\n\n\n\n# Load the required libraries\nlibrary(haven)\nlibrary(dplyr)\n\n# Load the SPSS file\nheart_data &lt;- read_sav(\"heart_disease_cleveland.sav\")\n\n# Convert all variables to factors where needed\nheart_data &lt;- heart_data %&gt;% mutate(across(where(is.labelled), as_factor))\n\n# Set the random seed (replace 123456 with your student or staff number without S or P)\nset.seed(123456)\n\n# Create an individualized dataset with 200 patients\nheart_data &lt;- heart_data[sample(nrow(heart_data), 200), ]\n\n# Verify the dataset contains exactly 200 rows\nnrow(heart_data)\n\n\n\nInstructions for SPSS users\n\nOpen the dataset in SPSS\nOpen the Syntax Editor in SPSS (File → New → Syntax)\nCopy and paste the syntax from the box below into the Syntax Editor, replacing 123456 with your numeric student or staff number without any letters (e.g., S or P)\nRun the syntax to create a custom dataset with 200 patients\n\nGo to Run → All to run the syntax\n\nVerify the dataset contains exactly 200 cases\n\nGo to Analyze → Descriptive Statistics → Frequencies\nSelect the sex variable and click OK\nThe output should show 200 cases\n\nSave the dataset as a new file for further analysis\n\n\n* Set random seed (replace 123456 with your student or staff number without any letters).\nSET SEED = 123456.\n\n* Initialize sampling counters for 200 samples from 303 cases.\nDO IF $CASENUM = 1.\n  COMPUTE #s_$_1 = 200.\n  COMPUTE #s_$_2 = 303.\nEND IF.\n\n* Perform sampling logic.\nDO IF #s_$_2 &gt; 0.\n  COMPUTE filter_$ = RV.UNIFORM(0, 1) * #s_$_2 &lt; #s_$_1.\n  COMPUTE #s_$_1 = #s_$_1 - filter_$.\n  COMPUTE #s_$_2 = #s_$_2 - 1.\nELSE.\n  COMPUTE filter_$ = 0.\nEND IF.\n\n* Apply filter to retain sampled cases.\nVARIABLE LABELS filter_$ '200 from the first 303 cases (SAMPLE)'.\nFORMATS filter_$ (f1.0).\nFILTER BY filter_$.\nEXECUTE.\n\n\n\n\n\n\n\nImportant note for SPSS users\n\n\n\nThe syntax above will generate a custom dataset containing 200 patients randomly selected from the original dataset. The selected patients are identified using the filter variable filter_$, which is added as a column in the dataset.\nTo reapply the filter when you reopen the dataset, follow these steps:\n\nGo to Data → Select Cases.\nChoose Use filter variable and select filter_$ from the list.\nClick OK to apply the filter.\n\nPlease note that this filtering step must be repeated each time you open the dataset to ensure that only the selected patients are included in your analysis.\n\n\n\n\n\nStep 2: Create a baseline characteristics table\n\nInclude all variables in the dataset apart from the outcome variable target\n\nSummarize demographic variables (e.g., age, sex)\nSummarize clinical variables (e.g., chol, trestbps, thalach, cp)\n\nDecide on suitable summary measures for each variable\n\nUse appropriate measures for continuous variables (e.g., mean, standard deviation, median, interquartile range)\nUse frequency counts and percentages for categorical variables\n\nPresent your table clearly\n\nUse meaningful labels, headings, and clear formatting\n\n\n\n\nStep 3: Perform the analysis for the first research question\nResearch Question: Does maximum heart rate achieved differ across chest pain types?\n\nVisualize the data\n\nCreate a boxplot to compare the distribution of maximum heart rate achieved across the four chest pain types\n\nCalculate the estimated population means and 95% confidence intervals for maximum heart rate achieved for each of the four chest pain types\nSelect and perform an appropriate test\n\nUse one-way ANOVA if normality and equal variances are met\nUse Kruskal-Wallis test if assumptions are violated\nPerform post-hoc comparisons using Bonferroni adjusted p-values if significant differences are found\n\n\n\n\nStep 4: Perform the analysis for the second research question\nResearch Question: Is the presence of heart disease associated with sex or fasting blood sugar?\n\nSummarize the data\n\nCalculate and report the prevalence of heart disease for each group within sex and fasting blood sugar\nInclude percentages and 95% confidence intervals for each group\n\nSelect and perform an appropriate test\n\nUse a Chi-Square test of homogeneity or Fisher’s Exact Test, depending on the expected cell counts\n\n\n\n\nStep 5: Write a report\nYour report should be structured in the form of Methods and Results sections, as typically encountered in scientific papers.\n\nMethods\n\nOutline the steps taken to analyze the data\nDescribe statistical tests performed, assumptions checked, and adjustments applied\n\nResults\n\nInclude the baseline characteristics table\nPresent key findings for each research question\nInclude visualizations (e.g., boxplots) to support your findings where applicable\n\nFormatting guidelines\n\nProperly label all tables and figures\nLimit the report to 2–3 pages, including visuals and tables"
  },
  {
    "objectID": "Assignment_1.html#reporting-examples",
    "href": "Assignment_1.html#reporting-examples",
    "title": "Advanced Medical Statistics",
    "section": "Reporting examples",
    "text": "Reporting examples\nWhen presenting your analysis results, ensure clarity and adherence to proper reporting conventions. Use the following examples as a guide:\n\nThe mean cholesterol levels (95% CI) for the four chest pain types were as follows: typical angina, 245.3 mg/dL (95% CI: 230.1, 260.5); atypical angina, 220.4 mg/dL (95% CI: 205.7, 235.1); non-anginal pain, 230.2 mg/dL (95% CI: 215.6, 244.8); and asymptomatic chest pain, 200.1 mg/dL (95% CI: 185.4, 214.8). A one-way ANOVA was conducted to compare cholesterol levels across these groups, revealing a significant difference, F(3, 299) = 4.32, p = 0.006. Post-hoc pairwise comparisons using Bonferroni-adjusted p-values indicated that patients with typical angina had significantly higher cholesterol levels compared to those with asymptomatic chest pain (adjusted p = 0.015). No other pairwise differences were statistically significant after adjustment.\n\n\nThe prevalence of heart disease was higher among patients older than 65 (68.5%, 95% CI: 60.2%, 76.8%) compared to those 65 or younger (47.2%, 95% CI: 35.6%, 58.8%). Fisher’s Exact Test indicated a significant difference between these groups (p = 0.028)."
  },
  {
    "objectID": "Assignment_1.html#submission-instructions",
    "href": "Assignment_1.html#submission-instructions",
    "title": "Advanced Medical Statistics",
    "section": "Submission instructions",
    "text": "Submission instructions\nSubmit the following files as part of your assignment:\n\nThe report: Provide your report in Word or PDF format\nThe analysis file: Include your R script or SPSS output file"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R/SPSS labs for the Advanced Statistics course",
    "section": "",
    "text": "This website hosts the R and SPSS labs for the Advanced Statistics course. You can navigate between the different labs using the menu above."
  },
  {
    "objectID": "lab2_answers.html",
    "href": "lab2_answers.html",
    "title": "Advanced medical Statistics – Answers lab 2",
    "section": "",
    "text": "Welcome to lab 2 in the advanced medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can download from Brightspace."
  },
  {
    "objectID": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Advanced medical Statistics – Answers lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Means",
    "text": "Point Estimates and 95% Confidence Intervals for Population Means\nWe will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nThe mean, standard deviation, and standard error of the mean of the variable ‘birth weight in grams’ (bwt) are as follows:\n\nMean: 2944.7\nStandard deviation: 729\nStandard error (SE): 53.03\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\nThe estimated mean birth weight for the population is 2944.7 grams.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nManually calculate the corresponding 95% confidence interval based on the normal approximation.\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nUsing the normal approximation, the 95% confidence interval for the population mean birth weight can be calculated as mean ± 1.96 * SE, where the standard error (SE) is equal to 53.03. Therefore, the 95% confidence interval is (2840.7, 3048.6).\n\n\nYou can also use SPSS/R to calculate this 95% confidence interval by using the t-distribution. This gives the following result: (2840, 3049.3)\n\n\n\n\n\n\nQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nThe 95% confidence interval that uses the t-distribution is sightly wider compared to the 95% confidence interval that was manually computed using the normal approximation. However, given that the sample size is relatively large (n = 189), the difference is minimal. Larger differences would be expected with smaller sample sizes."
  },
  {
    "objectID": "lab2_answers.html#one-sample-t-test",
    "href": "lab2_answers.html#one-sample-t-test",
    "title": "Advanced medical Statistics – Answers lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\n\n\n\n    One Sample t-test\n\ndata:  lowbwt$bwt\nt = -1.0437, df = 188, p-value = 0.298\nalternative hypothesis: true mean is not equal to 3000\n95 percent confidence interval:\n 2840.049 3049.264\nsample estimates:\nmean of x \n 2944.656 \n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe degrees of freedom (df) for the one-sample t-test is equal to (sample size - 1), so df = 189 - 1 = 188 in this case.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe value of t-statistic is -1.04, and the corresponding p-value is 0.298. Since the p-value is greater than the significance level of 0.05, we fail to reject the null hypothesis. Therefore, the current sample does not provide sufficient evidence to conclude that the population mean significantly differs from 3000.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nBased on the histogram, the data appears to be reasonably normally distributed."
  },
  {
    "objectID": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Advanced medical Statistics – Answers lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable that takes a value 1 if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of 0 otherwise.\nThe frequency table for this variable is as follows:\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n130\n59\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe estimated proportion of low birth weight babies in the population is 59 / (130 + 59) = 0.31.\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation.\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nUsing the sample proportion p, the standard error (SE) can be calculated as \\(\\sqrt(p * (1 - p) / n)\\), where n is the total number of observations. The 95% confidence interval can then be calculated as p ± 1.96 * SE. In this case, the SE is equal to 0.03. Therefore, the 95% confidence interval is (0.24, 0.38)."
  },
  {
    "objectID": "lab2_answers.html#binomial-test",
    "href": "lab2_answers.html#binomial-test",
    "title": "Advanced medical Statistics – Answers lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%:\n\nR results\n\n\n\n    Exact binomial test\n\ndata:  sum(lowbwt$low == 1, na.rm = TRUE) and length(na.omit(lowbwt$low))\nnumber of successes = 59, number of trials = 189, p-value = 0.7509\nalternative hypothesis: true probability of success is not equal to 0.3\n95 percent confidence interval:\n 0.2468886 0.3834546\nsample estimates:\nprobability of success \n             0.3121693 \n\n\n\n\nSPSS results\n\n\n\nScreenshot of the SPSS output table\n\n\nBecause the binomial distribution with n=189 and p=0.3 is reasonably symmetric, the two-sided p-value provided by SPSS is close to the one provided by R. In the answers below, we will use the R results.\n\n\n\n\n\n\nQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nThe observed number of successes is 59, and the total number of trials is 189. Assuming a null hypothesis proportion of 30%, the expected number of successes is 0.3 * 189 = 56.7. The observed number of successes is relatively close to the expected number of successes under the null hypothesis, resulting in a p-value of the binomial test of 0.75. Since this p-value is greater than the significance level of 0.05, we fail to reject the null hypothesis. Therefore, the current sample does not provide sufficient evidence to conclude that the proportion of low birth weight babies differs significantly from 30%.\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50% of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%.\n\n\n\n\n\n\n\n\nAnswer question 10\n\n\n\nThe observed number of successes is 128, and the total number of trials is 200. Assuming a null hypothesis proportion of 50%, the expected number of successes is 0.5 * 200 = 100. The observed number of successes is considerably higher than the expected number of successes under the null hypothesis, resulting in a p-value of the binomial test that is &lt;0.001:\n\nbinom_test_result &lt;- binom.test(128, 200, p = 0.50)\nbinom_test_result\n\n\n    Exact binomial test\n\ndata:  128 and 200\nnumber of successes = 128, number of trials = 200, p-value = 9.13e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5692861 0.7064953\nsample estimates:\nprobability of success \n                  0.64 \n\n\nTherefore, we reject the null hypothesis and conclude that the proportion of Dutch adolescents who drink alcohol regularly is significantly higher than 50%.\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nAnswer question 11\n\n\n\nTo manually calculate an approximate p-value using the normal approximation, we first calculate the sample proportion \\(p\\) as the number of successes divided by the total number of trials (i.e., \\(p\\) = 128 / 200 = 0.64). We then calculate the standard error (SE) as \\(\\sqrt(\\pi * (1 - \\pi) / n)\\), where \\(n\\) is the total number of observations and \\(\\pi\\) is the null hypothesis proportion. This gives a SE of 0.035.\nUnder the null hypothesis, the sample proportion is approximately Normally distributed with mean \\(\\pi\\) = 0.5 and SE = 0.035. To calculate the probability of observing a sample proportion at least as extreme as the one observed in the data, we can calculate the z-score as \\((p - \\pi) / SE\\). In this case, the z-score is 3.96, resulting in a p-value &lt;0.001, which is consistent with the p-value obtained from the binomial test.\nThe use of the normal approximation is very reasonable in this case because \\(n * \\pi = 200 * 0.5 =100\\) is considerably larger than 5, which is the minimum requirement for the normal approximation to be valid."
  },
  {
    "objectID": "lab4_answers.html",
    "href": "lab4_answers.html",
    "title": "Advanced Medical Statistics – Answers lab 4",
    "section": "",
    "text": "Question 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nAnswer question 1\n\n\n\n\nStep 1: Extract the data\n\nSmokers with complications: \\(x_1 = 8\\), Total smokers: \\(n_1 = 20\\)\nNon-smokers with complications: \\(x_2 = 10\\), Total non-smokers: \\(n_2 = 60\\)\n\nStep 2: Calculate the sample proportions\n\n\\(p_1 = \\frac{x_1}{n_1} = \\frac{8}{20} = 0.4\\)\n\\(p_2 = \\frac{x_2}{n_2} = \\frac{10}{60} \\approx 0.167\\)\n\nStep 3: Compute the difference in proportions\n\n\\(\\text{Difference} = p_1 - p_2 = 0.4 - 0.167 \\approx 0.233\\)\n\nStep 4: Calculate the standard error (SE) of the difference\n\n\\(\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\)\n\\(\\text{SE} = \\sqrt{\\frac{0.4(1-0.4)}{20} + \\frac{0.167(1-0.167)}{60}} \\approx 0.120\\)\n\nStep 5: Determine the 95% confidence interval\n\n\\(Z_{critical} = 1.96\\)\n\\(\\text{Lower bound} = \\text{Difference} - Z_{critical} \\times \\text{SE} = 0.233 - 1.96 \\times 0.0.120 \\approx -0.001\\)\n\\(\\text{Upper bound} = \\text{Difference} + Z_{critical} \\times \\text{SE} = 0.233 + 1.96 \\times 0.120 \\approx 0.468\\)\n\nStep 6: Conclusion\n\nThe 95% confidence interval is approximately \\((-0.001, 0.468)\\)\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nAnswer question 2\n\n\n\nThe 95% confidence interval for the difference in proportions is \\((-0.001, 0.468)\\). Since this interval includes zero, we cannot reject the null hypothesis that the two proportions are equal. Therefore, we do not have sufficient evidence to conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\nResults of the two-sample test of proportions based on the normal approximation with continuity correction:\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  complications\nX-squared = 3.4409, df = 1, p-value = 0.0636\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.03449898  0.50116565\nsample estimates:\n   prop 1    prop 2 \n0.4000000 0.1666667 \n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nAnswer question 3\n\n\n\nThe p-value from the two-sample test of proportions is 0.064. Since this p-value is greater than the significance level of 0.05, we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIn addition to the p-value, output of the prop.test() function also provides an approximate 95% confidence interval for the difference in proportions. How does this confidence interval compare to the one you calculated manually?\n\n\n\n\n\n\n\n\nAnswer question 4\n\n\n\nThe 95% confidence interval for the difference in proportions was \\((-0.001, 0.468)\\). This is an approximate interval without continuity correction. The confidence interval provided in the output above is slightly wider due to the continuity correction. Without continuity correction (adjust = FALSE in R), the confidence interval provided by the prop.test() function is the same as the one that was calculated manually. SPSS gave the same result as R.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\n\n\n\n\nAnswer question 5\n\n\n\nThe expected counts for each cell in the contingency t able are as follows:\n\n\n            Complication No Complication\nSmokers              4.5            15.5\nNon-smokers         13.5            46.5\n\n\nOne of the cells in the contingency table has an expected count just below 5, which indicates that the two sample Z-test may not be fully accurate in this case.\n\n\n\n\n\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  complications\np-value = 0.05967\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.9154957 11.7051187\nsample estimates:\nodds ratio \n  3.274581 \n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nAnswer question 6\n\n\n\nThe p-value from Fisher’s exact test is 0.060. Since this p-value is greater than the significance level of 0.05, we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  side_effects\nX-squared = 25.136, df = 4, p-value = 4.723e-05\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe p-value from the chi-square test of homogeneity is &lt;0.0001. Since this p-value is less than the significance level of 0.05, we have sufficient evidence to reject the null hypothesis. Therefore, we can conclude that the distribution of vaccine side effects is not consistent across the three age groups.\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\nExpected cell counts:\n\n\n          None     Mild   Severe\n18–39 34.83871 34.83871 20.32258\n40–59 38.70968 38.70968 22.58065\n60+   46.45161 46.45161 27.09677\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nThe expected counts in all cells are well above 5, which indicates that the normal approximation is appropriate in this case.\n\n\n\n\n\nComparison of age groups 18–39 and 40–59\n\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_12\nX-squared = 5.3616, df = 2, p-value = 0.06851\n\n\nThe Bonferroni-corrected p-value for this comparison is 0.206.\n\n\n\n\n\n\nExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\nComparison of age groups 18–39 and 60+\n\n\n      None Mild Severe\n18–39   50   30     10\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_13\nX-squared = 24.208, df = 2, p-value = 5.536e-06\n\n\nThe Bonferroni-corrected p-value for this comparison is &lt;0.0001.\nComparison of age groups 40-59 and 60+\n\n\n      None Mild Severe\n40–59   40   40     20\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_23\nX-squared = 7.4497, df = 2, p-value = 0.02412\n\n\nThe Bonferroni-corrected p-value for this comparison is 0.072.\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nBased on the adjusted p-values from the pairwise comparisons, we can conclude that the distributions of side effects are significantly different between the age groups 18–39 and 60+ (Bonferroni-corrected p-value &lt; 0.001). However, there were no significant differences between the age groups 40–59 and 60+ (Bonferroni-corrected p-value = 0.072) or between the age groups 18–39 and 40–59 (Bonferroni-corrected p-value = 0.206)."
  },
  {
    "objectID": "lab4_answers.html#vaccine-side-effects-across-age-groups",
    "href": "lab4_answers.html#vaccine-side-effects-across-age-groups",
    "title": "Advanced Medical Statistics – Answers lab 4",
    "section": "",
    "text": "None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  side_effects\nX-squared = 25.136, df = 4, p-value = 4.723e-05\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\n\n\n\n\n\nAnswer question 7\n\n\n\nThe p-value from the chi-square test of homogeneity is &lt;0.0001. Since this p-value is less than the significance level of 0.05, we have sufficient evidence to reject the null hypothesis. Therefore, we can conclude that the distribution of vaccine side effects is not consistent across the three age groups.\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\nExpected cell counts:\n\n\n          None     Mild   Severe\n18–39 34.83871 34.83871 20.32258\n40–59 38.70968 38.70968 22.58065\n60+   46.45161 46.45161 27.09677\n\n\n\n\n\n\n\n\nAnswer question 8\n\n\n\nThe expected counts in all cells are well above 5, which indicates that the normal approximation is appropriate in this case.\n\n\n\n\n\nComparison of age groups 18–39 and 40–59\n\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_12\nX-squared = 5.3616, df = 2, p-value = 0.06851\n\n\nThe Bonferroni-corrected p-value for this comparison is 0.206.\n\n\n\n\n\n\nExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\nComparison of age groups 18–39 and 60+\n\n\n      None Mild Severe\n18–39   50   30     10\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_13\nX-squared = 24.208, df = 2, p-value = 5.536e-06\n\n\nThe Bonferroni-corrected p-value for this comparison is &lt;0.0001.\nComparison of age groups 40-59 and 60+\n\n\n      None Mild Severe\n40–59   40   40     20\n60+     30   50     40\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_23\nX-squared = 7.4497, df = 2, p-value = 0.02412\n\n\nThe Bonferroni-corrected p-value for this comparison is 0.072.\n\n\n\n\n\n\nQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?\n\n\n\n\n\n\n\n\nAnswer question 9\n\n\n\nBased on the adjusted p-values from the pairwise comparisons, we can conclude that the distributions of side effects are significantly different between the age groups 18–39 and 60+ (Bonferroni-corrected p-value &lt; 0.001). However, there were no significant differences between the age groups 40–59 and 60+ (Bonferroni-corrected p-value = 0.072) or between the age groups 18–39 and 40–59 (Bonferroni-corrected p-value = 0.206)."
  },
  {
    "objectID": "lab7.html",
    "href": "lab7.html",
    "title": "Advanced Medical Statistics – Lab 7",
    "section": "",
    "text": "In this part of the lab, we will build a prediction model for hospital length of stay (los) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\nKey variables in the dataset include:\n\nlos: Length of hospital stay (days, continuous outcome)\nage: Age at hospital admission (years)\ngender: Gender (0 = Male, 1 = Female)\nhr: Initial heart rate (beats per minute)\nsysbp and diasbp: Initial systolic and diastolic blood pressure (mmHg)\nbmi: Body mass index (kg/m^2)\ncvd: Presence of cardiovascular disease (0 = No, 1 = Yes)\nsho: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n\n\nDownload the dataset from Brighspace (whas500.sav) and open it in R or SPSS. When using R, make sure that the categorical variables are correctly coded as factors.\nCreate an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\nR instructions: use the lm() function to fit the model.\nSPSS instructions: use the General Linear Model procedure, which can be accessed via Analyze → General Linear Model → Univariate.\n\n\n\nTo identify the least significant predictor, we use the Type III ANOVA table:\n\nSignificance threshold: \\(p &gt; 0.10\\)\nRemove the predictor with the largest p-value above this threshold.\n\nR instructions: use the Anova() function from the car package to obtain the Type III ANOVA table (see previous lab).\nSPSS instructions: the Type III ANOVA table is generated in the General Linear Model output.\n\n\n\nIteratively remove the least significant predictor until all predictors have \\(p &lt; 0.10\\). At each step:\n\nRerun the regression model\nGenerate the Type III ANOVA table\nRemove the least significant predictor\n\n\n\n\nPresent the final linear regression model:\n\nSummarize the remaining predictors and their coefficients\nDiscuss how each variable contributes to predicting hospital length of stay\n\nCreate residual plots to assess the model assumptions (normality, homoscedasticity, linearity)."
  },
  {
    "objectID": "lab7.html#part-1-building-prediction-models-using-backward-elimination",
    "href": "lab7.html#part-1-building-prediction-models-using-backward-elimination",
    "title": "Advanced Medical Statistics – Lab 7",
    "section": "",
    "text": "In this part of the lab, we will build a prediction model for hospital length of stay (los) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\nKey variables in the dataset include:\n\nlos: Length of hospital stay (days, continuous outcome)\nage: Age at hospital admission (years)\ngender: Gender (0 = Male, 1 = Female)\nhr: Initial heart rate (beats per minute)\nsysbp and diasbp: Initial systolic and diastolic blood pressure (mmHg)\nbmi: Body mass index (kg/m^2)\ncvd: Presence of cardiovascular disease (0 = No, 1 = Yes)\nsho: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n\n\nDownload the dataset from Brighspace (whas500.sav) and open it in R or SPSS. When using R, make sure that the categorical variables are correctly coded as factors.\nCreate an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\nR instructions: use the lm() function to fit the model.\nSPSS instructions: use the General Linear Model procedure, which can be accessed via Analyze → General Linear Model → Univariate.\n\n\n\nTo identify the least significant predictor, we use the Type III ANOVA table:\n\nSignificance threshold: \\(p &gt; 0.10\\)\nRemove the predictor with the largest p-value above this threshold.\n\nR instructions: use the Anova() function from the car package to obtain the Type III ANOVA table (see previous lab).\nSPSS instructions: the Type III ANOVA table is generated in the General Linear Model output.\n\n\n\nIteratively remove the least significant predictor until all predictors have \\(p &lt; 0.10\\). At each step:\n\nRerun the regression model\nGenerate the Type III ANOVA table\nRemove the least significant predictor\n\n\n\n\nPresent the final linear regression model:\n\nSummarize the remaining predictors and their coefficients\nDiscuss how each variable contributes to predicting hospital length of stay\n\nCreate residual plots to assess the model assumptions (normality, homoscedasticity, linearity)."
  },
  {
    "objectID": "lab7.html#part-2-automated-procedures-for-building-prediction-models",
    "href": "lab7.html#part-2-automated-procedures-for-building-prediction-models",
    "title": "Advanced Medical Statistics – Lab 7",
    "section": "Part 2: Automated procedures for building prediction models",
    "text": "Part 2: Automated procedures for building prediction models\nIn this part, we explore automated procedures for predictor selection in regression models. These procedures can be useful when dealing with a large number of predictors. Both R and SPSS provide tools for these procedures, though SPSS has specific limitations with categorical variables in linear regression models.\n\nSPSS: Stepwise selection\n\nSPSS offers tools for forward, backward, or stepwise selection:\n\nAccess these procedures via Analyze → Regression → Linear\nUnder Method, choose:\n\nForward for forward selection\nBackward for backward elimination\nStepwise for a combination of both approaches\n\n\nSPSS automatically includes or excludes predictors based on significance levels\n\nImportant note for SPSS users: SPSS does not automatically handle categorical variables with more than two categories:\n\nYou must manually create dummy variables for each category (excluding the reference category) using Transform → Recode into Different Variables\nSPSS treats each dummy variable as a separate predictor during stepwise procedures\nThis means the overall contribution of the original categorical variable cannot be evaluated as a whole\nAs a result, some dummy variables may be included or excluded independently, breaking the connection to the original variable\n\n\n\nR: Automated model selection\nIn R, the stepAIC function from the MASS package allows for automated selection based on AIC (Akaike Information Criterion). Backward, forward, or stepwise selection can be specified using the direction argument:\n\nlibrary(MASS)\nfit &lt;- lm(los ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho, data = whas)\nstep_model &lt;- stepAIC(fit, direction = \"backward\")\nsummary(step_model)\n\n\n\nExercise: Automated procedures vs manual model\n\nApply the automated backward elimination procedure:\n\nR users:\n\nUse the stepAIC function with direction = \"backward\"\n\nSPSS users:\n\nUse Analyze → Regression → Linear\nSelect Backward under Method\n\n\nCompare the final model obtained from the automated procedure to the manually created model in Part 1:\n\nAre the same predictors included in both models?\nIf not, what differences do you observe, and what might explain them?"
  },
  {
    "objectID": "lab7.html#part-3-causal-diagrams",
    "href": "lab7.html#part-3-causal-diagrams",
    "title": "Advanced Medical Statistics – Lab 7",
    "section": "Part 3: Causal diagrams",
    "text": "Part 3: Causal diagrams\nFor each of the exercises below:\n\nTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\nCheck your answer using the DAGitty webtool\n\n\nExercise 1\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n\n\nDAG exercise 1\n\n\n\n\nExercise 2\nIn the graph depicted below, what happens when you additionally adjust for v5?\n\n\n\nDAG exercise 2\n\n\n\n\nExercise 3\nThis diagram is slightly different: v1 now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of v1 on O?\n\n\n\nDAG exercise 3\n\n\n\n\nExercise 4\nNow, v2 is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of v2 on O?\n\n\n\nDAG exercise 4\n\n\n\n\nExercise 5\nBack to the first DAG. However, v2 is now unmeasured. Can we still obtain an unconfounded estimate of the effect of E on O?\n\n\n\nDAG exercise 5\n\n\n\n\nExercise 6\nSee the DAG below: you adjusted for v5. What would be the consequence of this action?\n\n\n\nDAG exercise 6"
  },
  {
    "objectID": "R_lab1.html",
    "href": "R_lab1.html",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "",
    "text": "Welcome to lab 1 in the advanced medical statistics course. In this lab, we will explore descriptive statistics and probability calculations for random variables. We will use an example dataset to practice summarizing continuous and categorical variables, and introduce some basic concepts of probability distributions."
  },
  {
    "objectID": "R_lab1.html#descriptive-analysis-of-continuous-variables",
    "href": "R_lab1.html#descriptive-analysis-of-continuous-variables",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Descriptive analysis of continuous variables",
    "text": "Descriptive analysis of continuous variables\nLet’s start by calculating the summary statistics for the continuous variable age.\n\nMean: Use the mean() function to calculate the average age.\n\nmean_age &lt;- mean(lowbwt$age, na.rm = TRUE)\nmean_age\n\nHere, na.rm = TRUE ensures that any missing values are ignored in the calculation.\nStandard Deviation: Use the sd() function to calculate the standard deviation of age.\n\nsd_age &lt;- sd(lowbwt$age, na.rm = TRUE)\nsd_age\n\nMedian: Use the median() function to calculate the median age.\n\nmedian_age &lt;- median(lowbwt$age, na.rm = TRUE)\nmedian_age\n\nInterquartile Range (IQR): Use the IQR() function to calculate the interquartile range of age.\n\niqr_age &lt;- IQR(lowbwt$age, na.rm = TRUE)\niqr_age\n\nThe IQR() function calculates the range between the 25th and 75th percentiles of age.\n\nTo decide which summary measures (mean and standard deviation, or median and IQR) are appropriate to report, we need to understand the shape of the distribution of the age variable. We do this by creating a histogram:\n\nggplot(lowbwt, aes(x = age)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Age\", x = \"Age of Mother (years)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, and IQR for the variable lwt. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report."
  },
  {
    "objectID": "R_lab1.html#descriptive-analysis-of-categorical-variables",
    "href": "R_lab1.html#descriptive-analysis-of-categorical-variables",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Descriptive analysis of categorical variables",
    "text": "Descriptive analysis of categorical variables\nLet’s move on to analyzing the categorical variables. We will start by calculating the frequency and percentage of mothers who smoked during pregnancy (smoke):\n\nFrequency Table: Use the table() function to calculate the frequency of each category.\n\nfreq_smoke &lt;- table(lowbwt$smoke)\nfreq_smoke\n\nPercentage Calculation: Convert the counts to percentages by dividing by the total number of observations and multiplying by 100.\n\nperc_smoke &lt;- prop.table(freq_smoke) * 100\nperc_smoke\n\nThe prop.table() function in R is used to convert a frequency table into a table of proportions, essentially calculating the relative frequencies. When you multiply the result by 100, it converts these proportions into percentages.\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable history of hypertension (ht)\n\n\nIn addition to calculating frequencies and percentages, it can also be helpful to visualize categorical data. One common way to do this is by creating a bar chart. Below is an example of how you can create a bar chart for the smoke variable:\n\nggplot(lowbwt, aes(x = factor(smoke))) +\n  geom_bar(fill = \"blue\", color = \"black\") +\n  labs(title = \"Smoking Status During Pregnancy\", x = \"Smoking Status (0 = No, 1 = Yes)\", y = \"Frequency\") +\n  theme_minimal()\n\nThis bar chart shows the frequency of mothers who smoked and those who did not during pregnancy. Similarly, you can create a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension.\n\n\n\n\n\n\nQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension."
  },
  {
    "objectID": "R_lab1.html#binomial-distribution",
    "href": "R_lab1.html#binomial-distribution",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial distribution represents the number of successes in a fixed number of independent trials, each with the same probability of success. For example, if we have 10 patients and we want to know the probability that exactly 3 of them respond to a given treatment, where the response rate is known to be 40%, we can use the dbinom() function:\n\n# Probability that exactly 3 out of 10 patients respond to the treatment (assuming p = 0.4)\np_response_3 &lt;- dbinom(3, size = 10, prob = 0.4)\np_response_3\n\n\nExplanation of dbinom() Arguments\n\nx: The number of successes we are interested in (in this example, 3 patients responding).\nsize: The number of trials, which represents the total number of patients (in this example, 10 patients).\nprob: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nTo calculate cumulative probabilities, use the pbinom() function. For example, to calculate the probability that 3 or fewer patients out of 10 respond to the treatment:\n\np_cum_3 &lt;- pbinom(3, size = 10, prob = 0.4)\np_cum_3\n\n\n\nExplanation of pbinom() Arguments\n\nq: The number of successes we want to calculate the cumulative probability for (in this example, 3 or fewer successes).\nsize: The number of trials, which represents the total number of patients (in this example, 10 patients).\nprob: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nBy default, the pbinom() function calculates the probability that the number of successes is less than or equal to q. However, it is also possible to calculate the probability that the number of successes is greater than q by setting lower.tail = FALSE. The lower.tail argument specifies whether the cumulative probability is calculated from the lower tail (default, TRUE) or the upper tail (FALSE).\n\n\n\n\n\n\nQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution)."
  },
  {
    "objectID": "R_lab1.html#normal-distribution",
    "href": "R_lab1.html#normal-distribution",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nSuppose that we want to calculate the probability that a randomly selected individual has a weight less than or equal to 80 kg, assuming that the distribution of weight in the population follows a normal distribution with mean 72 kg and standard deviation 10 kg.\nTo calculate this probability, we first need to standardize the value using the formula:\n\\[\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{80-72}{10} = 0.8\n\\]\nwhere:\n\nx is the value we want to standardize (in this case, 80 kg).\nμ is the mean of the distribution (in this case, 72 kg).\nσ is the standard deviation of the distribution (in this case, 10 kg).\n\nUsing R, we can then use the pnorm() function to find the corresponding cumulative probability from the standard normal distribution:\n\n# Standardize the value\nz_value &lt;- (80 - 72) / 10\nz_value\n\n# Use the standard normal distribution to find the cumulative probability\ncum_prob_weight &lt;- pnorm(z_value)\ncum_prob_weight\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?"
  },
  {
    "objectID": "R_lab3.html",
    "href": "R_lab3.html",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "",
    "text": "Welcome to lab 3 in the advanced medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can download from Brightspace.\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\n\n# Load the dataset\nlowbwt &lt;- read_sav(\"lowbwt.sav\")\n\n# Convert all variables to factors where needed\nlowbwt &lt;- lowbwt %&gt;% mutate(across(where(is.labelled), as_factor))\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "R_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "R_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 1: Independent Samples t-test and Mann-Whitney U Test",
    "text": "Part 1: Independent Samples t-test and Mann-Whitney U Test\nIn this part of the lab, we will examine the effect of smoking during pregnancy on birth weight.\n\nExploratory data analysis\nWe will start by creating a boxplot to visualize the distribution of birth weights for mothers who smoked and those who did not.\n\n# Create boxplot comparing birth weights for mothers who smoked and those who did not\nggplot(lowbwt, aes(x = smoke, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Smoking Status\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\nIndependent samples t-test\nNext, we will perform an independent samples t-test to compare the mean birth weights between mothers who smoked and those who did not. In R, this can be done using the t.test() function:\n\n# Perform independent samples t-test\nindependent_t_test &lt;- t.test(bwt ~ smoke, data = lowbwt, var.equal = TRUE)\n\n# Print results\nprint(independent_t_test)\n\nExplanation:\n\nThe first argument bwt ~ smoke specifies the formula for the test, indicating that we are comparing the birth weights (bwt) between the two groups defined by the smoke variable.\nThe second argument data = lowbwt specifies the dataset.\nThe argument var.equal = TRUE indicates that we are assuming equal variances in the two groups, meaning that the classical independent samples t-test is performed. If you suspect unequal variances, Welch’s t-test can be conducted by setting var.equal = FALSE.\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\nChecking of assumptions\nTo assess whether the assumption of normality holds for the outcome variable in both groups, we create the following plot:\n\n# Create histograms of birth weight by smoking status\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(binwidth = 200, fill = \"blue\", colour=\"black\", alpha = 0.7) +\n  facet_wrap(~smoke) +\n  labs(x = \"Birth Weight (grams)\", title = \"Histograms of Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\nWe also need to check whether the assumption of a common population standard deviation holds. This can be done by performing the Levene test:\n\n# Perform Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(bwt ~ smoke, data = lowbwt)\nlevene_test\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nBased on the Levene test, does the assumption of equal variances hold?\n\n\n\n\n95% Confidence Interval for the mean difference\nIn addition to performing hypothesis tests, it is often informative to estimate the effect size and its uncertainty. One way to do this is by calculating a confidence interval for the mean difference in birth weight between the two groups. The 95% confidence interval is included in the default output of the t.test() function, so in principle we could extract it from there. As an exercise, we are also going to calculate it manually based on the formulas provided in the lecture/course syllabus.\nTo calculate the summary statistics required for the manual calculation, we use the following code:\n\n# Calculate summary statistics by smoking status\nsummary_stats &lt;- lowbwt %&gt;%\n  group_by(smoke) %&gt;%\n  summarise(mean = mean(bwt), sd = sd(bwt), n = n())\nsummary_stats\n\n\n\n\n\n\n\nExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not. You may simplify the calculation by using the 97.5th percentile from the standard normal distribution (1.96) rather than the corresponding percentile from the t-distribution.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\nMann-Whitney U Test\nIn case the assumptions of the independent samples t-test are violated, we can use the Mann-Whitney U test as a non-parametric alternative. This test can be performed in R using the wilcox.test() function:\n\n# Perform Mann-Whitney U test\nwilcoxon_test &lt;- wilcox.test(bwt ~ smoke, data = lowbwt)\nwilcoxon_test\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?"
  },
  {
    "objectID": "R_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "R_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\nIn this part of the lab, we are going to examine the effect of ethnicity on birth weight.\n\nExplaratory data analysis\n\n# Create boxplot comparing birth weights across ethnic groups\nggplot(lowbwt, aes(x = ethnicity, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Ethnicity\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Ethnicity\")\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\nOne-way ANOVA\nTo test the null hypothesis that the mean birth weights are equal across all ethnic groups, we can perform a one-way ANOVA using the aov() function:\n\n# Perform One-Way ANOVA\nanova_result &lt;- aov(bwt ~ ethnicity, data = lowbwt)\n\n# Print summary of ANOVA\nsummary(anova_result)\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\nPost-hoc tests\nIf the one-way ANOVA indicates a statistically significant difference in birth weight across ethnic groups, we can perform Bonferroni-corrected post-hoc tests to determine which specific groups differ from each other. In R, this can be done using the pairwise.t.test() function:\n\n# Perform pairwise comparisons with Bonferroni correction\nposthoc &lt;- pairwise.t.test(lowbwt$bwt, lowbwt$ethnicity, p.adjust.method = \"bonferroni\")\nposthoc\n\nExplanation:\n\nThe first argument of the pairwise.t.test() function specifies the outcome variable, which is the column of the lowbwt dataset that contains the birth weight values\nThe second argument specifies the grouping variable, which is the column of the lowbwt dataset that contains the ethnicity values\nThe p.adjust.method = \"bonferroni\" argument specifies that the p-values should be adjusted using the Bonferroni correction\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nWhat conclusions can be drawn from the post-hoc comparisons?\n\n\n\n\nChecking of assumptions\nTo assess whether the results of the one-way ANOVA are valid, we need to check the assumptions of normality and homogeneity of variances. This step is analogous to the previous examples, and is left as an exercise.\n\n\nKruskal-Wallis Test\nIf the assumptions of the one-way ANOVA are violated, we can use the Kruskal-Wallis test as a non-parametric alternative. The test can be performed in R using the kruskal.test() function:\n\n# Perform Kruskal-Wallis test\nkruskal_test &lt;- kruskal.test(bwt ~ ethnicity, data = lowbwt)\nkruskal_test\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?"
  },
  {
    "objectID": "R_lab3.html#part-3-unguided-exercises",
    "href": "R_lab3.html#part-3-unguided-exercises",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav (available on Brightspace) contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?"
  },
  {
    "objectID": "R_lab6.html",
    "href": "R_lab6.html",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "",
    "text": "Welcome to lab 6 on correlation and linear regression. In today’s exercises, we will be analyzing a dataset named pockets.sav, which you can download from Brightspace. This dataset contains measurements of periodontal pocket depth for a group of individuals, along with several demographic and lifestyle variables.\nBelow is an overview of the variables we will be working with:\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(ggplot2) # for data visualization\nlibrary(car)    # for calculating type-III ANOVA tables\n\n# Load the dataset\npockets &lt;- read_sav(\"datasets/pockets.sav\")\n\n# Convert labeled variables to factors\npockets &lt;- pockets %&gt;%\n  mutate(across(where(is.labelled), as_factor))"
  },
  {
    "objectID": "R_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "R_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 1: Pearson’s correlation coefficient and simple linear regression",
    "text": "Part 1: Pearson’s correlation coefficient and simple linear regression\nIn this section, we will investigate whether age is associated with pocket depth. We start by creating a scatterplot to visualize the relationship between pocketdepth and age:\n\n# Scatterplot of pocket depth vs. age\nggplot(pockets, aes(x = age, y = pocketdepth)) +\n  geom_point() +\n  labs(\n    x     = \"Age (years)\",\n    y     = \"Pocket Depth (mm)\",\n    title = \"Scatterplot of Pocket Depth vs Age\"\n  )\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\nPearson’s Correlation Coefficient\nTo quantify the strength and direction of the linear relationship between age and pocketdepth, we can calculate Pearson’s correlation coefficient:\n\n# Calculate Pearson's correlation coefficient\ncor(pockets$age, pockets$pocketdepth, use=\"complete.obs\")\n\nThe argument use = \"complete.obs\" tells R to exclude any missing values when calculating the correlation coefficient. In this case, there are no missing values in the age and pocketdepth variables, meaning that the argument is not strictly necessary. However, it is good practice to include it to avoid potential issues with missing data in other datasets.\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\nWe can also test whether the correlation coefficient is significantly different from zero using a hypothesis test. The null hypothesis is that the correlation coefficient is zero (i.e., no linear relationship between the variables), and the alternative hypothesis is that the correlation coefficient is not zero. In R, we can perform this test using the cor.test() function:\n\n# Test the significance of the correlation coefficient\ncor_test &lt;- cor.test(pockets$age, pockets$pocketdepth)\ncor_test\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\nFitting a Simple Linear Regression Model\nNext, we fit a simple linear regression model to quantify the relationship between age and pocketdepth. In R, this can be achieved using the lm() function:\n\n# Fit a simple linear regression model\nmodel_slr &lt;- lm(pocketdepth ~ age, data = pockets)\n\n# Print the summary of the model\nsummary(model_slr)\n\nThe formula pocketdepth ~ age specifies that pocketdepth is the dependent variable and age is the independent variable. The intercept term is included by default in the linear regression model and is therefore not explicitly specified in the formula.\n\n\n\n\n\n\nQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\nAssumption Checking\nTo obtain diagnostic plots for the simple linear regression model, we can supply the fitted model to the plot() function. By default, R will generate four diagnostic plots. In this lab, we are going to focus on the following two plots:\n\nQ-Q Plot: This plot helps us assess the normality of the residuals. This plot can be obtained by setting which = 2 in the plot() function.\nResiduals vs. Fitted: This plot helps us check for homoscedasticity (constant variance) and linearity assumptions. This plot can be obtained by setting which = 1 in the plot() function.\n\n\nNormality of Residuals\n\nplot(model_slr, which = 2)  # Q-Q plot\n\nIn addition to the Q-Q plot, we can also create a histogram of the residuals to visually inspect their distribution:\n\n# Histogram of residuals\nhist(residuals(model_slr), breaks = 15, col = \"lightblue\", border = \"black\", main = \"Histogram of Residuals\", xlab = \"Residuals\")\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\nHomoscedasticity and linearity\n\nplot(model_slr, which = 1)  # residuals vs fitted\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\nThe red line in R’s default residual-versus-fitted plot is a LOESS (locally estimated scatterplot smoothing) curve that shows the average trend in the residuals. It helps you see if there is a systematic pattern (e.g., curvature) that might indicate the linear model is misspecified. Ideally, you want that red line to be close to horizontal (i.e., around zero) with no strong curvature, suggesting that the linear fit is appropriate and there’s no obvious nonlinearity or other systematic pattern left in the residuals.\n\n\n\n\n\n\nQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?"
  },
  {
    "objectID": "R_lab6.html#part-2-ancova-analysis-of-covariance",
    "href": "R_lab6.html#part-2-ancova-analysis-of-covariance",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\nIn this section, we will fit an ANCOVA model to determine whether alcohol consumption is associated with pocket depth, controlling for age.\n\nExploratory Data Analysis\nWe start by creating a scatterplot to visualize the relationship between age and pocketdepth, using different colors to represent the levels of alcohol:\n\n# Scatterplot of pocketdepth vs. age, colored by alcohol\nggplot(pockets, aes(x = age, y = pocketdepth, color = alcohol)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth vs. Age by Alcohol Consumption\", x = \"Age\", y = \"Pocket Depth\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\nDummy Coding for alcohol\nThe variable alcohol has three categories (e.g., \"None\", \"1-2 glasses/day\", and \"&gt;2 glasses/day\"), meaning that we need to create two dummy variables to represent these categories in the model. R automatically generates these dummy variables once alcohol is recognized as a factor. To check if alcohol is a factor, we can use the is.factor() function:\n\n# Check whether alcohol is a factor\nis.factor(pockets$alcohol)\n\nIn this case, the output should be TRUE, given that we converted all categorical variables to factors when loading the dataset. Should alcohol not be a factor, we can either convert it to a factor using the factor() function or specify it as a factor in the model formula.\n\n\nFitting the ANCOVA Model\nTo fit the ANCOVA model, we can use the lm() function in R. The formula for the ANCOVA model is specified as pocketdepth ~ age + alcohol, where age is the continuous predictor and alcohol is the categorical predictor.\n\nmodel_ancova &lt;- lm(pocketdepth ~ age + alcohol, data = pockets)\nsummary(model_ancova)\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nQuestion 14\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\nTo test the overall significance of the alcohol variable as a predictor of pocketdepth, we construct an analysis of variance (ANOVA) table. The ANOVA table summarizes how much each term in a linear regression model contributes to explaining the overall variation in the response variable. There are different ways to construct this table depending on how the sum of squares is partitioned among model terms. A common approach is Type III ANOVA, which evaluates each variable or interaction after all other terms have been accounted for. Each effect is tested as if it were entered last, so its sum of squares reflects the unique contribution of that variable or interaction beyond what is already explained by the remaining terms.\nTo obtain the type III ANOVA table, we use the Anova() function from the car package. This function provides a more detailed ANOVA output compared to the base R anova() function, and has an argument type that allows you to specify the type of sum of squares to use:\n\nAnova(model_ancova, type=\"III\")\n\n\n\n\n\n\n\nQuestion 15\n\n\n\nBased on the ANOVA table, is the alcohol variable significantly associated with pocketdepth after accounting for age?\n\n\n\n\nModel Diagnostics\n\n\n\n\n\n\nExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model."
  },
  {
    "objectID": "R_lab6.html#part-3-interactions-in-ancova",
    "href": "R_lab6.html#part-3-interactions-in-ancova",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\nIn some cases, the relationship between the outcome variable and a predictor may depend on the level of another predictor. This is known as an interaction effect. In the context of ANCOVA, we can test for interactions between the continuous predictor (age) and the categorical predictor (alcohol).\n\nFitting the Interaction Model\nTo include an interaction term in the model, we can use the : operator in the formula. That is, the interaction term between age and alcohol can be specified as age:alcohol. With this term, the interaction model can be specified as pocketdepth ~ age + alcohol + age:alcohol. A short form for specifying the interaction model is pocketdepth ~ age * alcohol, which includes both the main effects of age and alcohol and their interaction terms.\n\nmodel_interaction &lt;- lm(pocketdepth ~ age * alcohol, data = pockets)\nsummary(model_interaction)\n\nTo test the significance of the interaction term, we again use the Anova() function from the car package:\n\nAnova(model_interaction, type=\"III\")\n\n\n\n\n\n\n\nQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?"
  },
  {
    "objectID": "R_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "R_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions."
  },
  {
    "objectID": "SPSS_lab1.html",
    "href": "SPSS_lab1.html",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "",
    "text": "Welcome to lab 1 in the advanced medical statistics course. In this lab, we will explore descriptive statistics and probability calculations for random variables. We will use an example dataset to practice summarizing continuous and categorical variables, and introduce some basic concepts of probability distributions."
  },
  {
    "objectID": "SPSS_lab1.html#descriptive-analysis-of-continuous-variables",
    "href": "SPSS_lab1.html#descriptive-analysis-of-continuous-variables",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Descriptive analysis of continuous variables",
    "text": "Descriptive analysis of continuous variables\nLet’s start by calculating the summary statistics for the continuous variable age.\n\nMean and Standard Deviation: Use the “Analyze &gt; Descriptive Statistics &gt; Descriptives…” menu in SPSS to calculate the mean age of the mothers. Select age as the variable, and ensure that the mean and standard deviation are checked in the “Options” dialog.\nMedian and Interquartile Range (IQR): To calculate the median and IQR for age, use the “Analyze &gt; Descriptive Statistics &gt; Frequencies…” menu. Select age as the variable and click on “Statistics…” to choose the median and quartiles.\n\nTo decide which summary measures (mean and standard deviation, or median and IQR) are appropriate to report, we need to understand the shape of the distribution of the age variable. Create a histogram by using “Graphs &gt; Legacy Dialogs &gt; Histogram…” and selecting age as the variable.\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, and IQR for the variable lwt. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report."
  },
  {
    "objectID": "SPSS_lab1.html#descriptive-analysis-of-categorical-variables",
    "href": "SPSS_lab1.html#descriptive-analysis-of-categorical-variables",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Descriptive analysis of categorical variables",
    "text": "Descriptive analysis of categorical variables\nLet’s move on to analyzing the categorical variables. We will start by calculating the frequency and percentage of mothers who smoked during pregnancy (smoke):\n\nFrequency Table: Use the “Analyze &gt; Descriptive Statistics &gt; Frequencies…” menu to calculate the frequency of each category for smoke.\nPercentage Calculation: SPSS will automatically calculate the percentages for each category in the frequency table output.\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable ht (history of hypertension).\n\n\nIn addition to calculating frequencies and percentages, it can also be helpful to visualize categorical data. One common way to do this is by creating a bar chart. To create a bar chart for the smoke variable, use “Graphs &gt; Legacy Dialogs &gt; Bar…” and select “Simple” and “Summaries for Groups of Cases.” Then select smoke as the Category Axis.\n\n\n\n\n\n\nQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension."
  },
  {
    "objectID": "SPSS_lab1.html#binomial-distribution",
    "href": "SPSS_lab1.html#binomial-distribution",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial distribution represents the number of successes in a fixed number of independent trials, each with the same probability of success. For example, if we have 10 patients and we want to know the probability that exactly 3 of them respond to a given treatment, where the response rate is known to be 40%, we can use SPSS to calculate this probability.\nTo perform this calculation in SPSS, go to “Transform &gt; Compute Variable…” and create a new variable called p_response_3. Use the function PDF.BINOM(x, n, p) to calculate the cumulative probability:\n\nx: The number of successes we are interested in (in this example, 3 patients responding).\nn: The number of trials (in this example, 10 patients).\np: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nCumulative probabilities, such as the probability that 3 or fewer patients out of 10 respond to the treatment, can similarly be calculated by creating a new variable with the CDF.BINOM(x, n, p) function.\nNote: Unlike other statistical software, SPSS requires that calculations performed using the “Transform &gt; Compute Variable…” tool always have a target variable specified. This means you cannot perform calculations without creating an output variable in your dataset, which can make one-time calculations cumbersome. This is a limitation of SPSS, especially when you just want to explore different scenarios without cluttering your dataset with temporary variables.\n\n\n\n\n\n\nQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution)."
  },
  {
    "objectID": "SPSS_lab1.html#normal-distribution",
    "href": "SPSS_lab1.html#normal-distribution",
    "title": "Advanced medical Statistics – Lab 1",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nSuppose that we want to calculate the probability that a randomly selected individual has a weight less than or equal to 80 kg, assuming that the distribution of weight in the population follows a normal distribution with mean 72 kg and standard deviation 10 kg.\nTo calculate this probability, we first need to standardize the value using the formula:\n\\[\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{80-72}{10} = 0.8\n\\]\nwhere:\n\nx is the value we want to standardize (in this case, 80 kg).\nμ is the mean of the distribution (in this case, 72 kg).\nσ is the standard deviation of the distribution (in this case, 10 kg).\n\nUsing SPSS, we can then use the “Transform &gt; Compute Variable…” menu and the CDF.NORMAL(z, mean, sd) function to find the corresponding cumulative probability from the standard normal distribution by setting z = 0.8, mean = 0 and sd = 1.\n\n\n\n\n\n\nQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?"
  },
  {
    "objectID": "SPSS_lab3.html",
    "href": "SPSS_lab3.html",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "",
    "text": "Welcome to lab 3 in the advanced medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can download from Brightspace.\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "SPSS_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "SPSS_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 1: Independent Samples t-test and Mann-Whitney U Test",
    "text": "Part 1: Independent Samples t-test and Mann-Whitney U Test\nIn this part of the lab, we will examine the effect of smoking during pregnancy on birth weight.\n\nExploratory data analysis\nTo get a sense of the data and the extent of a possible effect of smoking, we start by creating a boxplot. In SPSS, this can be achieved by following these steps:\n\nCreate Boxplot: Go to Graphs &gt; Chart Builder.\nSelect Boxplot: In the Chart Builder, select Boxplot from the gallery.\nSelect Variables: Drag bwt to the y-axis and smoke to the x-axis to create a boxplot comparing the birth weights for mothers who smoked and those who did not.\nRun the Graph: Click OK to generate the boxplot.\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\nIndependent samples t-test\nNext, we will perform an independent samples t-test to compare the mean birth weights between mothers who smoked and those who did not. To conduct this analysis in SPSS, follow these steps:\n\nPerform the Test: Go to Analyze &gt; Compare Means &gt; Independent-Samples t Test.\nSelect Test Variable and Grouping Variable: Move bwt to the Test Variable box and smoke to the Grouping Variable box.\nDefine Groups: Click on Define Groups and specify 0 and 1 for mothers who did not smoke and mothers who smoked, respectively.\nRun the Test: Click OK to run the analysis.\n\nSPSS will generate several tables. Here we focus on the table Independent Samples Test, which includes:\n\nLevene’s Test for Equality of Variances: Tests the assumption of equal variances between the two groups. The F and Sig. (p-value) columns represent the results of this test. If the p-value is greater than 0.05, the assumption of equal variances holds, and you should use the row labeled “Equal variances assumed” for interpreting the t-test results. If the p-value is less than or equal to 0.05, the assumption of equal variances is violated, and you should use the row labeled “Equal variances not assumed” for interpreting the t-test results.\nt-test for Equality of Means: Provides the t-test results, including the t-value, degrees of freedom, and p-value. It also includes the Mean Difference and 95% Confidence Interval of the Difference. It is important to choose the appropriate row (“Equal variances assumed” or “Equal variances not assumed”) based on the result of Levene’s test.\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nBased on the Levene test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nIs there a significant difference in birth weights between mothers who smoked and those who did not?\n\n\n\n\nChecking of assumptions\nTo assess whether the assumption of normality holds for the outcome variable in both groups, follow these steps to create histograms:\n\nCreate the Graph: Go to Graphs &gt; Chart Builder.\nCreate Histogram: In the Chart Builder, select Histogram from the gallery.\nSelect Variables: Drag bwt into the x-axis box.\nSplit by Group: In the Chart Builder, click on the Groups/Points ID tab. Check the Rows panel variable checkbox to add a panel field to the chart preview. Then, drag the smoke variable into this field to split the histograms by group.\nRun the Graph: Click OK to generate the histograms.\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n95% Confidence Interval for the mean difference\nIn addition to performing hypothesis tests, it is often informative to estimate the effect size and its uncertainty. One way to do this is by calculating a confidence interval for the mean difference in birth weight between the two groups. The 95% confidence interval is included in the output provided by the independent samples t-test procedure, so in principle, we could extract it from there. As an exercise, we are also going to calculate it manually based on the formulas provided in the lecture/course syllabus.\nTo calculate the summary statistics required for the manual calculation, follow these steps:\n\nNavigate to Explore Analysis:\n\nGo to Analyze &gt; Descriptive Statistics &gt; Explore.\n\nSelect Variables:\n\nMove bwt to the Dependent List box.\nMove smoke to the Factor List box.\n\nSet Display Options:\n\nCheck the option Statistics to prevent SPSS from cluttering the output with a range of plots.\n\nRun the Analysis:\n\nClick OK to run the analysis. The output will provide group-wise means, standard deviations, and sample sizes, which can be used for manual calculations.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not. You may simplify the calculation by using the 97.5th percentile from the standard normal distribution (1.96) rather than the corresponding percentile from the t-distribution.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the independent samples t-test procedure?\n\n\n\n\nMann-Whitney U Test\nIn case the assumption of normality is violated, we can use the Mann-Whitney U test as a non-parametric alternative to the independent samples t-test. To perform this test in SPSS, follow these steps:\n\nNavigate to the Mann-Whitney U Test:\n\nGo to Analyze &gt; Nonparametric Tests &gt; Legacy Dialogs &gt; 2 Independent Samples.\n\nSelect Variables:\n\nMove bwt to the Test Variable List.\nMove smoke to the Grouping Variable.\nClick Define Groups and specify the values of the smoke variable (e.g., 0 = No, 1 = Yes). Click Continue.\n\nSelect the Mann-Whitney U Test:\n\nUnder Test Type, ensure that Mann-Whitney U is selected.\n\nRun the Test:\n\nClick OK to run the analysis. The output will include the test statistic and associated p-value.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?"
  },
  {
    "objectID": "SPSS_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "SPSS_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\nIn this part of the lab, we are going to examine the effect of ethnicity on birth weight.\n\nExploratory data analysis\nTo get a sense of the differences in birth weights across the different ethnic groups, we will create a boxplot. In SPSS, follow these steps:\n\nCreate Boxplot:\n\nGo to Graphs &gt; Chart Builder.\nSelect Boxplot from the gallery.\n\nAssign Variables:\n\nDrag bwt to the y-axis.\nDrag ethnicity to the x-axis.\n\nRun the Graph:\n\nClick OK to generate the boxplot.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\nOne-way ANOVA\nTo test whether the mean birth weights differ across ethnic groups, we will perform a one-way ANOVA. Follow these steps:\n\nPerform the Test:\n\nGo to Analyze &gt; Compare Means &gt; One-Way ANOVA.\n\nSelect Variables:\n\nSet Dependent Variable: Birth Weight (bwt).\nSet Factor: Ethnicity (ethnicity).\n\nRun the Test:\n\nClick OK to run the analysis.\n\n\nSPSS will generate the following key output:\n\nANOVA Table: Contains the F-statistic and the associated p-value. These values indicate whether there is a statistically significant difference in mean birth weights across ethnic groups.\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\nPost-hoc tests\nIf the one-way ANOVA indicates a statistically significant difference, we will perform post-hoc tests to determine which specific groups differ from each other. To do this in SPSS:\n\nEnable Post-Hoc Tests:\n\nIn the one-way ANOVA dialog, click on the Post Hoc button.\nSelect Bonferroni as the method.\n\nRun the Test:\n\nClick Continue and then OK to run the analysis.\n\n\nThe post-hoc output will include pairwise comparisons of the means between all groups, with adjusted p-values.\n\n\n\n\n\n\nQuestion 9\n\n\n\nWhich groups differ significantly, and what do the results suggest about the differences in birth weights across ethnic groups?\n\n\n\n\nChecking of assumptions\nTo assess whether the results of the one-way ANOVA are valid, we need to check the assumptions of normality and homogeneity of variances. This step is analogous to the previous examples, and is left as an exercise.\n\n\nKruskal-Wallis Test\nIf the assumptions of the one-way ANOVA are not satisfied, we can use the Kruskal-Wallis test as a non-parametric alternative. To perform this test in SPSS, follow these steps:\n\nNavigate to the Kruskal-Wallis Test:\n\nGo to Analyze &gt; Nonparametric Tests &gt; Legacy Dialogs &gt; K Independent Samples.\n\nSelect Variables:\n\nMove bwt to the Test Variable List.\nMove ethnicity to the Grouping Variable.\nClick Define Range and specify the range of the grouping variable (e.g., 1 to 3 for the ethnicity variable). Click Continue.\n\nSelect the Kruskal-Wallis Test:\n\nUnder Test Type, ensure that Kruskal-Wallis H is selected.\n\nRun the Test:\n\nClick OK to run the analysis. The output will include the test statistic and associated p-value.\n\n\n\n\n\n\n\n\nQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?"
  },
  {
    "objectID": "SPSS_lab3.html#part-3-unguided-exercises",
    "href": "SPSS_lab3.html#part-3-unguided-exercises",
    "title": "Advanced Medical Statistics – Lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav (available on Brightspace) contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?"
  },
  {
    "objectID": "SPSS_lab6.html",
    "href": "SPSS_lab6.html",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "",
    "text": "Welcome to lab 6 on correlation and linear regression. In today’s exercises, we will be analyzing a dataset named pockets.sav, which you can download from Brightspace. This dataset contains measurements of periodontal pocket depth for a group of individuals, along with several demographic and lifestyle variables.\nBelow is an overview of the variables we will be working with:"
  },
  {
    "objectID": "SPSS_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "SPSS_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 1: Pearson’s Correlation Coefficient and Simple Linear Regression",
    "text": "Part 1: Pearson’s Correlation Coefficient and Simple Linear Regression\nIn this section, we will investigate whether age is associated with pocket depth. We begin by creating a scatterplot to visualize the relationship between pocketdepth and age.\n\nOpen pockets.sav in SPSS.\n\nGo to Graphs → Legacy Dialogs → Scatter/Dot.\n\nChoose Simple Scatter and click Define.\n\nPlace age on the X Axis and pocketdepth on the Y Axis.\n\nClick OK to generate the scatterplot.\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\nPearson’s Correlation Coefficient\nTo quantify the strength and direction of the linear relationship between age and pocketdepth, we will calculate Pearson’s correlation coefficient:\n\nGo to Analyze → Correlate → Bivariate.\n\nMove age and pocketdepth into the Variables list.\n\nMake sure Pearson is checked under Correlation Coefficients.\n\nClick OK.\n\nSPSS will output the correlation coefficient and a p-value testing whether the correlation is different from zero.\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\nYou will also see a Sig. (2-tailed) value in the SPSS output for the Pearson correlation, which corresponds to the test that the correlation coefficient is significantly different from zero.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis (that the correlation is zero)?\n\n\n\n\nFitting a Simple Linear Regression Model\nNext, we fit a simple linear regression model to quantify the relationship between age and pocketdepth.\n\nGo to Analyze → Regression → Linear.\n\nPut pocketdepth in the Dependent box.\n\nPut age in the Independent(s) box.\n\nClick OK to run the analysis.\n\nSPSS will produce output tables, including Model Summary, ANOVA, and Coefficients.\n\n\n\n\n\n\nQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05) according to the regression output?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent?\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the slope coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\nAssumption Checking\nTo assess assumptions (normality of residuals, homoscedasticity, etc.), we inspect residual plots.\n\nIn the Linear Regression dialog, click Plots.\n\nMove ZRESID (standardized residuals) to the Y: box and ZPRED (standardized predicted values) to the X: box under Scatter.\n\nAlso check Histogram and Normal probability plot.\n\nClick Continue → OK.\n\nSPSS will generate:\n\nA Histogram of the residuals (for checking normality).\nA Normal P-P plot of the residuals (another way to check normality).\nA scatterplot of standardized residuals vs. standardized predicted values (for checking homoscedasticity and linearity).\n\n\nNormality of Residuals\nInspect the Histogram and Normal P-P plot in the output.\n\n\n\n\n\n\nQuestion 9\n\n\n\nDo the histogram and Normal P-P plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\nHomoscedasticity and Linearity\nLook at the standardized residuals versus standardized predicted values scatterplot.\n\n\n\n\n\n\nQuestion 10\n\n\n\nDoes the plot suggest constant variance?\n\n\n\n\n\n\n\n\nQuestion 11\n\n\n\nIs there any strong curvature or systematic pattern that would indicate the model is misspecified (i.e., not truly linear)?"
  },
  {
    "objectID": "SPSS_lab6.html#part-2-ancova-analysis-of-covariance",
    "href": "SPSS_lab6.html#part-2-ancova-analysis-of-covariance",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\nIn this section, we will fit an ANCOVA model to determine whether alcohol consumption is associated with pocket depth, controlling for age.\n\nExploratory Data Analysis\nWe start by creating a scatterplot to visualize the relationship between age and pocketdepth, using different colors to represent the levels of alcohol:\n\nGo to Graphs → Legacy Dialogs → Scatter/Dot.\n\nChoose Simple Scatter and click Define.\n\nPlace age on the X Axis, pocketdepth on the Y Axis, and alcohol in Set Markers by.\n\nClick OK to generate the scatterplot.\n\n\n\n\n\n\n\nQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\nFitting the ANCOVA Model\nIn SPSS, there are multiple ways to fit linear regression models. If all your explanatory variables are continuous, you can use Analyze → Regression → Linear. However, if your model includes categorical predictors, it is often more convenient to use Analyze → General Linear Model, as SPSS will automatically handle the dummy coding for categorical variables in that procedure.\nIn this case, we have both continuous (age) and categorical (alcohol) explanatory variables, so we will use the General Linear Model procedure.\n\nGo to Analyze → General Linear Model → Univariate.\n\nPlace pocketdepth in the Dependent Variable box.\n\nPlace age under Covariate(s).\n\nPlace alcohol under Fixed Factor(s).\n\nClick Options, and select Parameter estimates.\nClick OK to run the analysis.\n\n\n\n\n\n\n\nQuestion 13\n\n\n\nFrom the table with the estimated regression coefficients, what is the estimated difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nQuestion 14\n\n\n\nFrom the same table, what is the estimated difference in pocket depth between individuals who consume \"1–2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\nTo test the overall significance of the alcohol variable as a predictor of pocketdepth, we construct an analysis of variance (ANOVA) table. The ANOVA table summarizes how much each term in a linear regression model contributes to explaining the overall variation in the response variable. There are different ways to construct this table depending on how the sum of squares is partitioned among model terms. A common approach is Type III ANOVA, which evaluates each variable or interaction after all other terms have been accounted for. Each effect is tested as if it were entered last, so its sum of squares reflects the unique contribution of that variable or interaction beyond what is already explained by the remaining terms.\nIn SPSS, the type III ANOVA table is automatically generated when you fit a linear model using the General Linear Model procedure. This table is displayed under the heading Tests of Between-Subjects Effects in the output.\n\n\n\n\n\n\nQuestion 15\n\n\n\nBased on the ANOVA table, is there a significant association between alcohol consumption and pocketdepth after accounting for age?\n\n\n\n\nModel Diagnostics\nSimilar to the simple regression case, you can request certain diagnostic plots in the Options menu of the Univariate dialog. However, the options are limited compared to the linear regression dialog. If you want more control over the diagnostic plots, you can save the residuals and predicted values to your dataset and then create the plots manually:\n\nIn the Univariate dialog, click Save.\nSelect Unstandardized predicted values and Standardized residuals.\nClick Continue to go back to the main dialog, and Click OK to run the analysis.\nAfter the analysis finishes, go to Graphs → Legacy Dialogs → Scatter/Dot (or Histogram) to plot the new residual and predicted-value columns, explore their relationship, or check for normality.\n\n\n\n\n\n\n\nExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model. Do you see any notable violations?"
  },
  {
    "objectID": "SPSS_lab6.html#part-3-interactions-in-ancova",
    "href": "SPSS_lab6.html#part-3-interactions-in-ancova",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\nIn some cases, the relationship between the outcome variable and a predictor may depend on the level of another predictor. This is known as an interaction effect. In the context of ANCOVA, we can test for interactions between the continuous predictor (age) and the categorical predictor (alcohol).\n\nFitting the Interaction Model\n\nGo to Analyze → General Linear Model → Univariate.\n\nPlace pocketdepth in the Dependent Variable box.\n\nPlace age under Covariate(s).\n\nPlace alcohol under Fixed Factor(s).\n\nClick Model, and select Build terms under Specify Model.\nUnder Build Terms, set the type to Main effects.\nIn the Factors & Covariates box, select alcohol and age and move the two variables to the Model box.\nUnder Build Terms, set the type to Interaction.\nIn the Factors & Covariates box, select alcohol and age and move the two variables to the Model box.\nClick Continue to go back to the main dialog.\nClick OK to run the analysis.\n\n\n\n\n\n\n\nQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?"
  },
  {
    "objectID": "SPSS_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "SPSS_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Advanced Medical Statistics – Lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions."
  }
]