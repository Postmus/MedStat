[
  {
    "objectID": "SPSS_lab9.html",
    "href": "SPSS_lab9.html",
    "title": "Medical Statistics – Lab 9",
    "section": "",
    "text": "In this section, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav). The outcome of interest for today’s analysis is overall survival, defined as the time from hospital admission to death from any cause. This information is captured in the variable lenfol (length of follow-up in days) and the variable fstat (follow-up status; dead or censored).\n\n\nThe variable miord represents the sequence of myocardial infarction (MI) events, categorized as either a first MI or a recurrent MI. Our aim is to analyze the relationship between MI order and overall survival outcomes.\n\n\nWe start by constructing a Kaplan-Meier survival curve to compare the survival probabilities between patients with a first MI and those with a recurrent MI.\nGo to Analyze -&gt; Survival -&gt; Kaplan-Meier. Select the lenfol variable as the Time and the fstat variable as the Status variable. Click on Define Event, enter 1 as the event code, and press Continue to return to the main dialog. Move the miord variable to the Factor box. Click on Compare Factor and select Log Rank. Press Continue to return to the main dialog. Click on Options and under Plots select Survival. Click on Continue to return to the main dialog and subsequenly on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\nNext, we will perform a Cox proportional hazards regression analysis to assess the association between MI order and overall survival while adjusting for potential confounders. We will start with the unadjusted model:\nGo to Analyze -&gt; Survival -&gt; Cox Regression. Select the lenfol variable as the Time, the fstat variable as the Status variable, and define the event indicator. Move the miord variable to the Covariates box (Block 1 of 1). Press Categorical and move the miord variable to the Categorical Covariates box. Press Continue to return to the main dialog. Click on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\nNow, let’s adjust the Cox regression model using age and gender as potential confounders:\nGo to Analyze -&gt; Survival -&gt; Cox Regression. Select the lenfol variable as the Time, the fstat variable as the Status variable, and define the event indicator. Move the miord, age, and gender variables to the Covariates box (Block 1 of 1). Press Categorical and move the miord and gender variables to the Categorical Covariates box. Press Continue to return to the main dialog. Click on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?"
  },
  {
    "objectID": "SPSS_lab9.html#association-between-mi-order-and-overall-survival",
    "href": "SPSS_lab9.html#association-between-mi-order-and-overall-survival",
    "title": "Medical Statistics – Lab 9",
    "section": "",
    "text": "The variable miord represents the sequence of myocardial infarction (MI) events, categorized as either a first MI or a recurrent MI. Our aim is to analyze the relationship between MI order and overall survival outcomes.\n\n\nWe start by constructing a Kaplan-Meier survival curve to compare the survival probabilities between patients with a first MI and those with a recurrent MI.\nGo to Analyze -&gt; Survival -&gt; Kaplan-Meier. Select the lenfol variable as the Time and the fstat variable as the Status variable. Click on Define Event, enter 1 as the event code, and press Continue to return to the main dialog. Move the miord variable to the Factor box. Click on Compare Factor and select Log Rank. Press Continue to return to the main dialog. Click on Options and under Plots select Survival. Click on Continue to return to the main dialog and subsequenly on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\nNext, we will perform a Cox proportional hazards regression analysis to assess the association between MI order and overall survival while adjusting for potential confounders. We will start with the unadjusted model:\nGo to Analyze -&gt; Survival -&gt; Cox Regression. Select the lenfol variable as the Time, the fstat variable as the Status variable, and define the event indicator. Move the miord variable to the Covariates box (Block 1 of 1). Press Categorical and move the miord variable to the Categorical Covariates box. Press Continue to return to the main dialog. Click on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\nNow, let’s adjust the Cox regression model using age and gender as potential confounders:\nGo to Analyze -&gt; Survival -&gt; Cox Regression. Select the lenfol variable as the Time, the fstat variable as the Status variable, and define the event indicator. Move the miord, age, and gender variables to the Covariates box (Block 1 of 1). Press Categorical and move the miord and gender variables to the Categorical Covariates box. Press Continue to return to the main dialog. Click on Ok to run the analysis.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?"
  },
  {
    "objectID": "SPSS_lab7.html",
    "href": "SPSS_lab7.html",
    "title": "Medical Statistics – Lab 7",
    "section": "",
    "text": "In part 1 of the lab, we are going to analyze the risk of in-hospital death in patients hospitalized because of acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001 (file whas500.sav from the Datasets menu). The outcome of interest is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\nGo to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the gender variable as the Row(s) and the stat variable as the Column(s). Click on Cells and under percentages check the box Row. Click op Continue and OK to create the table.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. To perform this analysis in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender variable to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To perform this test in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the age variables to the Block 1 of 1 box. Press Next to create a second block. Move the gender variable to the Block 2 of 2 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\nThe output will include a table with the results of the likelihood ratio test comparing the full model (variables included in block 1 and 2) to the reduced model (variables included in block 1 only). This table can be found in the section Block 2: Method = Enter and is labeled Omnibus tests of model coefficients. The p-value for the LRT comparing the full model to the reduced model can be found in the row block.\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In SPSS, this can be achieved by checking the option Hosmer-Lemeshow goodness-of-fit in the logistic regression dialog box.\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box Hosmer-Lemeshow goodness-of-fit to include the results of the Hosmer-Lemeshow test in the output produced. Press Continue to return to the main dialog and click on OK to perform the analysis.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab7.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "href": "SPSS_lab7.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "title": "Medical Statistics – Lab 7",
    "section": "",
    "text": "In part 1 of the lab, we are going to analyze the risk of in-hospital death in patients hospitalized because of acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001 (file whas500.sav from the Datasets menu). The outcome of interest is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\nGo to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the gender variable as the Row(s) and the stat variable as the Column(s). Click on Cells and under percentages check the box Row. Click op Continue and OK to create the table.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. To perform this analysis in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender variable to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To perform this test in SPSS, go to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the age variables to the Block 1 of 1 box. Press Next to create a second block. Move the gender variable to the Block 2 of 2 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box CI for exp(B) to include confidence intervals for the estimated hazard ratios. Press Continue to return to the main dialog and click on OK to run the analysis.\nThe output will include a table with the results of the likelihood ratio test comparing the full model (variables included in block 1 and 2) to the reduced model (variables included in block 1 only). This table can be found in the section Block 2: Method = Enter and is labeled Omnibus tests of model coefficients. The p-value for the LRT comparing the full model to the reduced model can be found in the row block.\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In SPSS, this can be achieved by checking the option Hosmer-Lemeshow goodness-of-fit in the logistic regression dialog box.\nGo to Analyze -&gt; Regression -&gt; Binary Logistic. Press reset to clear the previous analysis. Move the dstat variable to the Dependent box and the gender and age variables to the Block 1 of 1 box. Press Categorical. Specify that the gender variable is categorical by moving it from the Covariates box to the Categorical Covariates box. Press Continue to return to the main dialog. Press Options and check the box Hosmer-Lemeshow goodness-of-fit to include the results of the Hosmer-Lemeshow test in the output produced. Press Continue to return to the main dialog and click on OK to perform the analysis.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab7.html#part-2-unguided-exercises",
    "href": "SPSS_lab7.html#part-2-unguided-exercises",
    "title": "Medical Statistics – Lab 7",
    "section": "Part 2: unguided exercises",
    "text": "Part 2: unguided exercises\n\nExercise 1\nMultiple logistic regression was used to construct a prognostic index to predict coronary artery disease from data on 348 patients with valvular heart disease who had undergone routine coronary arteriography before valve replacement (Ramsdale et al. 1982). The estimated equation was:\n\\[logit(p) = ln(p/(1-p)) = b_{0} + 1.167 \\times x{1} + 0.0106 \\times x_{2} + \\textrm{other terms}\\]\nwhere \\(x_{1}\\) stands for the family history of ischaemic disease (0=no, 1=yes) and \\(x_{2}\\) is the estimated total number of cigarettes ever smoked in terms of thousand cigarettes, calculated as the average number smoked annually times the number of years smoking.\n\nWhat is the estimated odds ratio for having coronary artery disease for subjects with a positive family history relative to subjects with a negative family history?\nWhat total number of cigarettes ever smoked carries the same risk as a positive family history? Convert the result into years of smoking 20 cigarettes per day.\nWhat is the odds ratio for coronary artery disease for someone with a positive family history who had smoked 20 cigarettes a day for 30 years compared to a non smoker with no family history?\n\n\n\nExercise 2\nData from 37 patients receiving a non-depleted allogenic bone marrow transplant were examined to see which variables were associated with the occurrence of acute graft-versus-host disease (GvHD: 0=no, 1=yes) (Bagot et al., 1988). Possible predictors are TYPE (type of leukemia: 1=AML, acute myeloid leukaemia; 2=ALL, acute lymphocytic leukaemia; 3=CML, chronic myeloid leukemia), PREG (donor pregnancy: 0= no, 1=yes), and LOGIND (the logarithm of an index of mixed epidermal cell-lymphocyte reactions). The data are in the file GvHD.sav available from the Downloads menu.\n\nPerform a likelihood ratio test to determine whether there is a significant association between the type of leukemia and the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nIn the adjusted model, What is the estimated odds ratio for the occurrence of GvHD for patients with ALL compared to those with ALM?\nUse the Hosmer-Lemeshow goodness-of-fit test to evaluate the fit of the model. Based on the results, does the model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "SPSS_lab4.html",
    "href": "SPSS_lab4.html",
    "title": "Medical Statistics – Lab 4",
    "section": "",
    "text": "Welcome to lab 4 in the medical statistics course. In this lab, we will focus on the analysis of categorical data and the comparison of proportions between groups. We will also perform several statistical tests for the analysis of paired data."
  },
  {
    "objectID": "SPSS_lab4.html#smoking-and-post-surgical-complications",
    "href": "SPSS_lab4.html#smoking-and-post-surgical-complications",
    "title": "Medical Statistics – Lab 4",
    "section": "Smoking and post-surgical complications",
    "text": "Smoking and post-surgical complications\nA study was conducted to investigate whether smoking is associated with an increased risk of post-surgical complications. The relationship between smoking status (smoker or non-smoker) and the occurrence of complications following surgery was examined. The outcome of interest was whether or not a complication occurred (yes or no), with smoking status serving as the explanatory variable to compare complication rates between the two groups.\nThe data from the study are summarized in the following 2x2 contingency table:\n\n\n\n\nComplication\nNo Complication\nTotal\n\n\n\n\nSmokers\n8\n12\n20\n\n\nNon-smokers\n10\n50\n60\n\n\nTotal\n18\n62\n80\n\n\n\n\nConfidence intervals and hypothesis testing for the difference in proportions using the normal approximation\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\nWe can also use the normal approximation to test the hypothesis that the proportion of complications is the same for smokers and non-smokers. This test is known as the two-sample Z test for equality of proportions.\nAs explained in the syllabus, the two-sample Z test uses the pooled population proportion \\(\\hat{p}\\), which is calculated as the total number of events divided by the total sample size. This pooled proportion is used under the null hypothesis, which assumes that the two groups share the same underlying proportion. The standard error of the difference in proportions is then calculated as \\(\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}\\), where \\(n_1\\) and \\(n_2\\) are the sample sizes in the two groups.\nIn contrast, the 95% confidence interval for the difference in proportions does not rely on the pooled proportion. Instead, it calculates the standard error separately for each group using the observed proportions, resulting in an unpooled standard error: \\(\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\), where \\(p_1\\) and \\(p_2\\) are the sample proportions for each group. This approach provides an interval that better reflects the variability in the observed data, independent of the null hypothesis assumption.\nTo conduct the two-sample Z test, we need to create a new dataset containing the above observations. To achieve this, go to File -&gt; New -&gt; Data to create a new data file. Navigate to the Variable View tab and create three variables: Group, Complication, and Frequency. The Group variable should have two levels: Smokers (coded as 1) and Non-smokers (coded as 2), while the Complication variable should have two levels: Complication (coded as 1) and No complication (coded as 2). The Frequency variable will contain the counts of observations for each combination of group and complication status.\nNavigate to the Data View tab and enter the data from the contingency table into the new dataset. For example, in the first row in the screenshot below, the Group variable is set to Smokers (value=1), the Complication variable is set to Complication (value=1), and the Frequency variable is set to 8 (number of individuals in that cell of the contingency table). Repeat this process for the remaining rows to enter all the data.\n\n\n\nScreenshot of the SPSS dataset\n\n\nOnce the data is entered, we need to weight the data by the Frequency variable to account for the multiple observations in each cell. To do this, go to Data -&gt; Weight Cases. Select the Frequency variable and click OK.\nFinally, we can proceed with the two-sample Z test. Go to Analyze -&gt; Compare Means -&gt; Independent-Samples proportions. Select the Group variable as the Grouping Variable and the Complication variable as the Test Variable. Under Confidence Intervals..., select Wald and Wald (Continuity Corrected) to calculate the approximate 95% confidence intervals for the difference in proportions with and without continuity correction. Under Test Type, select Wald H0 (Continuity Corrected) to conduct the two-sample Z test with continuity correction. Click OK to run the test.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIn addition to the p-value, The SPSS output also provides two approximate 95% confidence intervals for the difference in proportions. How do these confidence intervals compare to the one you calculated manually?\n\n\n\n\nChecking of assumptions\nFor the use of the normal approximation to be valid, the expected number of events and non-events in each group should be at least 5.\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\nFisher’s exact test\nWhen the expected cell counts are small, the normal approximation may not be appropriate. In such cases, Fisher’s exact test is recommended for testing the association between two categorical variables.\nIn SPSS, we can perform Fisher’s exact test by going to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the Group variable as the Row(s) and the Complication variable as the Column(s). Click on Statistics and check the box Chi_square. Click on Exact Tests and check the box Exact. Click OK to run the test.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?"
  },
  {
    "objectID": "SPSS_lab4.html#vaccine-side-effects-across-age-groups",
    "href": "SPSS_lab4.html#vaccine-side-effects-across-age-groups",
    "title": "Medical Statistics – Lab 4",
    "section": "Vaccine side effects across age groups",
    "text": "Vaccine side effects across age groups\nA study was conducted to investigate whether the occurrence of vaccine side effects differs across age groups. Researchers categorized side effects into three types: none, mild, and severe. The study participants were divided into three age groups: 18–39, 40–59, and 60+, and data was collected on the type of side effect experienced by individuals in each group.\nThe research objective was to determine whether the distribution of side effects is consistent across these age groups.\nThe data is summarized in the following contingency table:\n\n\n\nAge Group\nNone\nMild\nSevere\nTotal\n\n\n\n\n18–39\n50\n30\n10\n90\n\n\n40–59\n40\n40\n20\n100\n\n\n60+\n30\n50\n40\n120\n\n\nTotal\n120\n120\n70\n310\n\n\n\n\nChi-square test of homogeneity\n\n\n\n\n\n\nImportantExercise\n\n\n\nCreate a new dataset in SPSS to enter the data from the contingency table.\n\n\nUsing the new dataset, we can perform a chi-square test of homogeneity to determine whether the distribution of side effects is consistent across the three age groups. Go to Analyze -&gt; Descriptive Statistics -&gt; Crosstabs. Select the Age Group variable as the Row(s) and the Side Effect variable as the Column(s). Click on Statistics and check the box Chi_square. Click OK to run the test.\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\nChecking of assumptions\nTo use the chi-square test, the expected cell counts should be at least 5 for most cells.\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\n\n\nPost-hoc pairwise comparisons\nFinally, we are interested in determining which age groups have significantly different distributions of side effects. We start by comparing the first two age groups (18–39 and 40–59). Assuming the name of the age group variable is Age_Group and the three age groups are coded as 1, 2, and 3, we can filter out the third age group (60+) by going to Data -&gt; Select Cases. Select If condition is satisfied and enter the condition Age_Group &lt; 3. Click OK to filter the data.\nNow that the data is filtered, we can perform a chi-square test for the subset of the data corresponding to the first two age groups by repeating the steps described above. Manually adjust the p-value for multiple comparisons using the Bonferroni correction.\n\n\n\n\n\n\nImportantExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?"
  },
  {
    "objectID": "SPSS_lab4.html#introduction",
    "href": "SPSS_lab4.html#introduction",
    "title": "Medical Statistics – Lab 4",
    "section": "Introduction",
    "text": "Introduction\nIn this part of the lab, we will analyze paired data on pocket depth before and after an intervention. Pocket depth refers to the depth of the gum pockets around teeth, measured using a periodontal probe. It is an important indicator of periodontal health. Healthy gums typically have pocket depths less than 3 mm, while deeper pockets may indicate conditions such as gingivitis or periodontitis.\nThe dataset pockets_paired.sav, available from the Datasets menu, contains the following columns:\n\nsubjectID: Unique identifier for each participant\npocket_depth_before: Average pocket depth (in mm) measured before the intervention\npocket_depth_after: Average pocket depth (in mm) measured after the intervention\n\nThe objective is to determine whether the intervention significantly reduces pocket depth. We will apply three statistical methods to analyze the paired data:\n\nPaired t-test\nSign test\nWilcoxon signed-rank test\n\n\nPaired t-test\nWe start by performing a paired t-test to compare the average pocket depth before and after the intervention. To conduct this analysis in SPSS, follow these steps:\n\nPerform the Test: Go to Analyze &gt; Compare Means &gt; Paired-Samples T Test.\nSelect Variables: Move pocket_depth_before and pocket_depth_after to the Paired Variables box.\nRun the Test: Click OK to run the analysis.\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nBased on the results of the paired t-test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\nChecking of assumptions\nTo determine whether it is appropriate to apply the paired t-test to these data, we need to verify that the differences in pocket depth before and after the intervention are normally distributed. We can visually inspect the distribution of differences using a histogram.\nFirst, we need to create a new variable that calculates the difference between the two measurements. In SPSS, a new variable can be created by navigating to Transform &gt; Compute Variable. Enter a name for the new variable (e.g., diff) in the Target Variable field and enter the expression pocket_depth_after - pocket_depth_before in the Numeric Expression field. Then, click OK to create the new variable.\nNext, we can create a histogram of the differences in pocket depth to visually assess the normality of the distribution by following these steps:\n\nGo to Graphs &gt; Legacy Dialogs &gt; Histogram.\nSelect the diff variable as the Variable and click OK.\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nBased on the histogram, do the differences in pocket depth appear to be approximately normally distributed?\n\n\n\n\n\nSign test and Wilcoxon signed-rank test\nThe sign test is a non-parametric test used to compare two related samples. It is based on the signs of the differences between the pairs of observations. We will apply the sign test to the pocket depth data to determine whether the intervention has a significant effect.\nThe Wilcoxon signed-rank test is another non-parametric test used to compare two related samples. It is based on the ranks of the absolute differences between the pairs of observations. In this case, the sign test is more appropriate because the Wilcoxon signed-rank test requires the assumption of symmetry in the distribution of differences, whereas the previously constructed histogram suggests that the distribution of these differences is left-skewed.\nTo perform the sign test and the Wilcoxon signed-rank test in SPSS, follow these steps:\n\nPerform the Test: Nonparametric Tests &gt; Legacy Dialogs &gt; 2 Related Samples.\nSelect Variables: Move pocket_depth_before and pocket_depth_after to the Test Pairs box.\nRun the Test: Check the Sign box as well as the Wilcoxon box and click OK to run the analysis.\n\n\n\n\n\n\n\nImportantQuestion 12\n\n\n\nBased on the results of the sign test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\n\n\n\n\n\nImportantQuestion 13\n\n\n\nBased on the results of the Wilcoxon signed-rank test, can we conclude that the intervention significantly reduces pocket depth?"
  },
  {
    "objectID": "SPSS_lab2.html",
    "href": "SPSS_lab2.html",
    "title": "Medical Statistics – Lab 2",
    "section": "",
    "text": "Welcome to lab 2 in the medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset. As a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Means",
    "text": "Point Estimates and 95% Confidence Intervals for Population Means\nWe will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nGo to Analyze =&gt; Descriptive Statistics =&gt; Frequencies. Select the variable ‘birth weight in grams’ from the list on the left. Then, uncheck ‘Display frequency tables.’ A frequency table shows how many individuals have a particular score, but due to the large number of different scores in this case, it would be quite overwhelming (you can keep it checked if you wish to see it).\nNext, click on the ‘Statistics’ button. Here you can specify which statistics you want for the selected variable(s). Select mean, standard deviation, and S.E. mean. Press ‘Continue’.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nCalculate the corresponding 95% confidence interval based on the normal approximation.\n\n\nYou can also use SPSS to calculate the 95% confidence interval. To do this, go to Analyze =&gt; Descriptive Statistics =&gt; Explore. Click on ‘Statistics,’ where you can specify the confidence interval you want to calculate. The default is set to 95%, so no changes are needed. SPSS uses the t-distribution to calculate this confidence interval, which provides a more accurate estimate when the population standard deviation is unknown and the sample size is small. Afterward, select ‘Statistics’ under ‘Display’ to ensure the output contains only the desired descriptive statistics. Press ‘OK’ to generate the output.”\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?"
  },
  {
    "objectID": "SPSS_lab2.html#one-sample-t-test",
    "href": "SPSS_lab2.html#one-sample-t-test",
    "title": "Medical Statistics – Lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\nTo determine whether the population mean birth weight differs significantly from a hypothesized value of 3000 grams, we conduct a one-sample t-test. To do this, go to Analyze =&gt; Compare Means =&gt; One-Sample t-Test. Select the variable ‘birth weight in grams’ and place it under ‘Test Variables’. We want to determine if the population mean significantly differs from the threshold value of 3000 grams. Enter ‘3000’ as the test value. Press ‘OK’. Now you will see the result of the t-test in your output.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\nOne of the assumptions underlying the one-sample t-test is that the data are normally distributed. We can check this assumption by creating a histogram. To do this. go to Graphs =&gt; Legacy Dialoges =&gt; Histrogram.... Select the variable ‘birth weight in grams’ and place it under ‘Variable’. Check the ‘Diplay normal curve’ checkbox and press ‘OK’.\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?"
  },
  {
    "objectID": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "SPSS_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable that takes a value 1 if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of 0 otherwise.\nWe start by making a frequency table to calculate the frequency of each category of the ‘low’ variable:\n\nGo to Analyze =&gt; Descriptive Statistics =&gt; Frequencies...\nSelect the variable ‘low’ and move it to ‘Variable(s)’\nPress ‘OK’ to obtain the frequency table\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation."
  },
  {
    "objectID": "SPSS_lab2.html#binomial-test",
    "href": "SPSS_lab2.html#binomial-test",
    "title": "Medical Statistics – Lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%:\n\nGo to Analyze =&gt; Compare Means =&gt; One-Sample Proportions....\nSelect the variable ‘low’ and move it to the ‘Test Variable List’.\nUnder ‘Define Success’, select ‘Value(s)’ and enter the number 1 to indicate that having a low birth weight baby is the event of interest.\nClick on the ‘Test’ button to open the dialog box, select ‘Exact Binomial’ as the test and enter 0.3 as the test value. Press the ‘Continue’ button.\nClick ‘OK’ to run the test.\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50% of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%. Use the exact binomial test for this question.\nHint: to perform this test in SPSS, we need to create a new dataset containing the above observations. To achieve this, construct a new data file containing two variables, like this:\n\n\n\nAlcohol\nnumber\n\n\n\n\n1\n128\n\n\n0\n72\n\n\n\nThen, instruct SPSS to weight the categories (Alcohol) by “number” via Data → Weight cases.\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nNoteDifferences in Two-Sided P-Value Calculation Between SPSS and R\n\n\n\nWhen conducting statistical tests, it is important to understand that different software packages can calculate two-sided p-values in slightly different ways, which may lead to variations in results. A key difference exists between how SPSS and base R handle this calculation:\n\nSPSS often calculates two-sided p-values by doubling the one-sided p-value. Specifically, SPSS determines the probability of the observed outcome in one direction (greater or less than a given value) and then multiplies this value by 2. This approach assumes that the distribution of the test statistic is symmetric under the null hypothesis. While this method is straightforward, it can be misleading if the distribution is skewed or the sample size is small, as it may not fully account for the asymmetry in the data.\nBase R (e.g., the binom.test() function) uses a more exact method for calculating two-sided p-values. R’s approach sums the probabilities of observing outcomes that are as extreme as, or more extreme than, the observed value in both directions (both tails of the distribution). This method does not assume symmetry and provides a more accurate p-value, particularly for small samples or skewed distributions."
  },
  {
    "objectID": "SPSS_Assignment_1.html",
    "href": "SPSS_Assignment_1.html",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "",
    "text": "In this assignment, you will work with the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "SPSS_Assignment_1.html#introduction",
    "href": "SPSS_Assignment_1.html#introduction",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "",
    "text": "In this assignment, you will work with the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "SPSS_Assignment_1.html#dataset-description",
    "href": "SPSS_Assignment_1.html#dataset-description",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Dataset description",
    "text": "Dataset description\nThe Cleveland heart disease dataset originates from the Cleveland Clinic Foundation and focuses on heart disease diagnosis. It includes data from 303 patients on the following variables:\n\nage: Age in years\nsex: Sex (1 = female; 2 = male)\ncp: Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\ntrestbps: Resting blood pressure (mm Hg at hospital admission)\nchol: Serum cholesterol in mg/dl\nfbs: Fasting blood sugar &gt; 120 mg/dl\nrestecg: Resting electrocardiographic results (1 = normal; 2 = ST-T wave abnormality; 3 = left ventricular hypertrophy)\nthalach: Maximum heart rate achieved\nexang: Exercise-induced angina (1 = no; 2 = yes)\noldpeak: ST depression induced by exercise relative to rest\nslope: Slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\nca: Number of major vessels (0-3) colored by fluoroscopy\nthal: Thallium heart scan results (1 = normal; 2 = fixed defect; 3 = reversible defect)\ntarget: Diagnosis of heart disease (1 = heart disease; 2 = no heart disease)"
  },
  {
    "objectID": "SPSS_Assignment_1.html#objectives",
    "href": "SPSS_Assignment_1.html#objectives",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Objectives",
    "text": "Objectives\nIn this assignment, you will explore the Cleveland heart disease dataset to answer two personalized research questions. Your specific research questions are determined by the last two digits of your student (or staff) number (see the lookup table below).\nResearch Question 1 (ANOVA/Kruskal-Wallis): Does maximum heart rate differ across [Grouping variable] categories?\nResearch Question 2 (Chi-Square/Fisher’s Exact): Is the presence of heart disease associated with [Binary predictor]?"
  },
  {
    "objectID": "SPSS_Assignment_1.html#steps-to-complete-the-assignment",
    "href": "SPSS_Assignment_1.html#steps-to-complete-the-assignment",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Steps to complete the assignment",
    "text": "Steps to complete the assignment\n\nStep 1: Load the dataset and identify your personalized research questions\nDownload the Cleveland heart disease dataset (see Downloads section at the end of this document) and open it in SPSS.\nUse the last two digits of your student number to find your personalized research questions in the table below:\n\n\n\nLast 2 digits\nRQ1: Grouping variable\nRQ2: Binary predictor\n\n\n\n\n00–24\nChest pain type (cp)\nSex (sex)\n\n\n25–49\nChest pain type (cp)\nFasting blood sugar (fbs)\n\n\n50–74\nThalassemia (thal)\nSex (sex)\n\n\n75–99\nThalassemia (thal)\nFasting blood sugar (fbs)\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA student with number S2734567 has last two digits 67, which falls in the 50–74 range. Their research questions are:\n\nRQ1: Does maximum heart rate differ across thalassemia categories?\nRQ2: Is the presence of heart disease associated with sex?\n\n\n\n\n\nStep 2: Create a baseline characteristics table\n\nInclude all variables in the dataset apart from the outcome variable target\n\nSummarize demographic variables (e.g., age, sex)\nSummarize clinical variables (e.g., chol, trestbps, thalach, cp)\n\nDecide on suitable summary measures for each variable\n\nUse appropriate measures for continuous variables (e.g., mean, standard deviation, median, interquartile range)\nUse frequency counts and percentages for categorical variables\n\nPresent your table clearly\n\nUse meaningful labels, headings, and clear formatting\n\n\n\n\nStep 3: Perform the analysis for your first research question\nUsing your grouping variable from the lookup table:\n\nVisualize the data\n\nCreate a boxplot to compare the distribution of maximum heart rate across the categories of your grouping variable\n\nCalculate the estimated population means and 95% confidence intervals for maximum heart rate for each category of the grouping variable\nSelect and perform an appropriate test\n\nUse one-way ANOVA if normality and equal variances are met\nUse Kruskal-Wallis test if assumptions are violated\nPerform post-hoc comparisons using Bonferroni adjusted p-values if significant differences are found\n\n\n\n\nStep 4: Perform the analysis for your second research question\nUsing the binary predictor from the lookup table:\n\nSummarize the data\n\nCalculate and report the prevalence of heart disease for each category of your binary predictor\nInclude percentages and 95% confidence intervals for each group\n\nSelect and perform an appropriate test\n\nUse a Chi-Square test of homogeneity or Fisher’s Exact Test, depending on the expected cell counts\n\n\n\n\nStep 5: Write a report\nYour report should be structured in the form of Methods and Results sections, as typically encountered in scientific papers.\n\nMethods\n\nState your personalized research questions (based on your student number)\nOutline the steps taken to analyze the data\nDescribe statistical tests performed, assumptions checked, and adjustments applied\n\nResults\n\nInclude the baseline characteristics table\nPresent key findings for each research question\nInclude visualizations (e.g., boxplots) to support your findings where applicable\n\nFormatting guidelines\n\nProperly label all tables and figures\nLimit the report to 2–3 pages, including visuals and tables"
  },
  {
    "objectID": "SPSS_Assignment_1.html#reporting-examples",
    "href": "SPSS_Assignment_1.html#reporting-examples",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Reporting examples",
    "text": "Reporting examples\nWhen presenting your analysis results, ensure clarity and adherence to proper reporting conventions. Use the following examples as a guide:\n\nThe mean cholesterol levels (95% CI) for the four chest pain types were as follows: typical angina, 245.3 mg/dL (95% CI: 230.1, 260.5); atypical angina, 220.4 mg/dL (95% CI: 205.7, 235.1); non-anginal pain, 230.2 mg/dL (95% CI: 215.6, 244.8); and asymptomatic chest pain, 200.1 mg/dL (95% CI: 185.4, 214.8). A one-way ANOVA was conducted to compare cholesterol levels across these groups, revealing a significant difference, F(3, 299) = 4.32, p = 0.006. Post-hoc pairwise comparisons using Bonferroni-adjusted p-values indicated that patients with typical angina had significantly higher cholesterol levels compared to those with asymptomatic chest pain (adjusted p = 0.015). No other pairwise differences were statistically significant after adjustment.\n\n\nThe prevalence of heart disease was higher among patients older than 65 (68.5%, 95% CI: 60.2%, 76.8%) compared to those 65 or younger (47.2%, 95% CI: 35.6%, 58.8%). Fisher’s Exact Test indicated a significant difference between these groups (p = 0.028)."
  },
  {
    "objectID": "SPSS_Assignment_1.html#submission-instructions",
    "href": "SPSS_Assignment_1.html#submission-instructions",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Submission instructions",
    "text": "Submission instructions\nSubmit the following files as part of your assignment:\n\nThe report: Provide your report in Word or PDF format\nThe SPSS output: Include your SPSS output file (.spv) with all analysis results"
  },
  {
    "objectID": "SPSS_Assignment_1.html#downloads",
    "href": "SPSS_Assignment_1.html#downloads",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Downloads",
    "text": "Downloads\nCleveland heart disease dataset (SPSS format)"
  },
  {
    "objectID": "R_lab8.html",
    "href": "R_lab8.html",
    "title": "Medical Statistics – Lab 8",
    "section": "",
    "text": "Welcome to lab 8. In this lab, we will build prediction models using backward elimination and automated procedures, and we will practice reasoning with causal diagrams (DAGs).\nWe will use the Worcester Heart Attack Study dataset (whas500.sav, available from the Datasets menu) and the following R packages: haven, dplyr, ggplot2, car, and MASS.\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(MASS)\n\nwhas500 &lt;- read_sav(\"whas500.sav\")\nwhas500 &lt;- whas500 |&gt; mutate(across(where(is.labelled), as_factor))"
  },
  {
    "objectID": "R_lab8.html#part-1-building-prediction-models-using-backward-elimination",
    "href": "R_lab8.html#part-1-building-prediction-models-using-backward-elimination",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 1: Building prediction models using backward elimination",
    "text": "Part 1: Building prediction models using backward elimination\nIn this part of the lab, we will build a prediction model for hospital length of stay (los) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\nKey variables in the dataset include:\n\nlos: Length of hospital stay (days, continuous outcome)\nage: Age at hospital admission (years)\ngender: Gender (0 = Male, 1 = Female)\nhr: Initial heart rate (beats per minute)\nsysbp and diasbp: Initial systolic and diastolic blood pressure (mmHg)\nbmi: Body mass index (kg/m^2)\ncvd: Presence of cardiovascular disease (0 = No, 1 = Yes)\nsho: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n\nStep 1: Fit the initial linear regression model\nFit an initial linear regression model for hospital length of stay (los) using lm() with predictors age, gender, hr, sysbp, diasbp, bmi, cvd, and sho. Summarize the model output to inspect coefficients and p-values.\n\n\nStep 2: Eliminate the least significant predictor\nTo identify the least significant predictor, we use the Type III ANOVA table:\n\nSignificance threshold: \\(p &gt; 0.10\\)\nRemove the predictor with the largest p-value above this threshold.\n\nUse the Anova() function from the car package to obtain the Type III ANOVA table (see lab week 6).\n\n\nStep 3: Repeat the steps\nIteratively remove the least significant predictor until all predictors have \\(p &lt; 0.10\\). At each step:\n\nRerun the regression model\nGenerate the Type III ANOVA table\nRemove the least significant predictor\n\n\n\nStep 4: Final model\nPresent the final linear regression model:\n\nReport the final model, including regression coefficients and 95% CIs.\nCreate residual plots to assess the model assumptions (normality, homoscedasticity, linearity).\n\n\n\n\n\n\n\nTip95% CIs for regression coefficients\n\n\n\nTo obtain 95% CIs for the regression coefficients after fitting your final model (e.g., fit &lt;- lm(...)), use:\nconfint(fit)"
  },
  {
    "objectID": "R_lab8.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "href": "R_lab8.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 2: Automated procedures for building prediction models (logistic regression)",
    "text": "Part 2: Automated procedures for building prediction models (logistic regression)\nIn this part, we explore automated procedures for predictor selection in logistic regression prediction models. We use the same WHAS dataset but now focus on predicting in-hospital death (dstat: alive/dead) from candidate predictors.\n\nAutomated model selection (AIC)\nstepAIC() from the MASS package can be used for automated selection based on AIC. Because AIC compares overall model fit (not individual p-values), the selected model may differ from manual backward elimination.\n\n# Create a 0/1 outcome (1 = dead)\nwhas500 &lt;- whas500 |&gt;\n  mutate(dstat01 = ifelse(dstat == \"dead\", 1, 0))\n\nfit_full &lt;- glm(\n  dstat01 ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho,\n  family = binomial,\n  data = whas500\n)\n\nfit_step &lt;- stepAIC(fit_full, direction = \"backward\", trace = FALSE)\nsummary(fit_step)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nInspect your final selected logistic regression model. Which predictors are retained in the prediction model?\n\n\n\n\n\n\n\n\nTipReporting odds ratios (ORs) and 95% CIs\n\n\n\nFor logistic regression, summary() reports regression coefficients on the log-odds scale. For reporting, it is often more useful to report odds ratios (ORs) with 95% confidence intervals (CIs).\nYou can compute these from the summary() output (Wald CI):\n\ns &lt;- summary(fit_step)$coefficients\n\nOR &lt;- exp(s[, \"Estimate\"])\nCI_lower &lt;- exp(s[, \"Estimate\"] - 1.96 * s[, \"Std. Error\"])\nCI_upper &lt;- exp(s[, \"Estimate\"] + 1.96 * s[, \"Std. Error\"])\n\ncbind(OR = OR, CI_lower = CI_lower, CI_upper = CI_upper)\n\nOr compute them more directly by exponentiating the coefficient confidence intervals:\n\nexp(cbind(OR = coef(fit_step), confint.default(fit_step)))"
  },
  {
    "objectID": "R_lab8.html#part-3-causal-diagrams",
    "href": "R_lab8.html#part-3-causal-diagrams",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 3: Causal diagrams",
    "text": "Part 3: Causal diagrams\nFor each of the exercises below:\n\nTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\nCheck your answer using the DAGitty webtool\n\n\nExercise 1\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n\n\nDAG exercise 1\n\n\n\n\nExercise 2\nIn the graph depicted below, what happens when you additionally adjust for v5?\n\n\n\nDAG exercise 2\n\n\n\n\nExercise 3\nThis diagram is slightly different: v1 now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of v1 on O?\n\n\n\nDAG exercise 3\n\n\n\n\nExercise 4\nNow, v2 is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of v2 on O?\n\n\n\nDAG exercise 4\n\n\n\n\nExercise 5\nBack to the first DAG. However, v2 is now unmeasured. Can we still obtain an unconfounded estimate of the effect of E on O?\n\n\n\nDAG exercise 5\n\n\n\n\nExercise 6\nSee the DAG below: you adjusted for v5. What would be the consequence of this action?\n\n\n\nDAG exercise 6"
  },
  {
    "objectID": "R_lab6.html",
    "href": "R_lab6.html",
    "title": "Medical Statistics – Lab 6",
    "section": "",
    "text": "Welcome to lab 6 on correlation and linear regression. In today’s exercises, we will be analyzing a dataset named pockets.sav, which you can download from the Datasets menu. This dataset contains measurements of periodontal pocket depth for a group of individuals, along with several demographic and lifestyle variables.\nBelow is an overview of the variables we will be working with:\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(ggplot2) # for data visualization\nlibrary(car)    # for calculating type-III ANOVA tables\n\n# Load the dataset\npockets &lt;- read_sav(\"pockets.sav\")\n\n# Convert labeled variables to factors\npockets &lt;- pockets %&gt;%\n  mutate(across(where(is.labelled), as_factor))"
  },
  {
    "objectID": "R_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "R_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 1: Pearson’s correlation coefficient and simple linear regression",
    "text": "Part 1: Pearson’s correlation coefficient and simple linear regression\nIn this section, we will investigate whether age is associated with pocket depth. We start by creating a scatterplot to visualize the relationship between pocketdepth and age:\n\n# Scatterplot of pocket depth vs. age\nggplot(pockets, aes(x = age, y = pocketdepth)) +\n  geom_point() +\n  labs(\n    x     = \"Age (years)\",\n    y     = \"Pocket Depth (mm)\",\n    title = \"Scatterplot of Pocket Depth vs Age\"\n  )\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\nPearson’s Correlation Coefficient\nTo quantify the strength and direction of the linear relationship between age and pocketdepth, we can calculate Pearson’s correlation coefficient:\n\n# Calculate Pearson's correlation coefficient\ncor(pockets$age, pockets$pocketdepth, use=\"complete.obs\")\n\nThe argument use = \"complete.obs\" tells R to exclude any missing values when calculating the correlation coefficient. In this case, there are no missing values in the age and pocketdepth variables, meaning that the argument is not strictly necessary. However, it is good practice to include it to avoid potential issues with missing data in other datasets.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\nWe can also test whether the correlation coefficient is significantly different from zero using a hypothesis test. The null hypothesis is that the correlation coefficient is zero (i.e., no linear relationship between the variables), and the alternative hypothesis is that the correlation coefficient is not zero. In R, we can perform this test using the cor.test() function:\n\n# Test the significance of the correlation coefficient\ncor_test &lt;- cor.test(pockets$age, pockets$pocketdepth)\ncor_test\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\nFitting a Simple Linear Regression Model\nNext, we fit a simple linear regression model to quantify the relationship between age and pocketdepth. In R, this can be achieved using the lm() function:\n\n# Fit a simple linear regression model\nmodel_slr &lt;- lm(pocketdepth ~ age, data = pockets)\n\n# Print the summary of the model\nsummary(model_slr)\n\nThe formula pocketdepth ~ age specifies that pocketdepth is the dependent variable and age is the independent variable. The intercept term is included by default in the linear regression model and is therefore not explicitly specified in the formula.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\nAssumption Checking\nTo obtain diagnostic plots for the simple linear regression model, we can supply the fitted model to the plot() function. By default, R will generate four diagnostic plots. In this lab, we are going to focus on the following two plots:\n\nQ-Q Plot: This plot helps us assess the normality of the residuals. This plot can be obtained by setting which = 2 in the plot() function.\nResiduals vs. Fitted: This plot helps us check for homoscedasticity (constant variance) and linearity assumptions. This plot can be obtained by setting which = 1 in the plot() function.\n\n\nNormality of Residuals\n\nplot(model_slr, which = 2)  # Q-Q plot\n\nIn addition to the Q-Q plot, we can also create a histogram of the residuals to visually inspect their distribution:\n\n# Histogram of residuals\nhist(residuals(model_slr), breaks = 15, col = \"lightblue\", \n     border = \"black\", main = \"Histogram of Residuals\", \n     xlab = \"Residuals\")\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\nHomoscedasticity and linearity\n\nplot(model_slr, which = 1)  # residuals vs fitted\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\nThe red line in R’s default residual-versus-fitted plot is a LOESS (locally estimated scatterplot smoothing) curve that shows the average trend in the residuals. It helps you see if there is a systematic pattern (e.g., curvature) that might indicate the linear model is misspecified. Ideally, you want that red line to be close to horizontal (i.e., around zero) with no strong curvature, suggesting that the linear fit is appropriate and there’s no obvious nonlinearity or other systematic pattern left in the residuals.\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?"
  },
  {
    "objectID": "R_lab6.html#part-2-ancova-analysis-of-covariance",
    "href": "R_lab6.html#part-2-ancova-analysis-of-covariance",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\nIn this section, we will fit an ANCOVA model to determine whether alcohol consumption is associated with pocket depth, controlling for age.\n\nExploratory Data Analysis\nWe start by creating a scatterplot to visualize the relationship between age and pocketdepth, using different colors to represent the levels of alcohol:\n\n# Scatterplot of pocketdepth vs. age, colored by alcohol\nggplot(pockets, aes(x = age, y = pocketdepth, color = alcohol)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth vs. Age by Alcohol Consumption\", \n       x = \"Age\", y = \"Pocket Depth\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nImportantQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\nDummy Coding for alcohol\nThe variable alcohol has three categories (e.g., \"None\", \"1-2 glasses/day\", and \"&gt;2 glasses/day\"), meaning that we need to create two dummy variables to represent these categories in the model. R automatically generates these dummy variables once alcohol is recognized as a factor. To check if alcohol is a factor, we can use the is.factor() function:\n\n# Check whether alcohol is a factor\nis.factor(pockets$alcohol)\n\nIn this case, the output should be TRUE, given that we converted all categorical variables to factors when loading the dataset. Should alcohol not be a factor, we can either convert it to a factor using the factor() function or specify it as a factor in the model formula.\n\n\nFitting the ANCOVA Model\nTo fit the ANCOVA model, we can use the lm() function in R. The formula for the ANCOVA model is specified as pocketdepth ~ age + alcohol, where age is the continuous predictor and alcohol is the categorical predictor.\n\nmodel_ancova &lt;- lm(pocketdepth ~ age + alcohol, data = pockets)\nsummary(model_ancova)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that R uses the first level of a factor (alphabetically or numerically) as the default reference category. In this dataset, “None” is the reference category.\n\n\n\n\n\n\n\n\nImportantQuestion 13\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nImportantQuestion 14\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\nTo test the overall significance of the alcohol variable as a predictor of pocketdepth, we construct an analysis of variance (ANOVA) table. The ANOVA table summarizes how much each term in a linear regression model contributes to explaining the overall variation in the response variable. There are different ways to construct this table depending on how the sum of squares is partitioned among model terms. A common approach is Type III ANOVA, which evaluates each variable or interaction after all other terms have been accounted for. Each effect is tested as if it were entered last, so its sum of squares reflects the unique contribution of that variable or interaction beyond what is already explained by the remaining terms.\nTo obtain the type III ANOVA table, we use the Anova() function from the car package. This function provides a more detailed ANOVA output compared to the base R anova() function, and has an argument type that allows you to specify the type of sum of squares to use:\n\nAnova(model_ancova, type=\"III\")\n\n\n\n\n\n\n\nImportantQuestion 15\n\n\n\nBased on the ANOVA table, is the alcohol variable significantly associated with pocketdepth after accounting for age?\n\n\n\n\nModel Diagnostics\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model."
  },
  {
    "objectID": "R_lab6.html#part-3-interactions-in-ancova",
    "href": "R_lab6.html#part-3-interactions-in-ancova",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\nIn some cases, the relationship between the outcome variable and a predictor may depend on the level of another predictor. This is known as an interaction effect. In the context of ANCOVA, we can test for interactions between the continuous predictor (age) and the categorical predictor (alcohol).\n\nFitting the Interaction Model\nTo include an interaction term in the model, we can use the : operator in the formula. That is, the interaction term between age and alcohol can be specified as age:alcohol. With this term, the interaction model can be specified as pocketdepth ~ age + alcohol + age:alcohol. A short form for specifying the interaction model is pocketdepth ~ age * alcohol, which includes both the main effects of age and alcohol and their interaction terms.\n\nmodel_interaction &lt;- lm(pocketdepth ~ age * alcohol, data = pockets)\nsummary(model_interaction)\n\nTo test the significance of the interaction term, we again use the Anova() function from the car package:\n\nAnova(model_interaction, type=\"III\")\n\n\n\n\n\n\n\nImportantQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?"
  },
  {
    "objectID": "R_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "R_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions."
  },
  {
    "objectID": "R_lab3.html",
    "href": "R_lab3.html",
    "title": "Medical Statistics – Lab 3",
    "section": "",
    "text": "Welcome to lab 3 in the medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can downloaded from the Dataset section of the menu. In this lab, we will use the following R packages: haven, dplyr, ggplot2, car, and dunn.test. The first three packages were also used in the previous labs and therefore already installed on your computer. For car (used to perform Levene’s test) and dunn.test (used to perform Dunn’s post-hoc test) you may have to download and install them first by running install.packages(c(\"car\", \"dunn.test\")).\n# Load the required packages\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(dunn.test)\n\n# Load the dataset\n#lowbwt &lt;- read_sav(\"lowbwt.sav\")\nlowbwt &lt;- read_sav(\"datasets/lowbwt.sav\")\n\n# Convert all variables to factors where needed\nlowbwt &lt;- lowbwt |&gt; mutate(across(where(is.labelled), as_factor))\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "R_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "R_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 1: Independent Samples t-test and Mann-Whitney U Test",
    "text": "Part 1: Independent Samples t-test and Mann-Whitney U Test\nIn this part of the lab, we will examine the effect of smoking during pregnancy on birth weight.\n\nExploratory data analysis\nWe will start by creating a boxplot to visualize the distribution of birth weights for mothers who smoked and those who did not.\n\n# Create boxplot comparing birth weights for mothers who smoked and those who did not\nggplot(lowbwt, aes(x = smoke, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Smoking Status\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\nIndependent samples t-test\nNext, we will perform an independent samples t-test to compare the mean birth weights between mothers who smoked and those who did not. In R, this can be done using the t.test() function:\n\n# Perform independent samples t-test\nindependent_t_test &lt;- t.test(bwt ~ smoke, data = lowbwt, var.equal = TRUE)\n\n# Print results\nprint(independent_t_test)\n\nExplanation:\n\nThe first argument bwt ~ smoke specifies the formula for the test, indicating that we are comparing the birth weights (bwt) between the two groups defined by the smoke variable.\nThe second argument data = lowbwt specifies the dataset.\nThe argument var.equal = TRUE indicates that we are assuming equal variances in the two groups, meaning that the classical independent samples t-test is performed. If you suspect unequal variances, the Welch t-test can be conducted by setting var.equal = FALSE.\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\nChecking of assumptions\nTo assess whether the assumption of normality holds for the outcome variable in both groups, we create the following plot:\n\n# Create histograms of birth weight by smoking status\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(binwidth = 200, fill = \"blue\", colour=\"black\", alpha = 0.7) +\n  facet_wrap(~smoke) +\n  labs(x = \"Birth Weight (grams)\", title = \"Histograms of Birth Weight by Smoking Status\")\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\nWe also need to check whether the assumption of a common population standard deviation holds. This can be done by performing Levene’s test:\n\n# Perform Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(bwt ~ smoke, data = lowbwt)\nlevene_test\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on Levene’s test, does the assumption of equal variances hold?\n\n\n\n\n95% Confidence Interval for the mean difference\nIn addition to performing hypothesis tests, it is often informative to estimate the effect size and its uncertainty. One way to do this is by calculating a confidence interval for the mean difference in birth weight between the two groups. The 95% confidence interval is included in the default output of the t.test() function, so in principle we could extract it from there. As an exercise, we are also going to calculate it manually based on the formulas provided in the lecture/course syllabus.\nTo calculate the summary statistics required for the manual calculation, we use the following code:\n\n# Calculate summary statistics by smoking status\nsummary_stats &lt;- lowbwt |&gt;\n  group_by(smoke) |&gt;\n  summarise(\n    mean = mean(bwt),\n    sd = sd(bwt),\n    n = n(),\n    .groups = \"drop\"\n  )\nsummary_stats\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\nMann-Whitney U Test\nIn case the assumptions of the independent samples t-test are violated, we can use the Mann-Whitney U test as a non-parametric alternative. This test can be performed in R using the wilcox.test() function:\n\n# Perform Mann-Whitney U test\nwilcoxon_test &lt;- wilcox.test(bwt ~ smoke, data = lowbwt)\nwilcoxon_test\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?"
  },
  {
    "objectID": "R_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "R_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\nIn this part of the lab, we are going to examine the effect of ethnicity on birth weight.\n\nExplaratory data analysis\n\n# Create boxplot comparing birth weights across ethnic groups\nggplot(lowbwt, aes(x = ethnicity, y = bwt)) +\n  geom_boxplot() +\n  labs(x = \"Ethnicity\", y = \"Birth Weight (grams)\", title = \"Birth Weight by Ethnicity\")\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\nOne-way ANOVA\nTo test the null hypothesis that the mean birth weights are equal across all ethnic groups, we can perform a one-way ANOVA using the aov() function:\n\n# Perform One-Way ANOVA\nanova_result &lt;- aov(bwt ~ ethnicity, data = lowbwt)\n\n# Print summary of ANOVA\nsummary(anova_result)\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\nPost-hoc tests\nIf the one-way ANOVA indicates a statistically significant difference in birth weight across ethnic groups, we can perform Bonferroni-corrected post-hoc tests to determine which specific groups differ from each other. In R, this can be done using the pairwise.t.test() function:\n\n# Perform pairwise comparisons with Bonferroni correction\nposthoc &lt;- pairwise.t.test(lowbwt$bwt, lowbwt$ethnicity, p.adjust.method = \"bonferroni\")\nposthoc\n\nExplanation:\n\nThe first argument of the pairwise.t.test() function specifies the outcome variable, which is the column of the lowbwt dataset that contains the birth weight values\nThe second argument specifies the grouping variable, which is the column of the lowbwt dataset that contains the ethnicity values\nThe p.adjust.method = \"bonferroni\" argument specifies that the p-values should be adjusted using the Bonferroni correction\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nWhat conclusions can be drawn from the post-hoc comparisons?\n\n\n\n\nChecking of assumptions\nTo assess whether the results of the one-way ANOVA are valid, we need to check the assumptions of normality and homogeneity of variances. This step is analogous to the previous examples, and is left as an exercise.\n\n\nKruskal-Wallis Test\nIf the assumptions of the one-way ANOVA are violated, we can use the Kruskal-Wallis test as a non-parametric alternative. The test can be performed in R using the kruskal.test() function:\n\n# Perform Kruskal-Wallis test\nkruskal_test &lt;- kruskal.test(bwt ~ ethnicity, data = lowbwt)\nkruskal_test\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?\n\n\n\n\nPost-hoc tests for Kruskal-Wallis\nIf the Kruskal-Wallis test indicates a statistically significant difference between groups, we can perform Dunn’s test as a post-hoc analysis to determine which specific groups differ from each other. Dunn’s test is the non-parametric equivalent of the pairwise t-tests used after ANOVA.\n\n# Perform Dunn's test with Bonferroni correction\ndunn_test &lt;- dunn.test(lowbwt$bwt, lowbwt$ethnicity, method = \"bonferroni\", altp = TRUE)\ndunn_test\n\nExplanation:\n\nThe first argument specifies the outcome variable (birth weight)\nThe second argument specifies the grouping variable (ethnicity)\nThe method = \"bonferroni\" argument specifies that the p-values should be adjusted using the Bonferroni correction\nThe altp = TRUE argument ensures that two-sided p-values are reported\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nWhat conclusions can be drawn from Dunn’s test? Are these consistent with the post-hoc comparisons from the one-way ANOVA?"
  },
  {
    "objectID": "R_lab3.html#part-3-unguided-exercises",
    "href": "R_lab3.html#part-3-unguided-exercises",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?"
  },
  {
    "objectID": "R_lab1.html",
    "href": "R_lab1.html",
    "title": "Medical Statistics – Lab 1",
    "section": "",
    "text": "Welcome to lab 1 in the medical statistics course. In this lab, we will explore descriptive statistics and probability calculations for random variables. We will use an example dataset to practice summarizing continuous and categorical variables, and introduce some basic concepts of probability distributions."
  },
  {
    "objectID": "R_lab1.html#descriptive-analysis-of-continuous-variables",
    "href": "R_lab1.html#descriptive-analysis-of-continuous-variables",
    "title": "Medical Statistics – Lab 1",
    "section": "Descriptive analysis of continuous variables",
    "text": "Descriptive analysis of continuous variables\nLet’s start by calculating the summary statistics for the continuous variable age. We’ll use the summarise() function from the dplyr package, which allows us to calculate multiple summary statistics in a single, organized call:\n\nage_summary &lt;- lowbwt |&gt;\n  summarise(\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    median = median(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    iqr = IQR(age, na.rm = TRUE)\n  )\n\nage_summary\n\nThe summarise() function creates a new data frame containing the summary statistics you specify. The functions mean(), sd(), median(), and IQR() are built-in base R functions that work within summarise(). The quantile() function calculates the quartiles: Q1 (25th percentile) and Q3 (75th percentile). The na.rm = TRUE argument ensures that any missing values are ignored in the calculations.\n\n\n\n\n\n\nNoteIQR vs [Q1, Q3]\n\n\n\nNote the difference between IQR and the quartile interval:\n\nIQR (Interquartile Range) = Q3 - Q1, which is a single number representing the spread of the middle 50% of the data\n[Q1, Q3] is the interval itself, showing the actual range where the middle 50% of observations fall\n\nIn scientific papers, it’s more common to report the median along with [Q1, Q3] (e.g., “median age: 23 years [19, 26]”) rather than reporting the IQR as a single value.\n\n\nTo decide which summary measures (mean and standard deviation, or median and [Q1, Q3]) are appropriate to report, we need to understand the shape of the distribution of the age variable. We do this by creating a histogram:\n\nggplot(lowbwt, aes(x = age)) +\n  geom_histogram(binwidth = 2, \n                 fill = \"blue\", \n                 color = \"black\") +\n  labs(title = \"Histogram of Age\", \n       x = \"Age of Mother (years)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, Q1, Q3, and IQR for the variable lwt using the summarise() function. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report."
  },
  {
    "objectID": "R_lab1.html#descriptive-analysis-of-categorical-variables",
    "href": "R_lab1.html#descriptive-analysis-of-categorical-variables",
    "title": "Medical Statistics – Lab 1",
    "section": "Descriptive analysis of categorical variables",
    "text": "Descriptive analysis of categorical variables\nLet’s move on to analyzing the categorical variables. We’ll calculate the frequency and percentage of mothers who smoked during pregnancy (smoke) using the count() and mutate() functions from dplyr package:\n\nsmoke_summary &lt;- lowbwt |&gt;\n  count(smoke) |&gt;\n  mutate(percentage = n / sum(n) * 100)\n\nsmoke_summary\n\nThe count() function calculates the frequency of each category and automatically creates a new column called n that contains these frequencies. The mutate() function then adds another column with the percentages, calculated by dividing each frequency (n) by the total number of observations (sum(n)) and multiplying by 100.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable history of hypertension (ht).\n\n\nIn addition to calculating frequencies and percentages, it can also be helpful to visualize categorical data. One common way to do this is by creating a bar chart. Below is an example of how you can create a bar chart for the smoke variable:\n\nggplot(lowbwt, aes(x = factor(smoke))) +\n  geom_bar(fill = \"blue\", color = \"black\") +\n  labs(title = \"Smoking Status During Pregnancy\", \n       x = \"Smoking Status\", \n       y = \"Frequency\") +\n  theme_minimal()\n\nThis bar chart shows the frequency of mothers who smoked and those who did not during pregnancy. Similarly, you can create a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension."
  },
  {
    "objectID": "R_lab1.html#binomial-distribution",
    "href": "R_lab1.html#binomial-distribution",
    "title": "Medical Statistics – Lab 1",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial distribution represents the number of successes in a fixed number of independent trials, each with the same probability of success. For example, if we have 10 patients and we want to know the probability that exactly 3 of them respond to a given treatment, where the response rate is known to be 40%, we can use the dbinom() function:\n\n# Probability that exactly 3 out of 10 patients respond \n# to the treatment (assuming p = 0.4)\np_response_3 &lt;- dbinom(3, size = 10, prob = 0.4)\np_response_3\n\n\nExplanation of dbinom() Arguments\n\nx: The number of successes we are interested in (in this example, 3 patients responding).\nsize: The number of trials, which represents the total number of patients (in this example, 10 patients).\nprob: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nTo calculate cumulative probabilities, use the pbinom() function. For example, to calculate the probability that 3 or fewer patients out of 10 respond to the treatment:\n\np_cum_3 &lt;- pbinom(3, size = 10, prob = 0.4)\np_cum_3\n\n\n\nExplanation of pbinom() Arguments\n\nq: The number of successes we want to calculate the cumulative probability for (in this example, 3 or fewer successes).\nsize: The number of trials, which represents the total number of patients (in this example, 10 patients).\nprob: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nBy default, the pbinom() function calculates the probability that the number of successes is less than or equal to q. However, it is also possible to calculate the probability that the number of successes is greater than q by setting lower.tail = FALSE. The lower.tail argument specifies whether the cumulative probability is calculated from the lower tail (default, TRUE) or the upper tail (FALSE).\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution)."
  },
  {
    "objectID": "R_lab1.html#normal-distribution",
    "href": "R_lab1.html#normal-distribution",
    "title": "Medical Statistics – Lab 1",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nSuppose that we want to calculate the probability that a randomly selected individual has a weight less than or equal to 80 kg, assuming that the distribution of weight in the population follows a normal distribution with mean 72 kg and standard deviation 10 kg.\nTo calculate this probability, we first need to standardize the value using the formula:\n\\[\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{80-72}{10} = 0.8\n\\]\nwhere:\n\nx is the value we want to standardize (in this case, 80 kg).\nμ is the mean of the distribution (in this case, 72 kg).\nσ is the standard deviation of the distribution (in this case, 10 kg).\n\nUsing R, we can then use the pnorm() function to find the corresponding cumulative probability from the standard normal distribution:\n\n# Standardize the value\nz_value &lt;- (80 - 72) / 10\nz_value\n\n# Use the standard normal distribution to find \n# the cumulative probability\ncum_prob_weight &lt;- pnorm(z_value)\ncum_prob_weight\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?"
  },
  {
    "objectID": "lab9_answers.html",
    "href": "lab9_answers.html",
    "title": "Medical Statistics – Answers lab 9",
    "section": "",
    "text": "Call: survfit(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n                miord=first \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    329       5    0.985 0.00674       0.9717        0.998\n    2    324       7    0.964 0.01034       0.9435        0.984\n    3    317       1    0.960 0.01074       0.9397        0.982\n    4    316       2    0.954 0.01150       0.9321        0.977\n    5    314       1    0.951 0.01186       0.9284        0.975\n    6    313       4    0.939 0.01317       0.9137        0.965\n    7    309       3    0.930 0.01406       0.9029        0.958\n   10    306       1    0.927 0.01434       0.8994        0.956\n   11    305       2    0.921 0.01487       0.8923        0.951\n   14    303       2    0.915 0.01538       0.8852        0.946\n   16    301       1    0.912 0.01563       0.8817        0.943\n   17    300       1    0.909 0.01587       0.8782        0.940\n   18    299       2    0.903 0.01634       0.8713        0.935\n   19    297       2    0.897 0.01678       0.8644        0.930\n   22    295       1    0.894 0.01700       0.8609        0.928\n   33    294       2    0.888 0.01742       0.8540        0.922\n   34    292       1    0.884 0.01762       0.8506        0.920\n   37    291       1    0.881 0.01782       0.8472        0.917\n   42    290       1    0.878 0.01802       0.8438        0.914\n   46    289       1    0.875 0.01821       0.8404        0.912\n   57    288       1    0.872 0.01840       0.8370        0.909\n   61    287       1    0.869 0.01858       0.8336        0.906\n   64    286       1    0.866 0.01877       0.8303        0.904\n   69    285       1    0.863 0.01894       0.8269        0.901\n   81    284       1    0.860 0.01912       0.8235        0.898\n   83    283       1    0.857 0.01929       0.8202        0.896\n   88    282       1    0.854 0.01946       0.8168        0.893\n   93    281       1    0.851 0.01963       0.8134        0.890\n   95    280       1    0.848 0.01979       0.8101        0.888\n   97    279       1    0.845 0.01995       0.8068        0.885\n  100    278       1    0.842 0.02011       0.8034        0.882\n  108    277       1    0.839 0.02027       0.8001        0.880\n  109    276       1    0.836 0.02042       0.7968        0.877\n  113    275       1    0.833 0.02057       0.7935        0.874\n  116    274       1    0.830 0.02072       0.7902        0.871\n  117    273       1    0.827 0.02087       0.7868        0.869\n  134    272       1    0.824 0.02101       0.7835        0.866\n  135    271       1    0.821 0.02115       0.7802        0.863\n  137    270       1    0.818 0.02129       0.7770        0.860\n  140    269       1    0.815 0.02143       0.7737        0.858\n  145    268       1    0.812 0.02156       0.7704        0.855\n  146    267       1    0.809 0.02169       0.7671        0.852\n  169    266       2    0.802 0.02195       0.7605        0.847\n  187    264       1    0.799 0.02208       0.7573        0.844\n  192    263       1    0.796 0.02220       0.7540        0.841\n  200    262       1    0.793 0.02232       0.7507        0.838\n  233    261       1    0.790 0.02244       0.7475        0.836\n  235    260       1    0.787 0.02256       0.7442        0.833\n  259    259       2    0.781 0.02279       0.7377        0.827\n  269    257       1    0.778 0.02291       0.7345        0.824\n  274    256       1    0.775 0.02302       0.7312        0.822\n  287    255       1    0.772 0.02313       0.7280        0.819\n  297    254       1    0.769 0.02324       0.7248        0.816\n  313    253       1    0.766 0.02334       0.7215        0.813\n  343    252       1    0.763 0.02345       0.7183        0.810\n  345    251       1    0.760 0.02355       0.7151        0.807\n  358    250       1    0.757 0.02365       0.7119        0.805\n  359    249       2    0.751 0.02385       0.7054        0.799\n  363    247       1    0.748 0.02394       0.7022        0.796\n  382    241       1    0.745 0.02405       0.6989        0.793\n  392    237       1    0.741 0.02415       0.6956        0.790\n  397    236       1    0.738 0.02425       0.6923        0.787\n  405    231       1    0.735 0.02435       0.6889        0.784\n  419    223       1    0.732 0.02447       0.6854        0.781\n  442    215       1    0.728 0.02459       0.6818        0.778\n  446    210       1    0.725 0.02472       0.6781        0.775\n  465    202       1    0.721 0.02485       0.6743        0.772\n  535    186       1    0.718 0.02502       0.6701        0.768\n  537    185       1    0.714 0.02518       0.6659        0.765\n  542    184       1    0.710 0.02534       0.6618        0.761\n  552    181       1    0.706 0.02550       0.6576        0.758\n  559    179       1    0.702 0.02567       0.6533        0.754\n  614    170       1    0.698 0.02584       0.6489        0.750\n  646    168       1    0.694 0.02602       0.6444        0.747\n  654    167       1    0.689 0.02620       0.6400        0.743\n  673    164       1    0.685 0.02637       0.6355        0.739\n  704    162       1    0.681 0.02655       0.6309        0.735\n  714    161       1    0.677 0.02672       0.6264        0.731\n  865    159       1    0.673 0.02688       0.6218        0.727\n  903    158       1    0.668 0.02705       0.6173        0.723\n  920    157       1    0.664 0.02721       0.6128        0.720\n  936    156       1    0.660 0.02737       0.6082        0.716\n  953    155       1    0.655 0.02752       0.6037        0.712\n 1048    154       1    0.651 0.02767       0.5992        0.708\n 1152    137       1    0.646 0.02787       0.5941        0.703\n 1165    132       1    0.642 0.02809       0.5888        0.699\n 1200    123       1    0.636 0.02834       0.5832        0.694\n 1233    116       1    0.631 0.02862       0.5772        0.690\n 1279    105       1    0.625 0.02897       0.5706        0.684\n 1317     98       1    0.619 0.02937       0.5635        0.679\n 1359     90       1    0.612 0.02984       0.5559        0.673\n 1377     87       1    0.605 0.03031       0.5480        0.667\n 1496     71       1    0.596 0.03106       0.5382        0.660\n 1527     70       1    0.588 0.03176       0.5285        0.653\n 1576     69       1    0.579 0.03242       0.5189        0.646\n 1671     68       1    0.571 0.03304       0.5093        0.639\n 1926     51       1    0.559 0.03424       0.4961        0.631\n 2160      9       1    0.497 0.06603       0.3833        0.645\n 2350      3       1    0.331 0.14230       0.1429        0.769\n 2353      2       1    0.166 0.13710       0.0328        0.839\n 2358      1       1    0.000     NaN           NA           NA\n\n                miord=recurrent \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    171       3    0.982  0.0100        0.963        1.000\n    2    168       1    0.977  0.0116        0.954        1.000\n    3    167       2    0.965  0.0141        0.938        0.993\n    5    165       1    0.959  0.0152        0.930        0.989\n    6    164       1    0.953  0.0161        0.922        0.985\n    7    163       3    0.936  0.0188        0.900        0.973\n   10    160       2    0.924  0.0203        0.885        0.965\n   11    158       2    0.912  0.0216        0.871        0.956\n   17    156       1    0.906  0.0223        0.864        0.951\n   18    155       1    0.901  0.0229        0.857        0.947\n   19    154       1    0.895  0.0235        0.850        0.942\n   20    153       2    0.883  0.0246        0.836        0.933\n   22    151       1    0.877  0.0251        0.829        0.928\n   26    150       1    0.871  0.0256        0.823        0.923\n   31    149       1    0.865  0.0261        0.816        0.918\n   32    148       2    0.854  0.0270        0.802        0.908\n   33    146       1    0.848  0.0275        0.796        0.904\n   49    145       1    0.842  0.0279        0.789        0.899\n   52    144       1    0.836  0.0283        0.783        0.894\n   53    143       1    0.830  0.0287        0.776        0.889\n   55    142       1    0.825  0.0291        0.769        0.884\n   57    141       1    0.819  0.0295        0.763        0.879\n   60    140       1    0.813  0.0298        0.756        0.873\n   62    139       1    0.807  0.0302        0.750        0.868\n   64    138       1    0.801  0.0305        0.744        0.863\n   69    137       1    0.795  0.0309        0.737        0.858\n   76    136       1    0.789  0.0312        0.731        0.853\n   91    135       1    0.784  0.0315        0.724        0.848\n  101    134       1    0.778  0.0318        0.718        0.843\n  118    133       1    0.772  0.0321        0.712        0.837\n  129    132       1    0.766  0.0324        0.705        0.832\n  132    131       1    0.760  0.0326        0.699        0.827\n  140    130       1    0.754  0.0329        0.693        0.822\n  143    129       1    0.749  0.0332        0.686        0.816\n  151    128       1    0.743  0.0334        0.680        0.811\n  166    127       1    0.737  0.0337        0.674        0.806\n  187    126       1    0.731  0.0339        0.667        0.801\n  197    125       1    0.725  0.0341        0.661        0.795\n  226    124       1    0.719  0.0344        0.655        0.790\n  289    123       1    0.713  0.0346        0.649        0.785\n  295    122       1    0.708  0.0348        0.643        0.779\n  297    121       1    0.702  0.0350        0.636        0.774\n  312    120       1    0.696  0.0352        0.630        0.768\n  321    119       1    0.690  0.0354        0.624        0.763\n  328    118       1    0.684  0.0355        0.618        0.758\n  354    117       1    0.678  0.0357        0.612        0.752\n  385    114       1    0.672  0.0359        0.606        0.747\n  406    111       1    0.666  0.0361        0.599        0.741\n  422    109       1    0.660  0.0363        0.593        0.735\n  467    103       1    0.654  0.0365        0.586        0.729\n  473    102       1    0.647  0.0367        0.579        0.723\n  479    100       1    0.641  0.0369        0.573        0.717\n  497     98       1    0.634  0.0371        0.566        0.711\n  530     94       1    0.628  0.0373        0.559        0.705\n  562     87       1    0.620  0.0376        0.551        0.699\n  612     84       1    0.613  0.0378        0.543        0.692\n  632     82       1    0.606  0.0381        0.535        0.685\n  644     81       1    0.598  0.0384        0.527        0.678\n  649     80       1    0.591  0.0386        0.520        0.671\n  670     79       1    0.583  0.0388        0.512        0.664\n  718     78       1    0.576  0.0390        0.504        0.658\n  849     77       1    0.568  0.0392        0.496        0.651\n  905     76       1    0.561  0.0394        0.489        0.644\n 1054     75       1    0.553  0.0396        0.481        0.637\n 1065     74       1    0.546  0.0398        0.473        0.630\n 1096     73       1    0.538  0.0399        0.465        0.623\n 1136     68       1    0.530  0.0401        0.457        0.615\n 1159     65       1    0.522  0.0403        0.449        0.608\n 1174     61       1    0.514  0.0406        0.440        0.600\n 1217     57       1    0.505  0.0408        0.431        0.591\n 1232     55       1    0.495  0.0411        0.421        0.583\n 1377     35       1    0.481  0.0423        0.405        0.572\n 1506     29       1    0.465  0.0440        0.386        0.559\n 1536     28       1    0.448  0.0454        0.367        0.547\n 1548     27       1    0.432  0.0467        0.349        0.533\n 1553     26       1    0.415  0.0477        0.331        0.520\n 1577     25       1    0.398  0.0486        0.314        0.506\n 1579     24       1    0.382  0.0494        0.296        0.492\n 1624     23       1    0.365  0.0499        0.279        0.477\n 1627     22       1    0.349  0.0503        0.263        0.463\n 1954     14       1    0.324  0.0525        0.235        0.445\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\n3 years is equal to 3 * 365 = 1095 days. The estimated survival probabilities at 3 years can be read of the Kaplan-Meier table by finding the closest time point less than or equal to 1095 days for each group. This gives an estimated survival probability of 0.651 for patients with a first MI and 0.546 for patients with a recurrent MI.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe Kaplan-Meier curves suggest that patients with a recurrent MI have lower survival probabilities compared to those with a first MI. This indicates a potential difference in survival times between the two groups.\n\n\nTo formally test the difference in survival between the two groups, we can use the logrank test:\n\n\nCall:\nsurvdiff(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n                  N Observed Expected (O-E)^2/E (O-E)^2/V\nmiord=first     329      125    146.1      3.04      9.57\nmiord=recurrent 171       90     68.9      6.43      9.57\n\n Chisq= 9.6  on 1 degrees of freedom, p= 0.002 \n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe p-value from the logrank test is a measure of the evidence against the null hypothesis of no difference in survival between the two groups. A small p-value (typically &lt; 0.05) indicates that there is sufficient evidence to reject the null hypothesis and conclude that there is a significant difference in survival between the groups. In this case, the p-value of 0.002 is less than 0.05, suggesting a significant difference in overall survival between patients with a first MI and those with a recurrent MI.\n\n\n\n\n\n\n\nCall:\ncoxph(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n  n= 500, number of events= 215 \n\n                 coef exp(coef) se(coef)     z Pr(&gt;|z|)   \nmiordrecurrent 0.4266    1.5320   0.1391 3.067  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nmiordrecurrent     1.532     0.6527     1.166     2.012\n\nConcordance= 0.54  (se = 0.017 )\nLikelihood ratio test= 9.14  on 1 df,   p=0.002\nWald test            = 9.41  on 1 df,   p=0.002\nScore (logrank) test = 9.55  on 1 df,   p=0.002\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe HR for patients with a recurrent MI compared to those with a first MI is 1.53 based on the unadjusted Cox regression model. This means that patients with a recurrent MI have a 53% higher risk of death compared to patients with a first MI.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value associated with the miord variable in the Cox regression model is 0.002, which is consistent with the result of the logrank test. Both tests provide evidence of a significant association between MI order and overall survival, indicating that patients with a recurrent MI have a higher risk of death compared to those with a first MI.\n\n\n\n\nCall:\ncoxph(formula = Surv(lenfol, fstat_numeric) ~ miord + age + gender, \n    data = whas500)\n\n  n= 500, number of events= 215 \n\n                    coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nmiordrecurrent  0.188124  1.206983  0.139486  1.349    0.177    \nage             0.066269  1.068514  0.006266 10.576   &lt;2e-16 ***\ngenderfemale   -0.062241  0.939657  0.140627 -0.443    0.658    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nmiordrecurrent    1.2070     0.8285    0.9183     1.586\nage               1.0685     0.9359    1.0555     1.082\ngenderfemale      0.9397     1.0642    0.7133     1.238\n\nConcordance= 0.729  (se = 0.018 )\nLikelihood ratio test= 144.2  on 3 df,   p=&lt;2e-16\nWald test            = 119.7  on 3 df,   p=&lt;2e-16\nScore (logrank) test = 128.4  on 3 df,   p=&lt;2e-16\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nAfter adjusting for gender and age, the HR for patients with a recurrent MI compared to those with a first MI is 1.21, with a corresponding p-value of 0.177. This adjusted HR is lower than the unadjusted HR of 1.53. The change in the HR after adjusting for these variables suggests that gender and age may confound the association between MI order and overall survival. In this case, the mean age of patients with a recurrent MI (73) is higher than that of patients with a first MI (68.2), which could explain the change in the HR after adjusting for age."
  },
  {
    "objectID": "lab9_answers.html#association-between-mi-order-and-overall-survival",
    "href": "lab9_answers.html#association-between-mi-order-and-overall-survival",
    "title": "Medical Statistics – Answers lab 9",
    "section": "",
    "text": "Call: survfit(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n                miord=first \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    329       5    0.985 0.00674       0.9717        0.998\n    2    324       7    0.964 0.01034       0.9435        0.984\n    3    317       1    0.960 0.01074       0.9397        0.982\n    4    316       2    0.954 0.01150       0.9321        0.977\n    5    314       1    0.951 0.01186       0.9284        0.975\n    6    313       4    0.939 0.01317       0.9137        0.965\n    7    309       3    0.930 0.01406       0.9029        0.958\n   10    306       1    0.927 0.01434       0.8994        0.956\n   11    305       2    0.921 0.01487       0.8923        0.951\n   14    303       2    0.915 0.01538       0.8852        0.946\n   16    301       1    0.912 0.01563       0.8817        0.943\n   17    300       1    0.909 0.01587       0.8782        0.940\n   18    299       2    0.903 0.01634       0.8713        0.935\n   19    297       2    0.897 0.01678       0.8644        0.930\n   22    295       1    0.894 0.01700       0.8609        0.928\n   33    294       2    0.888 0.01742       0.8540        0.922\n   34    292       1    0.884 0.01762       0.8506        0.920\n   37    291       1    0.881 0.01782       0.8472        0.917\n   42    290       1    0.878 0.01802       0.8438        0.914\n   46    289       1    0.875 0.01821       0.8404        0.912\n   57    288       1    0.872 0.01840       0.8370        0.909\n   61    287       1    0.869 0.01858       0.8336        0.906\n   64    286       1    0.866 0.01877       0.8303        0.904\n   69    285       1    0.863 0.01894       0.8269        0.901\n   81    284       1    0.860 0.01912       0.8235        0.898\n   83    283       1    0.857 0.01929       0.8202        0.896\n   88    282       1    0.854 0.01946       0.8168        0.893\n   93    281       1    0.851 0.01963       0.8134        0.890\n   95    280       1    0.848 0.01979       0.8101        0.888\n   97    279       1    0.845 0.01995       0.8068        0.885\n  100    278       1    0.842 0.02011       0.8034        0.882\n  108    277       1    0.839 0.02027       0.8001        0.880\n  109    276       1    0.836 0.02042       0.7968        0.877\n  113    275       1    0.833 0.02057       0.7935        0.874\n  116    274       1    0.830 0.02072       0.7902        0.871\n  117    273       1    0.827 0.02087       0.7868        0.869\n  134    272       1    0.824 0.02101       0.7835        0.866\n  135    271       1    0.821 0.02115       0.7802        0.863\n  137    270       1    0.818 0.02129       0.7770        0.860\n  140    269       1    0.815 0.02143       0.7737        0.858\n  145    268       1    0.812 0.02156       0.7704        0.855\n  146    267       1    0.809 0.02169       0.7671        0.852\n  169    266       2    0.802 0.02195       0.7605        0.847\n  187    264       1    0.799 0.02208       0.7573        0.844\n  192    263       1    0.796 0.02220       0.7540        0.841\n  200    262       1    0.793 0.02232       0.7507        0.838\n  233    261       1    0.790 0.02244       0.7475        0.836\n  235    260       1    0.787 0.02256       0.7442        0.833\n  259    259       2    0.781 0.02279       0.7377        0.827\n  269    257       1    0.778 0.02291       0.7345        0.824\n  274    256       1    0.775 0.02302       0.7312        0.822\n  287    255       1    0.772 0.02313       0.7280        0.819\n  297    254       1    0.769 0.02324       0.7248        0.816\n  313    253       1    0.766 0.02334       0.7215        0.813\n  343    252       1    0.763 0.02345       0.7183        0.810\n  345    251       1    0.760 0.02355       0.7151        0.807\n  358    250       1    0.757 0.02365       0.7119        0.805\n  359    249       2    0.751 0.02385       0.7054        0.799\n  363    247       1    0.748 0.02394       0.7022        0.796\n  382    241       1    0.745 0.02405       0.6989        0.793\n  392    237       1    0.741 0.02415       0.6956        0.790\n  397    236       1    0.738 0.02425       0.6923        0.787\n  405    231       1    0.735 0.02435       0.6889        0.784\n  419    223       1    0.732 0.02447       0.6854        0.781\n  442    215       1    0.728 0.02459       0.6818        0.778\n  446    210       1    0.725 0.02472       0.6781        0.775\n  465    202       1    0.721 0.02485       0.6743        0.772\n  535    186       1    0.718 0.02502       0.6701        0.768\n  537    185       1    0.714 0.02518       0.6659        0.765\n  542    184       1    0.710 0.02534       0.6618        0.761\n  552    181       1    0.706 0.02550       0.6576        0.758\n  559    179       1    0.702 0.02567       0.6533        0.754\n  614    170       1    0.698 0.02584       0.6489        0.750\n  646    168       1    0.694 0.02602       0.6444        0.747\n  654    167       1    0.689 0.02620       0.6400        0.743\n  673    164       1    0.685 0.02637       0.6355        0.739\n  704    162       1    0.681 0.02655       0.6309        0.735\n  714    161       1    0.677 0.02672       0.6264        0.731\n  865    159       1    0.673 0.02688       0.6218        0.727\n  903    158       1    0.668 0.02705       0.6173        0.723\n  920    157       1    0.664 0.02721       0.6128        0.720\n  936    156       1    0.660 0.02737       0.6082        0.716\n  953    155       1    0.655 0.02752       0.6037        0.712\n 1048    154       1    0.651 0.02767       0.5992        0.708\n 1152    137       1    0.646 0.02787       0.5941        0.703\n 1165    132       1    0.642 0.02809       0.5888        0.699\n 1200    123       1    0.636 0.02834       0.5832        0.694\n 1233    116       1    0.631 0.02862       0.5772        0.690\n 1279    105       1    0.625 0.02897       0.5706        0.684\n 1317     98       1    0.619 0.02937       0.5635        0.679\n 1359     90       1    0.612 0.02984       0.5559        0.673\n 1377     87       1    0.605 0.03031       0.5480        0.667\n 1496     71       1    0.596 0.03106       0.5382        0.660\n 1527     70       1    0.588 0.03176       0.5285        0.653\n 1576     69       1    0.579 0.03242       0.5189        0.646\n 1671     68       1    0.571 0.03304       0.5093        0.639\n 1926     51       1    0.559 0.03424       0.4961        0.631\n 2160      9       1    0.497 0.06603       0.3833        0.645\n 2350      3       1    0.331 0.14230       0.1429        0.769\n 2353      2       1    0.166 0.13710       0.0328        0.839\n 2358      1       1    0.000     NaN           NA           NA\n\n                miord=recurrent \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    171       3    0.982  0.0100        0.963        1.000\n    2    168       1    0.977  0.0116        0.954        1.000\n    3    167       2    0.965  0.0141        0.938        0.993\n    5    165       1    0.959  0.0152        0.930        0.989\n    6    164       1    0.953  0.0161        0.922        0.985\n    7    163       3    0.936  0.0188        0.900        0.973\n   10    160       2    0.924  0.0203        0.885        0.965\n   11    158       2    0.912  0.0216        0.871        0.956\n   17    156       1    0.906  0.0223        0.864        0.951\n   18    155       1    0.901  0.0229        0.857        0.947\n   19    154       1    0.895  0.0235        0.850        0.942\n   20    153       2    0.883  0.0246        0.836        0.933\n   22    151       1    0.877  0.0251        0.829        0.928\n   26    150       1    0.871  0.0256        0.823        0.923\n   31    149       1    0.865  0.0261        0.816        0.918\n   32    148       2    0.854  0.0270        0.802        0.908\n   33    146       1    0.848  0.0275        0.796        0.904\n   49    145       1    0.842  0.0279        0.789        0.899\n   52    144       1    0.836  0.0283        0.783        0.894\n   53    143       1    0.830  0.0287        0.776        0.889\n   55    142       1    0.825  0.0291        0.769        0.884\n   57    141       1    0.819  0.0295        0.763        0.879\n   60    140       1    0.813  0.0298        0.756        0.873\n   62    139       1    0.807  0.0302        0.750        0.868\n   64    138       1    0.801  0.0305        0.744        0.863\n   69    137       1    0.795  0.0309        0.737        0.858\n   76    136       1    0.789  0.0312        0.731        0.853\n   91    135       1    0.784  0.0315        0.724        0.848\n  101    134       1    0.778  0.0318        0.718        0.843\n  118    133       1    0.772  0.0321        0.712        0.837\n  129    132       1    0.766  0.0324        0.705        0.832\n  132    131       1    0.760  0.0326        0.699        0.827\n  140    130       1    0.754  0.0329        0.693        0.822\n  143    129       1    0.749  0.0332        0.686        0.816\n  151    128       1    0.743  0.0334        0.680        0.811\n  166    127       1    0.737  0.0337        0.674        0.806\n  187    126       1    0.731  0.0339        0.667        0.801\n  197    125       1    0.725  0.0341        0.661        0.795\n  226    124       1    0.719  0.0344        0.655        0.790\n  289    123       1    0.713  0.0346        0.649        0.785\n  295    122       1    0.708  0.0348        0.643        0.779\n  297    121       1    0.702  0.0350        0.636        0.774\n  312    120       1    0.696  0.0352        0.630        0.768\n  321    119       1    0.690  0.0354        0.624        0.763\n  328    118       1    0.684  0.0355        0.618        0.758\n  354    117       1    0.678  0.0357        0.612        0.752\n  385    114       1    0.672  0.0359        0.606        0.747\n  406    111       1    0.666  0.0361        0.599        0.741\n  422    109       1    0.660  0.0363        0.593        0.735\n  467    103       1    0.654  0.0365        0.586        0.729\n  473    102       1    0.647  0.0367        0.579        0.723\n  479    100       1    0.641  0.0369        0.573        0.717\n  497     98       1    0.634  0.0371        0.566        0.711\n  530     94       1    0.628  0.0373        0.559        0.705\n  562     87       1    0.620  0.0376        0.551        0.699\n  612     84       1    0.613  0.0378        0.543        0.692\n  632     82       1    0.606  0.0381        0.535        0.685\n  644     81       1    0.598  0.0384        0.527        0.678\n  649     80       1    0.591  0.0386        0.520        0.671\n  670     79       1    0.583  0.0388        0.512        0.664\n  718     78       1    0.576  0.0390        0.504        0.658\n  849     77       1    0.568  0.0392        0.496        0.651\n  905     76       1    0.561  0.0394        0.489        0.644\n 1054     75       1    0.553  0.0396        0.481        0.637\n 1065     74       1    0.546  0.0398        0.473        0.630\n 1096     73       1    0.538  0.0399        0.465        0.623\n 1136     68       1    0.530  0.0401        0.457        0.615\n 1159     65       1    0.522  0.0403        0.449        0.608\n 1174     61       1    0.514  0.0406        0.440        0.600\n 1217     57       1    0.505  0.0408        0.431        0.591\n 1232     55       1    0.495  0.0411        0.421        0.583\n 1377     35       1    0.481  0.0423        0.405        0.572\n 1506     29       1    0.465  0.0440        0.386        0.559\n 1536     28       1    0.448  0.0454        0.367        0.547\n 1548     27       1    0.432  0.0467        0.349        0.533\n 1553     26       1    0.415  0.0477        0.331        0.520\n 1577     25       1    0.398  0.0486        0.314        0.506\n 1579     24       1    0.382  0.0494        0.296        0.492\n 1624     23       1    0.365  0.0499        0.279        0.477\n 1627     22       1    0.349  0.0503        0.263        0.463\n 1954     14       1    0.324  0.0525        0.235        0.445\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\n3 years is equal to 3 * 365 = 1095 days. The estimated survival probabilities at 3 years can be read of the Kaplan-Meier table by finding the closest time point less than or equal to 1095 days for each group. This gives an estimated survival probability of 0.651 for patients with a first MI and 0.546 for patients with a recurrent MI.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe Kaplan-Meier curves suggest that patients with a recurrent MI have lower survival probabilities compared to those with a first MI. This indicates a potential difference in survival times between the two groups.\n\n\nTo formally test the difference in survival between the two groups, we can use the logrank test:\n\n\nCall:\nsurvdiff(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n                  N Observed Expected (O-E)^2/E (O-E)^2/V\nmiord=first     329      125    146.1      3.04      9.57\nmiord=recurrent 171       90     68.9      6.43      9.57\n\n Chisq= 9.6  on 1 degrees of freedom, p= 0.002 \n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe p-value from the logrank test is a measure of the evidence against the null hypothesis of no difference in survival between the two groups. A small p-value (typically &lt; 0.05) indicates that there is sufficient evidence to reject the null hypothesis and conclude that there is a significant difference in survival between the groups. In this case, the p-value of 0.002 is less than 0.05, suggesting a significant difference in overall survival between patients with a first MI and those with a recurrent MI.\n\n\n\n\n\n\n\nCall:\ncoxph(formula = Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n  n= 500, number of events= 215 \n\n                 coef exp(coef) se(coef)     z Pr(&gt;|z|)   \nmiordrecurrent 0.4266    1.5320   0.1391 3.067  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nmiordrecurrent     1.532     0.6527     1.166     2.012\n\nConcordance= 0.54  (se = 0.017 )\nLikelihood ratio test= 9.14  on 1 df,   p=0.002\nWald test            = 9.41  on 1 df,   p=0.002\nScore (logrank) test = 9.55  on 1 df,   p=0.002\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe HR for patients with a recurrent MI compared to those with a first MI is 1.53 based on the unadjusted Cox regression model. This means that patients with a recurrent MI have a 53% higher risk of death compared to patients with a first MI.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value associated with the miord variable in the Cox regression model is 0.002, which is consistent with the result of the logrank test. Both tests provide evidence of a significant association between MI order and overall survival, indicating that patients with a recurrent MI have a higher risk of death compared to those with a first MI.\n\n\n\n\nCall:\ncoxph(formula = Surv(lenfol, fstat_numeric) ~ miord + age + gender, \n    data = whas500)\n\n  n= 500, number of events= 215 \n\n                    coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nmiordrecurrent  0.188124  1.206983  0.139486  1.349    0.177    \nage             0.066269  1.068514  0.006266 10.576   &lt;2e-16 ***\ngenderfemale   -0.062241  0.939657  0.140627 -0.443    0.658    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nmiordrecurrent    1.2070     0.8285    0.9183     1.586\nage               1.0685     0.9359    1.0555     1.082\ngenderfemale      0.9397     1.0642    0.7133     1.238\n\nConcordance= 0.729  (se = 0.018 )\nLikelihood ratio test= 144.2  on 3 df,   p=&lt;2e-16\nWald test            = 119.7  on 3 df,   p=&lt;2e-16\nScore (logrank) test = 128.4  on 3 df,   p=&lt;2e-16\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nAfter adjusting for gender and age, the HR for patients with a recurrent MI compared to those with a first MI is 1.21, with a corresponding p-value of 0.177. This adjusted HR is lower than the unadjusted HR of 1.53. The change in the HR after adjusting for these variables suggests that gender and age may confound the association between MI order and overall survival. In this case, the mean age of patients with a recurrent MI (73) is higher than that of patients with a first MI (68.2), which could explain the change in the HR after adjusting for age."
  },
  {
    "objectID": "lab7_answers.html",
    "href": "lab7_answers.html",
    "title": "Medical Statistics – Answers lab 7",
    "section": "",
    "text": "alive dead\n  male     282   18\n  female   179   21\n\n\n        \n         alive  dead\n  male   0.940 0.060\n  female 0.895 0.105\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe proportion of in-hospital deaths is higher among females (0.105) than among females (0.06). This suggests that gender might be associated with the risk of in-hospital death.\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  contingencyTable\nX-squared = 3.3789, df = 1, p-value = 0.06603\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe p-value of the chi-square test (0.066) is larger than 0.05, indicating that there is no significant association between gender and in-hospital death.\n\n\n\n\n\nCall:\nglm(formula = dstat_numeric ~ gender, family = binomial, data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.7515     0.2431 -11.318   &lt;2e-16 ***\ngenderfemale   0.6087     0.3351   1.816   0.0693 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 270.55  on 498  degrees of freedom\nAIC: 274.55\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe odds ratio for in-hospital death for females compared to males is equal to \\(\\exp(0.6087) = 1.83\\). This means that the odds of in-hospital death are 1.83 times higher in females than in males.\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe linear predictor models the risk of in-hospital death on the log-odds scale. To obtain the predicted probabilities, we need to transform the log-odds back to the probability scale using the logistic function.\nThe predicted probabilities of in-hospital death on the log-odds scale are:\n\nMales: -2.7515 (intercept)\nFemales: -2.7515 + 0.6087 (intercept + coefficient) = -2.1428\n\nThe predicted probabilities on the probability scale are:\n\nMale: \\(1 / (1 + exp(-(-2.7515))) = 0.060\\)\nFemale: \\(1 / (1 + exp(-(-2.1428))) = 0.105\\)\n\nThe predicted proportions of in-hospital deaths match the observed proportions from the contingency table.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value of the Wald test for the regression coefficient of the dummy variable genderfemale is 0.069, which is larger than 0.05. This indicates that there is no significant association between gender and in-hospital death. This conclusion is consistent with the results of the chi-square test.\n\n\n\n\n\n\n\n\nCall:\nglm(formula = dstat_numeric ~ gender + age, family = binomial, \n    data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -6.89600    1.19131  -5.789  7.1e-09 ***\ngenderfemale  0.26776    0.34618   0.773 0.439246    \nage           0.05770    0.01524   3.785 0.000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 253.45  on 497  degrees of freedom\nAIC: 259.45\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nAfter adjusting for age, the odds ratio for in-hospital death for females compared to males decreases from 1.83 to \\(\\exp(0.26776) = 1.31\\). This indicates that the effect of gender on in-hospital death is confounded by age.\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe increase in the risk of in-hospital death on the log-odds scale is equal to 0.0577 * 10 = 0.577. This means that for every 10-year increase in age, the odds of in-hospital death increase by a factor of \\(\\exp(0.577) = 1.78\\).\n\n\n\n\n\n\n\nAnalysis of Deviance Table\n\nModel 1: dstat_numeric ~ age\nModel 2: dstat_numeric ~ gender + age\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       498     254.05                     \n2       497     253.45  1  0.59934   0.4388\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe p-value of the likelihood ratio test (0.4388) is approximately equal to the p-value of the Wald test (0.4392). This indicates that the results of the two tests are consistent, which is expected because, for testing the significance of a single coefficient, the Wald test and the likelihood ratio test often yield similar results. However, they are not strictly equivalent, as the likelihood ratio test is based on comparing model fit, while the Wald test assesses the coefficient’s deviation from zero using its standard error.\n\n\n\n\n\n\n\n\n\nGroups for Hosmer-Lemeshow C statistic:\ncutfit\n[0.00568,0.0178]  (0.0178,0.0279]  (0.0279,0.0369]  (0.0369,0.0514] \n              55               52               47               52 \n  (0.0514,0.065]   (0.065,0.0864]   (0.0864,0.103]    (0.103,0.126] \n              44               53               55               43 \n   (0.126,0.154]    (0.154,0.348] \n              50               49 \nGroups for Hosmer-Lemeshow H statistic:\ncutfit1\n[0.00534,0.0399]  (0.0399,0.0742]   (0.0742,0.108]    (0.108,0.143] \n             164              110               87               61 \n   (0.143,0.177]    (0.177,0.211]    (0.211,0.245]     (0.245,0.28] \n              54               19                2                2 \n    (0.28,0.314]    (0.314,0.348] \n               0                1 \n\n\n$C\n\n    Hosmer-Lemeshow C statistic\n\ndata:  fitted(model.sex.age) and model.sex.age$y\nX-squared = 11.465, df = 8, p-value = 0.1767\n\n\n$H\n\n    Hosmer-Lemeshow H statistic\n\ndata:  fitted(model.sex.age) and model.sex.age$y\nX-squared = 6.2434, df = 8, p-value = 0.62\n\n\n\n\n\n\n\n\nScreenshot of the Hosmer-Lemeshow test results in SPSS\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nThe p-value for the Hosmer-Lemeshow test is 0.177 in R (C statistic) and 0.120 in SPSS, indicating that the model provides a satisfactory fit to the data: a non-significant p-value suggests that the model fits the data well, meaning that the observed and expected frequencies do not differ significantly.\nNote: the discrepancy between the p-values from R and SPSS seems to be due to small differences in how the groups are created within the implementation of the test."
  },
  {
    "objectID": "lab7_answers.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "href": "lab7_answers.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "title": "Medical Statistics – Answers lab 7",
    "section": "",
    "text": "alive dead\n  male     282   18\n  female   179   21\n\n\n        \n         alive  dead\n  male   0.940 0.060\n  female 0.895 0.105\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe proportion of in-hospital deaths is higher among females (0.105) than among females (0.06). This suggests that gender might be associated with the risk of in-hospital death.\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  contingencyTable\nX-squared = 3.3789, df = 1, p-value = 0.06603\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe p-value of the chi-square test (0.066) is larger than 0.05, indicating that there is no significant association between gender and in-hospital death.\n\n\n\n\n\nCall:\nglm(formula = dstat_numeric ~ gender, family = binomial, data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.7515     0.2431 -11.318   &lt;2e-16 ***\ngenderfemale   0.6087     0.3351   1.816   0.0693 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 270.55  on 498  degrees of freedom\nAIC: 274.55\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe odds ratio for in-hospital death for females compared to males is equal to \\(\\exp(0.6087) = 1.83\\). This means that the odds of in-hospital death are 1.83 times higher in females than in males.\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe linear predictor models the risk of in-hospital death on the log-odds scale. To obtain the predicted probabilities, we need to transform the log-odds back to the probability scale using the logistic function.\nThe predicted probabilities of in-hospital death on the log-odds scale are:\n\nMales: -2.7515 (intercept)\nFemales: -2.7515 + 0.6087 (intercept + coefficient) = -2.1428\n\nThe predicted probabilities on the probability scale are:\n\nMale: \\(1 / (1 + exp(-(-2.7515))) = 0.060\\)\nFemale: \\(1 / (1 + exp(-(-2.1428))) = 0.105\\)\n\nThe predicted proportions of in-hospital deaths match the observed proportions from the contingency table.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value of the Wald test for the regression coefficient of the dummy variable genderfemale is 0.069, which is larger than 0.05. This indicates that there is no significant association between gender and in-hospital death. This conclusion is consistent with the results of the chi-square test.\n\n\n\n\n\n\n\n\nCall:\nglm(formula = dstat_numeric ~ gender + age, family = binomial, \n    data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -6.89600    1.19131  -5.789  7.1e-09 ***\ngenderfemale  0.26776    0.34618   0.773 0.439246    \nage           0.05770    0.01524   3.785 0.000154 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 253.45  on 497  degrees of freedom\nAIC: 259.45\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nAfter adjusting for age, the odds ratio for in-hospital death for females compared to males decreases from 1.83 to \\(\\exp(0.26776) = 1.31\\). This indicates that the effect of gender on in-hospital death is confounded by age.\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe increase in the risk of in-hospital death on the log-odds scale is equal to 0.0577 * 10 = 0.577. This means that for every 10-year increase in age, the odds of in-hospital death increase by a factor of \\(\\exp(0.577) = 1.78\\).\n\n\n\n\n\n\n\nAnalysis of Deviance Table\n\nModel 1: dstat_numeric ~ age\nModel 2: dstat_numeric ~ gender + age\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       498     254.05                     \n2       497     253.45  1  0.59934   0.4388\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe p-value of the likelihood ratio test (0.4388) is approximately equal to the p-value of the Wald test (0.4392). This indicates that the results of the two tests are consistent, which is expected because, for testing the significance of a single coefficient, the Wald test and the likelihood ratio test often yield similar results. However, they are not strictly equivalent, as the likelihood ratio test is based on comparing model fit, while the Wald test assesses the coefficient’s deviation from zero using its standard error.\n\n\n\n\n\n\n\n\n\nGroups for Hosmer-Lemeshow C statistic:\ncutfit\n[0.00568,0.0178]  (0.0178,0.0279]  (0.0279,0.0369]  (0.0369,0.0514] \n              55               52               47               52 \n  (0.0514,0.065]   (0.065,0.0864]   (0.0864,0.103]    (0.103,0.126] \n              44               53               55               43 \n   (0.126,0.154]    (0.154,0.348] \n              50               49 \nGroups for Hosmer-Lemeshow H statistic:\ncutfit1\n[0.00534,0.0399]  (0.0399,0.0742]   (0.0742,0.108]    (0.108,0.143] \n             164              110               87               61 \n   (0.143,0.177]    (0.177,0.211]    (0.211,0.245]     (0.245,0.28] \n              54               19                2                2 \n    (0.28,0.314]    (0.314,0.348] \n               0                1 \n\n\n$C\n\n    Hosmer-Lemeshow C statistic\n\ndata:  fitted(model.sex.age) and model.sex.age$y\nX-squared = 11.465, df = 8, p-value = 0.1767\n\n\n$H\n\n    Hosmer-Lemeshow H statistic\n\ndata:  fitted(model.sex.age) and model.sex.age$y\nX-squared = 6.2434, df = 8, p-value = 0.62\n\n\n\n\n\n\n\n\nScreenshot of the Hosmer-Lemeshow test results in SPSS\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nThe p-value for the Hosmer-Lemeshow test is 0.177 in R (C statistic) and 0.120 in SPSS, indicating that the model provides a satisfactory fit to the data: a non-significant p-value suggests that the model fits the data well, meaning that the observed and expected frequencies do not differ significantly.\nNote: the discrepancy between the p-values from R and SPSS seems to be due to small differences in how the groups are created within the implementation of the test."
  },
  {
    "objectID": "lab7_answers.html#part-2-unguided-exercises",
    "href": "lab7_answers.html#part-2-unguided-exercises",
    "title": "Medical Statistics – Answers lab 7",
    "section": "Part 2: unguided exercises",
    "text": "Part 2: unguided exercises\n\nExercise 1\nMultiple logistic regression was used to construct a prognostic index to predict coronary artery disease from data on 348 patients with valvular heart disease who had undergone routine coronary arteriography before valve replacement (Ramsdale et al. 1982). The estimated equation was:\n\\[logit(p) = ln(p/(1-p)) = b_{0} + 1.167 \\times x{1} + 0.0106 \\times x_{2} + \\textrm{other terms}\\]\nwhere \\(x_{1}\\) stands for the family history of ischaemic disease (0=no, 1=yes) and \\(x_{2}\\) is the estimated total number of cigarettes ever smoked in terms of thousand cigarettes, calculated as the average number smoked annually times the number of years smoking.\n\nWhat is the estimated odds ratio for having coronary artery disease for subjects with a positive family history relative to subjects with a negative family history?\nWhat total number of cigarettes ever smoked carries the same risk as a positive family history? Convert the result into years of smoking 20 cigarettes per day.\nWhat is the odds ratio for coronary artery disease for someone with a positive family history who had smoked 20 cigarettes a day for 30 years compared to a non smoker with no family history?\n\nAnswers:\n\n\\(\\exp(1.167) = 3.212\\).\nThe log(OR) of positive history, \\(1.167\\), is to be set equal to \\(0.0106 \\times x_2\\), where \\(0.0106\\) is the log odds ratio of smoking 1000 cigarettes. Thus \\(x_2=1.167/0.0106=110.094\\) thousands of cigarettes. Dividing this result by \\((365 \\times 20) / 1000 = 7.3\\), i.e., the total number cigarettes (per thousand) smoked in 1 year if smoking 20 cigarettes per day, we find \\(110.094/7.3=15.1\\) or just above 15 years. Thus the odds ratio of positive history is equivalent to that of daily smoking of 20 cigarettes for about 15 years.\nThe total number of cigarettes smoked (per thousand) is \\((20 \\times 365 \\times 30) / 1000 = 219\\), so the odds ratio is \\(\\exp(1.167 + 219 \\times 0.0106) = 32.7\\)\n\n\n\nExercise 2\nData from 37 patients receiving a non-depleted allogenic bone marrow transplant were examined to see which variables were associated with the occurrence of acute graft-versus-host disease (GvHD: 0=no, 1=yes) (Bagot et al., 1988). Possible predictors are TYPE (type of leukemia: 1=AML, acute myeloid leukaemia; 2=ALL, acute lymphocytic leukaemia; 3=CML, chronic myeloid leukemia), PREG (donor pregnancy: 0= no, 1=yes), and LOGIND (the logarithm of an index of mixed epidermal cell-lymphocyte reactions). The data are in the file GvHD.sav available on Brightspace.\n\nPerform a likelihood ratio test to determine whether there is a significant association between the type of leukemia and the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nIn the adjusted model, What is the estimated odds ratio for the occurrence of GvHD for patients with ALL compared to those with ALM?\nUse the Hosmer-Lemeshow goodness-of-fit test to evaluate the fit of the model. Based on the results, does the model provide a satisfactory fit to the data?\n\nAnswers:\n\n# Create a 0/1 numeric version of the outcome variable\nGvHD$gvhd.numeric &lt;- ifelse(GvHD$gvhd == \"no\", 0, 1)\n\n# Fit the full  model\nmodel.GvHD &lt;- glm(gvhd.numeric ~ type + preg + logind, family = binomial, data = GvHD)\nsummary(model.GvHD)\n\n\nCall:\nglm(formula = gvhd.numeric ~ type + preg + logind, family = binomial, \n    data = GvHD)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -2.4496     1.2113  -2.022   0.0431 *\ntypeALL      -0.1480     1.1722  -0.126   0.8996  \ntypeCML       2.1781     1.2420   1.754   0.0795 .\npregyes       2.4982     1.1026   2.266   0.0235 *\nlogind        1.4577     0.7547   1.932   0.0534 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 51.049  on 36  degrees of freedom\nResidual deviance: 28.832  on 32  degrees of freedom\nAIC: 38.832\n\nNumber of Fisher Scoring iterations: 5\n\n# Fit the reduced model\nmodel.GvHD.reduced &lt;- glm(gvhd.numeric ~ preg + logind, family = binomial, data = GvHD)\n\n# Perform likelihood ratio test\nanova(model.GvHD.reduced, model.GvHD, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: gvhd.numeric ~ preg + logind\nModel 2: gvhd.numeric ~ type + preg + logind\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        34     33.654                       \n2        32     28.832  2    4.822  0.08972 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value of the likelihood ratio test is 0.090, indicating that the type of leukemia is not significantly associated with the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nBy default, R and SPSS both use the first level of the factor (ALM) as the reference category. Therefore, the table with estimated regression coefficients is the same in both outputs. The estimated odds ratio for the occurrence of GvHD for patients with AML compared to those with ALL is \\(\\exp(-0.148) = 0.862\\).\nThe Hosmer-Lemeshow test results are as follows:\n\n\nR\n\n\nGroups for Hosmer-Lemeshow C statistic:\ncutfit\n[0.0109,0.0254] (0.0254,0.0552]  (0.0552,0.142]   (0.142,0.237]   (0.237,0.529] \n              4               4               3               4               4 \n  (0.529,0.582]   (0.582,0.756]   (0.756,0.879]   (0.879,0.891]   (0.891,0.988] \n              3               4               3               4               4 \nGroups for Hosmer-Lemeshow H statistic:\ncutfit1\n[0.00994,0.109]   (0.109,0.206]   (0.206,0.304]   (0.304,0.402]     (0.402,0.5] \n             11               2               3               1               1 \n    (0.5,0.597]   (0.597,0.695]   (0.695,0.793]    (0.793,0.89]    (0.89,0.989] \n              5               0               4               6               4 \n\n\n$C\n\n    Hosmer-Lemeshow C statistic\n\ndata:  fitted(model.GvHD) and GvHD$gvhd.numeric\nX-squared = 12.352, df = 8, p-value = 0.1362\n\n\n$H\n\n    Hosmer-Lemeshow H statistic\n\ndata:  fitted(model.GvHD) and GvHD$gvhd.numeric\nX-squared = 14.907, df = 8, p-value = 0.06097\n\n\n\n\nSPSS\n\n\n\nScreenshot of the Hosmer-Lemeshow test results in SPSS\n\n\nWhile R and SPSS produce different p-values, neither is significant. Therefore, the model provides a satisfactory fit to the data."
  },
  {
    "objectID": "lab4_answers.html",
    "href": "lab4_answers.html",
    "title": "Medical Statistics – Answers lab 4",
    "section": "",
    "text": "ImportantQuestion 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\n\nStep 1: Extract the data\n\nSmokers with complications: \\(x_1 = 8\\), Total smokers: \\(n_1 = 20\\)\nNon-smokers with complications: \\(x_2 = 10\\), Total non-smokers: \\(n_2 = 60\\)\n\nStep 2: Calculate the sample proportions\n\n\\(p_1 = \\frac{x_1}{n_1} = \\frac{8}{20} = 0.4\\)\n\\(p_2 = \\frac{x_2}{n_2} = \\frac{10}{60} \\approx 0.167\\)\n\nStep 3: Compute the difference in proportions\n\n\\(\\text{Difference} = p_1 - p_2 = 0.4 - 0.167 \\approx 0.233\\)\n\nStep 4: Calculate the standard error (SE) of the difference\n\n\\(\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\)\n\\(\\text{SE} = \\sqrt{\\frac{0.4(1-0.4)}{20} + \\frac{0.167(1-0.167)}{60}} \\approx 0.120\\)\n\nStep 5: Determine the 95% confidence interval\n\n\\(Z_{critical} = 1.96\\)\n\\(\\text{Lower bound} = \\text{Difference} - Z_{critical} \\times \\text{SE} = 0.233 - 1.96 \\times 0.0.120 \\approx -0.001\\)\n\\(\\text{Upper bound} = \\text{Difference} + Z_{critical} \\times \\text{SE} = 0.233 + 1.96 \\times 0.120 \\approx 0.468\\)\n\nStep 6: Conclusion\n\nThe 95% confidence interval is approximately \\((-0.001, 0.468)\\)\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe 95% confidence interval for the difference in proportions is \\((-0.001, 0.468)\\). Since this interval includes zero, we cannot reject the null hypothesis that the two proportions are equal. Therefore, we do not have sufficient evidence to conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\nResults of the two-sample test of proportions based on the normal approximation with continuity correction:\n\n# Create a contingency table\ncomplications &lt;- matrix(c(8, 12, 10, 50), nrow = 2, byrow = TRUE)\ncolnames(complications) &lt;- c(\"Complication\", \"No Complication\")\nrownames(complications) &lt;- c(\"Smokers\", \"Non-smokers\")\ncomplications &lt;- as.table(complications)\n\nprop.test(complications)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  complications\nX-squared = 3.4409, df = 1, p-value = 0.0636\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.03449898  0.50116565\nsample estimates:\n   prop 1    prop 2 \n0.4000000 0.1666667 \n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe p-value from the two-sample test of proportions is 0.064. Since this p-value is greater than the significance level of 0.05, we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIn addition to the p-value, output of the prop.test() function also provides an approximate 95% confidence interval for the difference in proportions. How does this confidence interval compare to the one you calculated manually?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe 95% confidence interval for the difference in proportions was \\((-0.001, 0.468)\\). This is an approximate interval without continuity correction. The confidence interval provided in the output above is slightly wider due to the continuity correction. Without continuity correction (adjust = FALSE in R), the confidence interval provided by the prop.test() function is the same as the one that was calculated manually. SPSS gave the same result as R.\n\n\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe expected counts for each cell in the contingency t able are as follows:\n\nchisq_test_overall &lt;- chisq.test(complications)\nchisq_test_overall$expected\n\n            Complication No Complication\nSmokers              4.5            15.5\nNon-smokers         13.5            46.5\n\n\nOne of the cells in the contingency table has an expected count just below 5, which indicates that the two sample Z-test may not be fully accurate in this case.\n\n\n\n\n\n\nfisher.test(complications)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  complications\np-value = 0.05967\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.9154957 11.7051187\nsample estimates:\nodds ratio \n  3.274581 \n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nThe p-value from Fisher’s exact test is 0.060. Since this p-value is greater than the significance level of 0.05, we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\n# Create a contingency table\nside_effects &lt;- matrix(c(50, 30, 10, 40, 40, 20, 30, 50, 40), nrow = 3, byrow = TRUE)\ncolnames(side_effects) &lt;- c(\"None\", \"Mild\", \"Severe\")\nrownames(side_effects) &lt;- c(\"18–39\", \"40–59\", \"60+\")\nside_effects &lt;- as.table(side_effects)\nside_effects\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n60+     30   50     40\n\n# Perform the chi-square test of homogeneity\nchisq_test_overall &lt;- chisq.test(side_effects)\nprint(chisq_test_overall)\n\n\n    Pearson's Chi-squared test\n\ndata:  side_effects\nX-squared = 25.136, df = 4, p-value = 4.723e-05\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe p-value from the chi-square test of homogeneity is &lt;0.0001. Since this p-value is less than the significance level of 0.05, we have sufficient evidence to reject the null hypothesis. Therefore, we can conclude that the distribution of vaccine side effects is not consistent across the three age groups.\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\nExpected cell counts:\n\n# Retrieve the table of expected counts\nchisq_test_overall$expected\n\n          None     Mild   Severe\n18–39 34.83871 34.83871 20.32258\n40–59 38.70968 38.70968 22.58065\n60+   46.45161 46.45161 27.09677\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe expected counts in all cells are well above 5, which indicates that the normal approximation is appropriate in this case.\n\n\n\n\n\nComparison of age groups 18–39 and 40–59\n\ntable_12 &lt;- side_effects[c(\"18–39\", \"40–59\"), ]\ntable_12\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n\n# Perform chi-square test for the subset of data\nchisq_test_12 &lt;- chisq.test(table_12)\nprint(chisq_test_12)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_12\nX-squared = 5.3616, df = 2, p-value = 0.06851\n\n# Adjust the p-value for multiple testing\np_adjusted_12 &lt;- 3*chisq_test_12$p.value\n\nThe Bonferroni-corrected p-value for this comparison is 0.206.\n\n\n\n\n\n\nImportantExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\nComparison of age groups 18–39 and 60+\n\ntable_13 &lt;- side_effects[c(\"18–39\", \"60+\"), ]\ntable_13\n\n      None Mild Severe\n18–39   50   30     10\n60+     30   50     40\n\n# Perform chi-square test for the subset of data\nchisq_test_13 &lt;- chisq.test(table_13)\nprint(chisq_test_13)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_13\nX-squared = 24.208, df = 2, p-value = 5.536e-06\n\n# Adjust the p-value for multiple testing\np_adjusted_13 &lt;- 3*chisq_test_13$p.value\n\nThe Bonferroni-corrected p-value for this comparison is &lt;0.0001.\nComparison of age groups 40-59 and 60+\n\ntable_23 &lt;- side_effects[c(\"40–59\", \"60+\"), ]\ntable_23\n\n      None Mild Severe\n40–59   40   40     20\n60+     30   50     40\n\n# Perform chi-square test for the subset of data\nchisq_test_23 &lt;- chisq.test(table_23)\nprint(chisq_test_23)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_23\nX-squared = 7.4497, df = 2, p-value = 0.02412\n\n# Adjust the p-value for multiple testing\np_adjusted_23 &lt;- 3*chisq_test_23$p.value\n\nThe Bonferroni-corrected p-value for this comparison is 0.072.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nBased on the adjusted p-values from the pairwise comparisons, we can conclude that the distributions of side effects are significantly different between the age groups 18–39 and 60+ (Bonferroni-corrected p-value &lt; 0.001). However, there were no significant differences between the age groups 40–59 and 60+ (Bonferroni-corrected p-value = 0.072) or between the age groups 18–39 and 40–59 (Bonferroni-corrected p-value = 0.206)."
  },
  {
    "objectID": "lab4_answers.html#vaccine-side-effects-across-age-groups",
    "href": "lab4_answers.html#vaccine-side-effects-across-age-groups",
    "title": "Medical Statistics – Answers lab 4",
    "section": "",
    "text": "# Create a contingency table\nside_effects &lt;- matrix(c(50, 30, 10, 40, 40, 20, 30, 50, 40), nrow = 3, byrow = TRUE)\ncolnames(side_effects) &lt;- c(\"None\", \"Mild\", \"Severe\")\nrownames(side_effects) &lt;- c(\"18–39\", \"40–59\", \"60+\")\nside_effects &lt;- as.table(side_effects)\nside_effects\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n60+     30   50     40\n\n# Perform the chi-square test of homogeneity\nchisq_test_overall &lt;- chisq.test(side_effects)\nprint(chisq_test_overall)\n\n\n    Pearson's Chi-squared test\n\ndata:  side_effects\nX-squared = 25.136, df = 4, p-value = 4.723e-05\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe p-value from the chi-square test of homogeneity is &lt;0.0001. Since this p-value is less than the significance level of 0.05, we have sufficient evidence to reject the null hypothesis. Therefore, we can conclude that the distribution of vaccine side effects is not consistent across the three age groups.\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\nExpected cell counts:\n\n# Retrieve the table of expected counts\nchisq_test_overall$expected\n\n          None     Mild   Severe\n18–39 34.83871 34.83871 20.32258\n40–59 38.70968 38.70968 22.58065\n60+   46.45161 46.45161 27.09677\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe expected counts in all cells are well above 5, which indicates that the normal approximation is appropriate in this case.\n\n\n\n\n\nComparison of age groups 18–39 and 40–59\n\ntable_12 &lt;- side_effects[c(\"18–39\", \"40–59\"), ]\ntable_12\n\n      None Mild Severe\n18–39   50   30     10\n40–59   40   40     20\n\n# Perform chi-square test for the subset of data\nchisq_test_12 &lt;- chisq.test(table_12)\nprint(chisq_test_12)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_12\nX-squared = 5.3616, df = 2, p-value = 0.06851\n\n# Adjust the p-value for multiple testing\np_adjusted_12 &lt;- 3*chisq_test_12$p.value\n\nThe Bonferroni-corrected p-value for this comparison is 0.206.\n\n\n\n\n\n\nImportantExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\nComparison of age groups 18–39 and 60+\n\ntable_13 &lt;- side_effects[c(\"18–39\", \"60+\"), ]\ntable_13\n\n      None Mild Severe\n18–39   50   30     10\n60+     30   50     40\n\n# Perform chi-square test for the subset of data\nchisq_test_13 &lt;- chisq.test(table_13)\nprint(chisq_test_13)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_13\nX-squared = 24.208, df = 2, p-value = 5.536e-06\n\n# Adjust the p-value for multiple testing\np_adjusted_13 &lt;- 3*chisq_test_13$p.value\n\nThe Bonferroni-corrected p-value for this comparison is &lt;0.0001.\nComparison of age groups 40-59 and 60+\n\ntable_23 &lt;- side_effects[c(\"40–59\", \"60+\"), ]\ntable_23\n\n      None Mild Severe\n40–59   40   40     20\n60+     30   50     40\n\n# Perform chi-square test for the subset of data\nchisq_test_23 &lt;- chisq.test(table_23)\nprint(chisq_test_23)\n\n\n    Pearson's Chi-squared test\n\ndata:  table_23\nX-squared = 7.4497, df = 2, p-value = 0.02412\n\n# Adjust the p-value for multiple testing\np_adjusted_23 &lt;- 3*chisq_test_23$p.value\n\nThe Bonferroni-corrected p-value for this comparison is 0.072.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nBased on the adjusted p-values from the pairwise comparisons, we can conclude that the distributions of side effects are significantly different between the age groups 18–39 and 60+ (Bonferroni-corrected p-value &lt; 0.001). However, there were no significant differences between the age groups 40–59 and 60+ (Bonferroni-corrected p-value = 0.072) or between the age groups 18–39 and 40–59 (Bonferroni-corrected p-value = 0.206)."
  },
  {
    "objectID": "lab2_answers.html",
    "href": "lab2_answers.html",
    "title": "Medical Statistics – Answers lab 2",
    "section": "",
    "text": "We will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nThe mean, standard deviation, and standard error of the mean of the variable ‘birth weight in grams’ (bwt) are as follows:\n\nMean: 2944.7\nStandard deviation: 729\nStandard error (SE): 53.03\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe estimated mean birth weight for the population is 2944.7 grams.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nManually calculate the corresponding 95% confidence interval based on the normal approximation.\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nUsing the normal approximation, the 95% confidence interval for the population mean birth weight can be calculated as mean ± 1.96 * SE, where the standard error (SE) is equal to 53.03. Therefore, the 95% confidence interval is (2840.7, 3048.6).\n\n\nYou can also use SPSS/R to calculate this 95% confidence interval by using the t-distribution. This gives the following result: (2840, 3049.3)\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe 95% confidence interval that uses the t-distribution is sightly wider compared to the 95% confidence interval that was manually computed using the normal approximation. However, given that the sample size is relatively large (n = 189), the difference is minimal. Larger differences would be expected with smaller sample sizes."
  },
  {
    "objectID": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Medical Statistics – Answers lab 2",
    "section": "",
    "text": "We will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nThe mean, standard deviation, and standard error of the mean of the variable ‘birth weight in grams’ (bwt) are as follows:\n\nMean: 2944.7\nStandard deviation: 729\nStandard error (SE): 53.03\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe estimated mean birth weight for the population is 2944.7 grams.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nManually calculate the corresponding 95% confidence interval based on the normal approximation.\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nUsing the normal approximation, the 95% confidence interval for the population mean birth weight can be calculated as mean ± 1.96 * SE, where the standard error (SE) is equal to 53.03. Therefore, the 95% confidence interval is (2840.7, 3048.6).\n\n\nYou can also use SPSS/R to calculate this 95% confidence interval by using the t-distribution. This gives the following result: (2840, 3049.3)\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe 95% confidence interval that uses the t-distribution is sightly wider compared to the 95% confidence interval that was manually computed using the normal approximation. However, given that the sample size is relatively large (n = 189), the difference is minimal. Larger differences would be expected with smaller sample sizes."
  },
  {
    "objectID": "lab2_answers.html#one-sample-t-test",
    "href": "lab2_answers.html#one-sample-t-test",
    "title": "Medical Statistics – Answers lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\n\n\n\n    One Sample t-test\n\ndata:  lowbwt$bwt\nt = -1.0437, df = 188, p-value = 0.298\nalternative hypothesis: true mean is not equal to 3000\n95 percent confidence interval:\n 2840.049 3049.264\nsample estimates:\nmean of x \n 2944.656 \n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe degrees of freedom (df) for the one-sample t-test is equal to (sample size - 1), so df = 189 - 1 = 188 in this case.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe value of t-statistic is -1.04, and the corresponding p-value is 0.298. Since the p-value is greater than the significance level of 0.05, we fail to reject the null hypothesis. Therefore, the current sample does not provide sufficient evidence to conclude that the population mean significantly differs from 3000.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nBased on the histogram, the data appears to be reasonably normally distributed."
  },
  {
    "objectID": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "lab2_answers.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Medical Statistics – Answers lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable that takes a value 1 if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of 0 otherwise.\nThe frequency table for this variable is as follows:\n\n\n\n  0   1 \n130  59 \n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe estimated proportion of low birth weight babies in the population is 59 / (130 + 59) = 0.31.\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation.\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nUsing the sample proportion p, the standard error (SE) can be calculated as \\(\\sqrt(p * (1 - p) / n)\\), where n is the total number of observations. The 95% confidence interval can then be calculated as p ± 1.96 * SE. In this case, the SE is equal to 0.03. Therefore, the 95% confidence interval is (0.24, 0.38)."
  },
  {
    "objectID": "lab2_answers.html#binomial-test",
    "href": "lab2_answers.html#binomial-test",
    "title": "Medical Statistics – Answers lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%:\n\nR results\n\n\n\n    Exact binomial test\n\ndata:  sum(lowbwt$low == 1, na.rm = TRUE) and length(na.omit(lowbwt$low))\nnumber of successes = 59, number of trials = 189, p-value = 0.7509\nalternative hypothesis: true probability of success is not equal to 0.3\n95 percent confidence interval:\n 0.2468886 0.3834546\nsample estimates:\nprobability of success \n             0.3121693 \n\n\n\n\nSPSS results\n\n\n\nScreenshot of the SPSS output table\n\n\nBecause the binomial distribution with n=189 and p=0.3 is reasonably symmetric, the two-sided p-value provided by SPSS is close to the one provided by R. In the answers below, we will use the R results.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nThe observed number of successes is 59, and the total number of trials is 189. Assuming a null hypothesis proportion of 30%, the expected number of successes is 0.3 * 189 = 56.7. The observed number of successes is relatively close to the expected number of successes under the null hypothesis, resulting in a p-value of the binomial test of 0.75. Since this p-value is greater than the significance level of 0.05, we fail to reject the null hypothesis. Therefore, the current sample does not provide sufficient evidence to conclude that the proportion of low birth weight babies differs significantly from 30%.\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50% of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%. Use the exact binomial test for this question.\n\n\n\n\n\n\n\n\nTipAnswer question 10\n\n\n\nThe observed number of successes is 128, and the total number of trials is 200. Assuming a null hypothesis proportion of 50%, the expected number of successes is 0.5 * 200 = 100. The observed number of successes is considerably higher than the expected number of successes under the null hypothesis, resulting in a p-value of the binomial test that is &lt;0.001:\n\nbinom_test_result &lt;- binom.test(128, 200, p = 0.50)\nbinom_test_result\n\n\n    Exact binomial test\n\ndata:  128 and 200\nnumber of successes = 128, number of trials = 200, p-value = 9.13e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5692861 0.7064953\nsample estimates:\nprobability of success \n                  0.64 \n\n\nTherefore, we reject the null hypothesis and conclude that the proportion of Dutch adolescents who drink alcohol regularly is significantly higher than 50%.\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nTipAnswer question 11\n\n\n\nTo manually calculate an approximate p-value using the normal approximation, we first calculate the sample proportion \\(p\\) as the number of successes divided by the total number of trials (i.e., \\(p\\) = 128 / 200 = 0.64). We then calculate the standard error (SE) as \\(\\sqrt(\\pi * (1 - \\pi) / n)\\), where \\(n\\) is the total number of observations and \\(\\pi\\) is the null hypothesis proportion. This gives a SE of 0.035.\nUnder the null hypothesis, the sample proportion is approximately Normally distributed with mean \\(\\pi\\) = 0.5 and SE = 0.035. To calculate the probability of observing a sample proportion at least as extreme as the one observed in the data, we can calculate the z-score as \\((p - \\pi) / SE\\). In this case, the z-score is 3.96, resulting in a p-value &lt;0.001, which is consistent with the p-value obtained from the binomial test.\nThe use of the normal approximation is very reasonable in this case because \\(n * \\pi = 200 * 0.5 =100\\) is considerably larger than 5, which is the minimum requirement for the normal approximation to be valid."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R/SPSS labs for the Medical Statistics course",
    "section": "",
    "text": "This website hosts the R and SPSS labs for the Medical Statistics course. You can navigate between the different labs using the menu above."
  },
  {
    "objectID": "Assignment_2.html",
    "href": "Assignment_2.html",
    "title": "Medical Statistics",
    "section": "",
    "text": "In this assignment, you will continue exploring the Cleveland heart disease dataset. The assignment consists of three parts:\n\nPart A: ANCOVA (Analysis of Covariance): Investigate sex differences in resting blood pressure while adjusting for age.\nPart B: Logistic Regression: Use a DAG to choose confounders and compare unadjusted vs adjusted estimates for cholesterol and heart disease.\nPart C: Building a Prediction Model: Build a prediction model for maximum heart rate using backward elimination."
  },
  {
    "objectID": "Assignment_2.html#introduction",
    "href": "Assignment_2.html#introduction",
    "title": "Medical Statistics",
    "section": "",
    "text": "In this assignment, you will continue exploring the Cleveland heart disease dataset. The assignment consists of three parts:\n\nPart A: ANCOVA (Analysis of Covariance): Investigate sex differences in resting blood pressure while adjusting for age.\nPart B: Logistic Regression: Use a DAG to choose confounders and compare unadjusted vs adjusted estimates for cholesterol and heart disease.\nPart C: Building a Prediction Model: Build a prediction model for maximum heart rate using backward elimination."
  },
  {
    "objectID": "Assignment_2.html#dataset-description",
    "href": "Assignment_2.html#dataset-description",
    "title": "Medical Statistics",
    "section": "Dataset description",
    "text": "Dataset description\nThe Cleveland heart disease dataset includes various demographic, clinical, and diagnostic variables. Key variables for this assignment include:\n\nage: Age in years\nsex: Sex (1 = female; 2 = male)\ncp: Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\ntrestbps: Resting blood pressure (mm Hg)\nchol: Serum cholesterol (mg/dl)\nfbs: Fasting blood sugar &gt; 120 mg/dl (0 = no; 1 = yes)\nthalach: Maximum heart rate achieved\ntarget: Diagnosis of heart disease (1 = heart disease; 2 = no heart disease)\n\n\nIndividualized dataset\nR users must use the same 200-patient subset that they used for Assignment 1. If you need to recreate it, ensure you use the same random seed (the numeric part of your student or staff number). SPSS users can use the whole original dataset (N = 303) for this second assignment without further sampling.\n\n\n\n\n\n\nImportant\n\n\n\nReminder for R Users:\nlibrary(haven)\nlibrary(dplyr)\n\nheart_data &lt;- read_sav(\"datasets/heart_disease_cleveland.sav\")\nheart_data &lt;- heart_data |&gt; mutate(across(where(is.labelled), as_factor))\n\nset.seed(123456) # Replace with your student number\nheart_data &lt;- heart_data[sample(nrow(heart_data), 200), ]"
  },
  {
    "objectID": "Assignment_2.html#part-a-ancova-analysis-of-covariance",
    "href": "Assignment_2.html#part-a-ancova-analysis-of-covariance",
    "title": "Medical Statistics",
    "section": "Part A: ANCOVA (Analysis of Covariance)",
    "text": "Part A: ANCOVA (Analysis of Covariance)\nIn this part, you will investigate the relationship between demographic factors and resting blood pressure (trestbps).\n\nA1. Research Question\nResearch Question: Is there a difference in resting blood pressure between males and females, after accounting for age?\n\n\nA2. Fitting the ANCOVA Model\nFit an additive ANCOVA model with trestbps as the outcome, and age and sex as predictors (this assumes the effect of age is the same for males and females).\nReport the estimated difference in resting blood pressure between males and females, holding age constant, together with a 95% CI, and use the regression p-value to determine if sex is significantly associated with trestbps after adjusting for age.\n\n\nA3. Model Evaluation and Assumptions\nAssess whether the additive ANCOVA model is reasonable.\n\nAssumption checks (additive model): For the fitted additive ANCOVA model, use the Q-Q plot and Residuals vs. Fitted plot to assess normality, homoscedasticity, and linearity of the errors.\nInteraction check (new model): Fit an interaction model (age * sex) and use a Type III ANOVA table to test the age:sex term; state whether this provides evidence against the additive assumption."
  },
  {
    "objectID": "Assignment_2.html#part-b-logistic-regression",
    "href": "Assignment_2.html#part-b-logistic-regression",
    "title": "Medical Statistics",
    "section": "Part B: Logistic Regression",
    "text": "Part B: Logistic Regression\nIn this part, you will investigate whether cholesterol is associated with the presence of heart disease (target), after adjusting for confounders identified in a DAG.\n\n\n\n\n\n\nTipCoding reminder (R users)\n\n\n\nFor logistic regression in R (glm(..., family = binomial)), the outcome should be coded as 0/1. If target is a factor, create a numeric version, e.g.:\nheart_data &lt;- heart_data |&gt;\n  mutate(target01 = ifelse(target == \"Heart Disease\", 1, 0))\nThen use target01 as the outcome in your logistic regression models.\n\n\n\nB1. Unadjusted Model\nFit a simple logistic regression model with target as the outcome and chol as the sole predictor.\n\nReport the Odds Ratio (OR) for a one-unit increase in cholesterol (or per 10 mg/dl if you rescale).\nInterpret the OR and its 95% confidence interval.\n\n\n\nB2. Causal Modeling and Adjusted Model\nExamine the following hypothetical causal diagram (DAG) for the relationship between cholesterol and heart disease, then use it to determine your adjustment set and fit the adjusted model.\n\n\n\n\n\ngraph TD\n    Age[Age] --&gt; Chol[Cholesterol]\n    Age[Age] --&gt; Target[Heart Disease]\n    Sex[Sex] --&gt; Chol[Cholesterol]\n    Sex[Sex] --&gt; Target[Heart Disease]\n    Chol[Cholesterol] --&gt; Target[Heart Disease]\n    Target[Heart Disease] --&gt; CP[Chest Pain]\n\n\n\n\n\n\nUse the “recipe” from Week 8 to analyze this diagram:\n\nRemove all arrows leaving the exposure (chol). Are there any remaining paths between chol and target? (These are backdoor paths).\nIdentify Confounders: Which variable(s) must be adjusted for to “block” these backdoor paths?\nAdjusted Model: Fit a multiple logistic regression model adjusting only for the confounder(s) you identified to estimate the unconfounded effect of cholesterol on heart disease.\nComparison: Put the unadjusted OR (B1) and adjusted OR (aOR) side by side. Say whether the OR changes, whether the CI widens or narrows, and what that suggests about confounding."
  },
  {
    "objectID": "Assignment_2.html#part-c-building-a-prediction-model",
    "href": "Assignment_2.html#part-c-building-a-prediction-model",
    "title": "Medical Statistics",
    "section": "Part C: Building a Prediction Model",
    "text": "Part C: Building a Prediction Model\nIn this final section, you will develop a broader model to predict maximum heart rate (thalach), using age, sex, cp, chol, fbs, and trestbps as the candidate predictors.\nPerform a manual backward elimination procedure:\n\nIdentify the predictor with the highest p-value that is above the significance threshold of 0.10 (using the Type III ANOVA table).\nRemove that predictor and refit the model.\nRepeat until all remaining predictors have a p-value \\(&lt; 0.10\\).\n\nPresent a table showing the removal steps (which variable was removed and its p-value at that step), and report the final model (coefficients, 95% CIs, and p-values)."
  },
  {
    "objectID": "Assignment_2.html#report-instructions",
    "href": "Assignment_2.html#report-instructions",
    "title": "Medical Statistics",
    "section": "Report instructions",
    "text": "Report instructions\nStructure your report using the same headings as the assignment (Part A, Part B, Part C). Under each part, write a short results-focused summary and report the requested estimates (with 95% CIs and p-values) and any required figures/tables. Use the Reporting Examples below as a template for how to write up numerical results in sentences.\n\nPart A (ANCOVA): Report the adjusted male–female difference in trestbps (95% CI, p-value), and briefly state what the diagnostic plots and interaction test suggest.\nPart B (Logistic regression + DAG): State the adjustment set, report the unadjusted OR and adjusted OR for chol (95% CIs), and briefly comment on what the change suggests about confounding.\nPart C (Prediction): Include the backward-elimination table, report the final model (coefficients, 95% CIs, p-values), and briefly summarize which predictors remain."
  },
  {
    "objectID": "Assignment_2.html#reporting-examples",
    "href": "Assignment_2.html#reporting-examples",
    "title": "Medical Statistics",
    "section": "Reporting Examples",
    "text": "Reporting Examples\n\n\n\n\n\n\nNoteReporting an ANCOVA Result\n\n\n\n“In an ANCOVA of exam score, we adjusted for study hours and compared instruction format. After accounting for study hours, online students scored 3.6 points lower than in‑person students (95% CI: 1.1, 6.1), \\(p = 0.005\\).”\n\n\n\n\n\n\n\n\nNoteReporting a Logistic Regression Result\n\n\n\n“In the unadjusted model, students in the evening program had higher odds of course withdrawal than daytime students (OR = 1.85, 95% CI: 1.10, 3.10, \\(p = 0.020\\)). After adjusting for baseline GPA (identified as a confounder in the DAG), the association attenuated but remained (aOR = 1.50, 95% CI: 1.02, 2.40, \\(p = 0.041\\)).”"
  },
  {
    "objectID": "Assignment_2.html#submission-instructions",
    "href": "Assignment_2.html#submission-instructions",
    "title": "Medical Statistics",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe report: Submit your report as a Word or PDF document.\nThe analysis file: Submit your R script (.R) or SPSS output/syntax files."
  },
  {
    "objectID": "Assignment_2.html#downloads",
    "href": "Assignment_2.html#downloads",
    "title": "Medical Statistics",
    "section": "Downloads",
    "text": "Downloads\nCleveland heart disease dataset (SPSS format)"
  },
  {
    "objectID": "lab1_answers.html",
    "href": "lab1_answers.html",
    "title": "Medical Statistics – Answers lab 1",
    "section": "",
    "text": "Part 1 - Descriptive statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nFrom the histogram, it follows that the distribution of age is right-skewed. In such a situation, we would generally prefer reporting median [IQR] or median [Q1, Q3] over mean (SD).\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, and IQR for the variable lwt. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report.\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nWe start by making a histogram of the variable lwt:\n\n\n\n\n\n\n\n\n\nFrom the histogram, it follows that the distribution of lwt is right-skewed. We therefore prefer reporting median [Q1, Q3]. The median [Q1, Q3] is equal to 121 [110, 140].\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable history of hypertension (ht)\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\n\n\n\nht\nfrequency\npercentage\n\n\n\n\n0\n177\n93.7%\n\n\n1\n12\n6.3%\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension.\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 - Probability calculations for random variables\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nIf we let the random variable X denote the number of blood donors of blood group B, it follows from the text in the exercise that X ~ Bin(0.08, 100). Fewer than 1,500 ml of group B blood will be obtained if X is equal to or smaller than 2. We can therefore answer this question by calculating \\(P(X \\leq 2)\\) using the cumulative distribution function of the binomial distribution, which gives a probability of 0.01 or 1%.\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution).\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nIf we let the random variable X denote the number of individuals in treatment group A, we get a difference between the number of patients in the two treatment groups of over 20 if \\(X &lt; 40\\) or if \\(X &gt; 60\\). Using the cumulative distribution function of the binomial distribution, these probabilities are equal to \\(P(X &lt; 40) = 0.0176\\) and \\(P(X &gt; 60) = 1 - P(X \\leq 60) = 0.0176\\). The probability that the difference between the number of patients in the two treatment groups exceeds 20 is therefore equal to 2*0.0176 = 0.035 or 3.5%\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nAt the start, the minimum height was \\((172 - 175.8) / 5.84 = -0.65\\) standard deviations from the mean. From the cumulative distribution function of the standard normal distribution, the proportion of men smaller than this would be about \\(26\\%\\). At the end of the \\(25\\)-year period, the minimum height was \\((172 - 179.1) / 5.84 = -1.22\\) standard deviations from the mean. From the cumulative distribution function of the standard normal distribution, the proportion of men below the minimum is now about \\(11\\%\\). Thus, the proportion of ineligible men has more than halved.\n\n\n\n\nPart 3 - Some conceptual questions\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nImagine we have some observations on blood pressure and calculate the mean, standard deviation, median and IQR. How do these measures change if all observations are\n\nincreased by 10\ndecreased by their mean\nmultiplied by 10\ndivided by their standard deviation\n\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\n\nStandard deviation and IQR remain the same, mean and median increase by 10.\nStandard deviation and IQR remain the same, mean and median decrease by the mean (the mean becomes zero).\nAll are multiplied by 10.\nAll are divided by standard deviation; the standard deviation becomes one.\n\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nTwo fair dice are rolled, the results are X1 and X2.\n\nWhat is the probability Prob(X1=X2)?\nWhat is the expected value E(X1), and the standard deviation SD(X1)?\nGive the expected value and the variance of X1+X2 and of X1-X2.\n\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nQuestion (a)\nAssume the dice have different colors, red (R) and blue (B). \\(P(R = 1) = \\frac{1}{6}\\), \\(P(R = 2) = \\frac{1}{6}\\), etc. The outcomes of Red and Blue are independent of each other. So for example: \\(P(R = 1 \\text{ and } B = 4) = P(R = 1) \\times P(B = 4) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}\\).\nThis is true for all possible combinations of outcomes. There are only \\(6\\) possible outcomes with equal results: \\(R = 1\\) and \\(B = 1\\), \\(R = 2\\) and \\(B = 2\\), and so on. In total: \\(\\text{Prob} = 6 \\cdot (\\frac{1}{6} \\cdot \\frac{1}{6}) = 6 \\times \\frac{1}{36} = \\frac{1}{6}\\).\nQuestion (b)\nUsing the formula on page 3.3 of the syllabus:\n\\(E(X_1) = \\frac{1}{6} \\cdot (1 + 2 + 3 + 4 + 5 + 6) = 3.5\\)\n\\(\\text{Var}(X_1) = \\frac{1}{6} \\cdot [(1 - 3.5)^2 + (2 - 3.5)^2 + \\ldots] = 2.92\\)\n\\(\\text{SD} = \\sqrt{2.92} = 1.71\\)\nQuestion (c) Using the calculation rules on page 3.5 of the syllabus:\n\\(E(X_1 + X_2) = E(X_1) + E(X_2) = 3.5 + 3.5 = 7\\)\n\\(\\text{Var}(X_1 + X_2) = 2.92 + 2.92 = 5.84\\)\n\\(E(X_1 - X_2) = 0\\)\n\\(\\text{Var}(X_1 - X_2) = 2.92 + 2.92 = 5.84\\)\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nThere are two hospitals in town. On average 45 deliveries take place each day in the larger hospital, and 15 in the smaller. The probability of a baby being a boy is about 0.52, and the probability of twins is about 0.012. On any day, which hospital is more likely\n\nto have a set of twins delivered\nto have more than 60 % of babies being boys?\n\n\n\n\n\n\n\n\n\nTipAnswer question 10\n\n\n\n\nThe larger hospital, simply because there are more births. (This question is about an absolute number: a set of twins).\nThe day-to-day variation in the proportion of boys will be greater in the smaller hospital, so it is more likely to have more than 60% of babies being boys on any day. (This question is about a relative number: 60% of the babies).\n\nTo make the differences more clear: consider a very small hospital (only one delivery per day) and a very large hospital (1000 deliveries a day).\n\nThe probability that the delivery in the small hospital will concern twins equals 0.012. The same is true for each delivery in the larger hospital, but since we have 1000 deliveries, we expect 12 of them to concern twins.\nIn the small hospital, the probability that the child is a boy is 0.52. If a boy is born, then 100% of the babies that day are boys. This will happen in 52% of the days. For the larger hospital, we expect to see 520 boys and 480 girls on an arbitrary day. Some days there will be more than 520 boys, and other days there will be fewer. However, the probability that more than 60% of the children (more than 600 out of 1000) will be boys is very small—for sure less than 0.52.\n\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nThe probability of a baby being a boy is 0.52. For six women delivering consecutively in the same labour ward on one day, which of the following exact sequences of boys and girls is most likely and which least likely?\nGBGBGB\nBBBGGG\nGBBBBB\n\n\n\n\n\n\n\n\nTipAnswer question 11\n\n\n\nAs the probability of a boy is slightly greater than the probability of a girl, the sequence with the most boys is most likely, which is the last sequence. The other two sequences are equally likely."
  },
  {
    "objectID": "lab3_answers.html",
    "href": "lab3_answers.html",
    "title": "Medical Statistics – Answers lab 3",
    "section": "",
    "text": "ImportantQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nBased on the boxplot, it appears that mothers who smoked have lower birth weights on average compared to those who did not smoke. This suggests that smoking status may indeed have an effect on birth weight.\n\n\n\n\n\n\n\n\n    Two Sample t-test\n\ndata:  bwt by smoke\nt = 2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n  70.69274 492.73382\nsample estimates:\n mean in group no mean in group yes \n         3054.957          2773.243 \n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe p-value of the independent samples t-test (p=0.009) is less than 0.05, which indicates that there is a statistically significant difference in the mean birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nBased on the histograms, the distributions in both groups appear reasonably normal for the purposes of conducting an independent samples t-test.\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.3901 0.2399\n      187               \n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on Levene’s test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe p-value of Levene’s test (p = 0.2399) is greater than 0.05, indicating insufficient evidence to reject the null hypothesis of equal variances. Therefore, we can assume that the assumption of equal variances holds.\n\n\n\n\n\n\n\n# A tibble: 2 × 4\n  smoke  mean    sd     n\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 no    3055.  752.   115\n2 yes   2773.  660.    74\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nPooled Standard Deviation\nThe pooled standard deviation is calculated using the formula:\n\\[\ns_p = \\sqrt{\\frac{(n_{no} - 1) \\cdot s_{no}^2 + (n_{yes} - 1) \\cdot s_{yes}^2}{n_{no} + n_{yes} - 2}}\n\\] Substituting the values:\n\\[\ns_p = \\sqrt{\\frac{(115 - 1) \\cdot 752^2 + (74 - 1) \\cdot 660^2}{115 + 74 - 2}} = 717.49\n\\]\nStandard Error of the Mean Difference\nThe standard error (SE) of the mean difference is calculated as:\n\\[\nSE = s_p \\cdot \\sqrt{\\frac{1}{n_{no}} + \\frac{1}{n_{yes}}}\n\\] Substituting the values:\n\\[\nSE = 717.49 \\cdot \\sqrt{\\frac{1}{115} + \\frac{1}{74}} = 106.93\n\\]\nMean Difference\nThe difference in means between the groups is:\n\\[\n\\text{Mean Difference} = \\bar{X}_{no} - \\bar{X}_{yes} = 3055 - 2773 = 282\n\\]\n95% Confidence Interval\nTo construct the exact 95% CI, we need the 97.5th percentile from the t-distribution with \\(115 + 74 - 2 = 187\\) degrees of freedom. Since the table in Appendix B does not go beyond 150 df, we use 1.96 (the value from the standard normal distribution) as an approximation. The confidence interval is then calculated as:\n\\[\n\\text{CI} = \\text{Mean Difference} \\pm 1.96 \\cdot SE\n\\] Substituting the values:\n\\[\n\\text{CI} = 282 \\pm 1.96 \\cdot 106.93 = (72.43, 491.57)\n\\]\n\n\n\n\n\nPooled Standard Deviation: 717.49\n\nStandard Error of the Mean Difference: 106.93\n\n95% Confidence Interval for the Mean Difference: (72.43, 491.57)\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe manually calculated 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not is (72.43, 491.57), is approximately equal to the one provided in the output of the t.test() function, wich is (70.69, 492.73). The latter is marginally wider due to the use of the 97.5 the percentile of the t-distribution with 187 degrees of freedom (1.97) rather than the standard normal distribution (1.96). Additionally, small differences arise from rounding in the manual calculation.\n\n\n\n\n\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by smoke\nW = 5243.5, p-value = 0.007109\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nThe null hypothesis for the Mann-Whitney U test is that there is no difference in the median birth weight between mothers who smoked and those who did not. The alternative hypothesis is that there is a difference in median birth weight between the two groups. The p-value of the test is 0.0071, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that there is a statistically significant difference in median birth weight between mothers who smoked and those who did not."
  },
  {
    "objectID": "lab3_answers.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "lab3_answers.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Medical Statistics – Answers lab 3",
    "section": "",
    "text": "ImportantQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nBased on the boxplot, it appears that mothers who smoked have lower birth weights on average compared to those who did not smoke. This suggests that smoking status may indeed have an effect on birth weight.\n\n\n\n\n\n\n\n\n    Two Sample t-test\n\ndata:  bwt by smoke\nt = 2.6336, df = 187, p-value = 0.009156\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n  70.69274 492.73382\nsample estimates:\n mean in group no mean in group yes \n         3054.957          2773.243 \n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the results of the independent samples t-test, is there a statistically significant difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe p-value of the independent samples t-test (p=0.009) is less than 0.05, which indicates that there is a statistically significant difference in the mean birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nBased on the histograms, the distributions in both groups appear reasonably normal for the purposes of conducting an independent samples t-test.\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.3901 0.2399\n      187               \n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on Levene’s test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe p-value of Levene’s test (p = 0.2399) is greater than 0.05, indicating insufficient evidence to reject the null hypothesis of equal variances. Therefore, we can assume that the assumption of equal variances holds.\n\n\n\n\n\n\n\n# A tibble: 2 × 4\n  smoke  mean    sd     n\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 no    3055.  752.   115\n2 yes   2773.  660.    74\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\nTipAnswer\n\n\n\n\n\n\nPooled Standard Deviation\nThe pooled standard deviation is calculated using the formula:\n\\[\ns_p = \\sqrt{\\frac{(n_{no} - 1) \\cdot s_{no}^2 + (n_{yes} - 1) \\cdot s_{yes}^2}{n_{no} + n_{yes} - 2}}\n\\] Substituting the values:\n\\[\ns_p = \\sqrt{\\frac{(115 - 1) \\cdot 752^2 + (74 - 1) \\cdot 660^2}{115 + 74 - 2}} = 717.49\n\\]\nStandard Error of the Mean Difference\nThe standard error (SE) of the mean difference is calculated as:\n\\[\nSE = s_p \\cdot \\sqrt{\\frac{1}{n_{no}} + \\frac{1}{n_{yes}}}\n\\] Substituting the values:\n\\[\nSE = 717.49 \\cdot \\sqrt{\\frac{1}{115} + \\frac{1}{74}} = 106.93\n\\]\nMean Difference\nThe difference in means between the groups is:\n\\[\n\\text{Mean Difference} = \\bar{X}_{no} - \\bar{X}_{yes} = 3055 - 2773 = 282\n\\]\n95% Confidence Interval\nTo construct the exact 95% CI, we need the 97.5th percentile from the t-distribution with \\(115 + 74 - 2 = 187\\) degrees of freedom. Since the table in Appendix B does not go beyond 150 df, we use 1.96 (the value from the standard normal distribution) as an approximation. The confidence interval is then calculated as:\n\\[\n\\text{CI} = \\text{Mean Difference} \\pm 1.96 \\cdot SE\n\\] Substituting the values:\n\\[\n\\text{CI} = 282 \\pm 1.96 \\cdot 106.93 = (72.43, 491.57)\n\\]\n\n\n\n\n\nPooled Standard Deviation: 717.49\n\nStandard Error of the Mean Difference: 106.93\n\n95% Confidence Interval for the Mean Difference: (72.43, 491.57)\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the t.test() function?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe manually calculated 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not is (72.43, 491.57), is approximately equal to the one provided in the output of the t.test() function, wich is (70.69, 492.73). The latter is marginally wider due to the use of the 97.5 the percentile of the t-distribution with 187 degrees of freedom (1.97) rather than the standard normal distribution (1.96). Additionally, small differences arise from rounding in the manual calculation.\n\n\n\n\n\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by smoke\nW = 5243.5, p-value = 0.007109\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nThe null hypothesis for the Mann-Whitney U test is that there is no difference in the median birth weight between mothers who smoked and those who did not. The alternative hypothesis is that there is a difference in median birth weight between the two groups. The p-value of the test is 0.0071, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that there is a statistically significant difference in median birth weight between mothers who smoked and those who did not."
  },
  {
    "objectID": "lab3_answers.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "lab3_answers.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Medical Statistics – Answers lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\n\nExplaratory data analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe boxplot suggests that there may be differences in birth weight across the three ehtnic groups. More specifically, The mean birth weight appears to be higher for the Causasian group compared to the Afro-American and Asian groups.\n\n\n\n\nOne-way ANOVA\n\n\n             Df   Sum Sq Mean Sq F value  Pr(&gt;F)   \nethnicity     2  5070608 2535304   4.972 0.00788 **\nResiduals   186 94846445  509927                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe p-value from the one-way ANOVA (p=0.0079) is less than 0.05, indicating that there is at least one ethic group with a significantly different birth weight.\n\n\n\n\nPost-hoc tests\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  lowbwt$bwt and lowbwt$ethnicity \n\n              Caucasian Afro-American\nAfro-American 0.048     -            \nAsian         0.027     1.000        \n\nP value adjustment method: bonferroni \n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nWhat conclusions can be drawn from the post-hoc comparisons?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nIt follows from the post-hoc comparisons with the Bonferroni-adjusted p-values that the mean birth weight of Caucasian infants is significantly different from that of Afro-American infants (p=0.048) and Asian infants (p=0.027). There is no significant difference in mean birth weight between Afro-American and Asian infants (p=1).\n\n\n\n\nKruskal-Wallis Test\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  bwt by ethnicity\nKruskal-Wallis chi-squared = 8.5909, df = 2, p-value = 0.01363\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?\n\n\n\n\n\n\n\n\nTipAnswer question 10\n\n\n\nThe p-value from the Kruskal-Wallis test (p=0.014) is consistent with the one-way ANOVA results, indicating that there is at least one ethnic group with a significantly different median birth weight.\n\n\n\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 8.5909, df = 2, p-value = 0.01\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |   Afro-Ame      Asian\n---------+----------------------\n   Asian |  -0.667476\n         |     1.0000\n         |\nCaucasia |  -2.380803  -2.337641\n         |     0.0518     0.0582\n\nalpha = 0.05\nReject Ho if p &lt;= alpha\n\n\n$chi2\n[1] 8.590907\n\n$Z\n[1] -0.6674762 -2.3808031 -2.3376414\n\n$altP\n[1] 0.50446798 0.01727494 0.01940586\n\n$altP.adjusted\n[1] 1.00000000 0.05182482 0.05821758\n\n$comparisons\n[1] \"Afro-American - Asian\"     \"Afro-American - Caucasian\"\n[3] \"Asian - Caucasian\"        \n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nWhat conclusions can be drawn from Dunn’s test? Are these consistent with the post-hoc comparisons from the one-way ANOVA?\n\n\n\n\n\n\n\n\nTipAnswer question 11\n\n\n\nUnlike the post-hoc comparisons from the one-way ANOVA, Dunn’s test with Bonferroni correction does not show statistically significant differences between any pair of groups at the 0.05 level. Although the adjusted p-values for Caucasian vs. Afro-American (p=0.052) and Caucasian vs. Asian (p=0.058) are close to 0.05, they do not reach statistical significance. This illustrates that non-parametric post-hoc tests can be more conservative than their parametric counterparts."
  },
  {
    "objectID": "lab3_answers.html#part-3-unguided-exercises",
    "href": "lab3_answers.html#part-3-unguided-exercises",
    "title": "Medical Statistics – Answers lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nExample solution\n\n\n\n\n\n\n\n\n\n\n    Two Sample t-test\n\ndata:  bwt by ht\nt = 2.0192, df = 187, p-value = 0.04489\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n  10.02413 861.09734\nsample estimates:\n mean in group no mean in group yes \n         2972.311          2536.750 \n\n\n\n\n\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  1.2851 0.2584\n      187               \n\n\nEvaluation of the results: Although the p-value from the independent samples t-test suggests that there is no statistically significant difference in birth weight between mothers with and without a history of hypertension, the group sizes are highly imbalanced (177 vs. 12). This imbalance reduces the reliability of the t-test results and may limit its power to detect true differences. Additionally, the small sample size in the hypertension group makes it harder to assess assumptions such as normality. To check the robustness of the findings, we also conduct the Mann-Whitney U test as a non-parametric alternative.\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bwt by ht\nW = 1350, p-value = 0.1169\nalternative hypothesis: true location shift is not equal to 0\n\n\nOverall conclusion: Both the independent samples t-test and the Mann-Whitney U test suggest that there is no statistically significant difference in birth weight between mothers with and without a history of hypertension. This consistency across methods supports the robustness of the findings, despite the imbalance in group sizes.\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav (available on Brightspace) contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?\n\n\n\n\n\nExample solution\n\nExploratory data analysis\n\n\n\n\n\n\n\n\n\nThe groups seem to be different with respect to their means, but also with respect to their variation (range has different length)\n\n\nPerform a one way ANOVA, and interpret the results. Are the conditions satisfied?\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  15516    7758   3.711 0.0436 *\nResiduals   19  39716    2090                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  2  3.6413 0.04585 *\n      19                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA reveals significant results P= 0.044; this means that the null hypothesis of equal means in the three groups is rejected. However, according to the test of homogeneity of variances, the assumption of equal variances is violated (P-value = 0.044).\n\n\nTry a log transformation on the data, and perform again a one-way ANOVA. Are now the assumptions satisfied?\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2 0.1784 0.08922   3.539 0.0494 *\nResiduals   19 0.4790 0.02521                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   1.889 0.1785\n      19               \n\n\nThe test of equal variances for the log-transformed data reveals that the assumption of equal variances can be made. Test results for the ANOVA: P-value = 0.049. This is on the boundary of significance. There is indication that at least one pair of groups have different means.\n\n\nWhich means do differ according to you? Why?\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$rcfl and df$group \n\n        Group 1 Group 2\nGroup 2 0.042   -      \nGroup 3 0.464   1.000  \n\nP value adjustment method: bonferroni \n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$log_rcfl and df$group \n\n        Group 1 Group 2\nGroup 2 0.047   -      \nGroup 3 0.597   1.000  \n\nP value adjustment method: bonferroni \n\n\nFrom the paired comparisons with Bonferroni-corrections it appears that group1 and group 2 differ significantly with respect to their means (but on the boundary, P-value = 0.047).\n\nTry a non-parametric approach on these data. What are now your conclusions?\n\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rcfl by group\nKruskal-Wallis chi-squared = 4.1852, df = 2, p-value = 0.1234\n\n\nThe Kruskal-Wallis test gives P-value = 0.123, indicating that there are no significant differences between the three groups.\n\n\nCaveat\nIn this example, the sample size is very limited, making it difficult — if not impossible — to verify whether the assumptions underlying the one-way ANOVA are met. The Kruskal-Wallis test is a good alternative in such cases."
  },
  {
    "objectID": "lab6_answers.html",
    "href": "lab6_answers.html",
    "title": "Medical Statistics – Answers lab 6",
    "section": "",
    "text": "ImportantQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe scatterplot suggests that there is a positive linear association between age and pocketdepth. As age increases, pocket depth tends to increase.\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  pockets$age and pockets$pocketdepth\nt = 3.9647, df = 98, p-value = 0.0001398\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1891831 0.5295346\nsample estimates:\n     cor \n0.371786 \n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe correlation coefficient is approximately 0.37, indicating a moderate positive linear relationship between age and pocketdepth. This aligns with the interpretation of the scatterplot.\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe p-value for the correlation coefficient test is approximately 0.00014, indicating that we have sufficient evidence to reject the null hypothesis of no correlation between age and pocketdepth.\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pocketdepth ~ age, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.49289 -0.44376 -0.07903  0.49597  2.13335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.85872    0.22862  16.878  &lt; 2e-16 ***\nage          0.02054    0.00518   3.965  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7535 on 98 degrees of freedom\nMultiple R-squared:  0.1382,    Adjusted R-squared:  0.1294 \nF-statistic: 15.72 on 1 and 98 DF,  p-value: 0.0001398\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe estimated coefficient for age (0.021) is statistically significant with a p-value of 0.00014, indicating that the relationship between age and pocketdepth is statistically significant at α = 0.05.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value for age in the regression output (0.00014) and the p-value for the correlation coefficient test (0.00014) are consistent with each other. They both indicate a statistically significant relationship between age and pocketdepth.\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nThe intercept (3.859) represents the estimated pocket depth when age is 0, which may not have a meaningful interpretation in this context. The coefficient for age (0.021) represents the estimated change in pocket depth for each additional year of age.\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe expected pocket depth for a person who is 40 years old can be calculated using the intercept and coefficient from the regression output: 3.859 + 0.021 * 40 = 4.699 mm\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe R-squared value of 0.138 indicates that approximately 14% of the variation in pocket depth is explained by age in this model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nThe two plots suggest that the residuals are approximately normally distributed, with the histogram showing a roughly symmetric shape and the Q-Q plot showing the residuals closely following the diagonal line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\n\n\n\n\n\n\nTipAnswer question 10\n\n\n\nThe residual-versus-fitted plot suggests that the variance of the residuals is relatively constant across the range of fitted values, indicating homoscedasticity.\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?\n\n\n\n\n\n\n\n\nTipAnswer question 11\n\n\n\nThere is no obvious pattern or curvature in the residual-versus-fitted plot, indicating that the linearity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "lab6_answers.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Medical Statistics – Answers lab 6",
    "section": "",
    "text": "ImportantQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\n\n\n\n\n\nTipAnswer question 1\n\n\n\nThe scatterplot suggests that there is a positive linear association between age and pocketdepth. As age increases, pocket depth tends to increase.\n\n\n\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  pockets$age and pockets$pocketdepth\nt = 3.9647, df = 98, p-value = 0.0001398\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1891831 0.5295346\nsample estimates:\n     cor \n0.371786 \n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\n\n\n\n\n\n\nTipAnswer question 2\n\n\n\nThe correlation coefficient is approximately 0.37, indicating a moderate positive linear relationship between age and pocketdepth. This aligns with the interpretation of the scatterplot.\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis?\n\n\n\n\n\n\n\n\nTipAnswer question 3\n\n\n\nThe p-value for the correlation coefficient test is approximately 0.00014, indicating that we have sufficient evidence to reject the null hypothesis of no correlation between age and pocketdepth.\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pocketdepth ~ age, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.49289 -0.44376 -0.07903  0.49597  2.13335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.85872    0.22862  16.878  &lt; 2e-16 ***\nage          0.02054    0.00518   3.965  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7535 on 98 degrees of freedom\nMultiple R-squared:  0.1382,    Adjusted R-squared:  0.1294 \nF-statistic: 15.72 on 1 and 98 DF,  p-value: 0.0001398\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05)?\n\n\n\n\n\n\n\n\nTipAnswer question 4\n\n\n\nThe estimated coefficient for age (0.021) is statistically significant with a p-value of 0.00014, indicating that the relationship between age and pocketdepth is statistically significant at α = 0.05.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent with each other?\n\n\n\n\n\n\n\n\nTipAnswer question 5\n\n\n\nThe p-value for age in the regression output (0.00014) and the p-value for the correlation coefficient test (0.00014) are consistent with each other. They both indicate a statistically significant relationship between age and pocketdepth.\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nTipAnswer question 6\n\n\n\nThe intercept (3.859) represents the estimated pocket depth when age is 0, which may not have a meaningful interpretation in this context. The coefficient for age (0.021) represents the estimated change in pocket depth for each additional year of age.\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nTipAnswer question 7\n\n\n\nThe expected pocket depth for a person who is 40 years old can be calculated using the intercept and coefficient from the regression output: 3.859 + 0.021 * 40 = 4.699 mm\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\n\n\n\n\nTipAnswer question 8\n\n\n\nThe R-squared value of 0.138 indicates that approximately 14% of the variation in pocket depth is explained by age in this model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDo the histogram and Q-Q plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\n\n\n\n\nTipAnswer question 9\n\n\n\nThe two plots suggest that the residuals are approximately normally distributed, with the histogram showing a roughly symmetric shape and the Q-Q plot showing the residuals closely following the diagonal line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nDoes the residual-versus-fitted plot suggest constant variance?\n\n\n\n\n\n\n\n\nTipAnswer question 10\n\n\n\nThe residual-versus-fitted plot suggests that the variance of the residuals is relatively constant across the range of fitted values, indicating homoscedasticity.\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nDoes the residual-versus-fitted plot suggest any violation of the linearity assumption?\n\n\n\n\n\n\n\n\nTipAnswer question 11\n\n\n\nThere is no obvious pattern or curvature in the residual-versus-fitted plot, indicating that the linearity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-2-ancova-analysis-of-covariance",
    "href": "lab6_answers.html#part-2-ancova-analysis-of-covariance",
    "title": "Medical Statistics – Answers lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\n\n\n\n\nTipAnswer question 12\n\n\n\nAs discussed previously, the scatterplot suggests a positive relationship between age and pocketdepth, with higher pocket depths observed for older individuals. It also suggests that there is a U-shaped relationship between alcohol consumption and pocketdepth, with individuals consuming “1-2 glasses/day” having lower pocket depths compared to those consuming “None” or “&gt;2 glasses/day”.\n\n\n\n\nFitting the ANCOVA Model\n\nR output\n\n\n\nCall:\nlm(formula = pocketdepth ~ age + alcohol, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35037 -0.45299 -0.06626  0.39746  1.76573 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             4.165461   0.270688  15.388  &lt; 2e-16 ***\nage                     0.014849   0.005248   2.829  0.00568 ** \nalcohol1–2 glasses/day -0.394551   0.172383  -2.289  0.02428 *  \nalcohol&gt;2 glasses/day   0.364565   0.188347   1.936  0.05586 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7069 on 96 degrees of freedom\nMultiple R-squared:  0.257, Adjusted R-squared:  0.2338 \nF-statistic: 11.07 on 3 and 96 DF,  p-value: 2.638e-06\n\n\n\n\nSPSS output\n\n\n\nScreenshot of the SPSS output tables\n\n\n\n\n\n\n\n\nImportantQuestion 13\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nTipAnswer question 13\n\n\n\nBy default, the reference level for the alcohol variable is set to \"None\" in R and to \"&gt;2 glasses/day\" in SPSS.\nLooking at the R output, the coefficient for \"&gt;2 glasses/day\" is 0.365, which means that individuals who consume \"&gt;2 glasses/day\" are expected to have a pocket depth that is 0.365 mm higher than those who consume \"None\", while controlling for age.\nLooking at the SPSS output, the coefficient for \"None\" is -0.365, which means that individuals who consume \"None\" are expected to have a pocket depth that is 0.365 mm lower than those who consume \"&gt;2 glasses/day\", while controlling for age.\nTherefore, the expected difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\" is 0.365 mm, independent of the reference level chosen.\n\n\n\n\n\n\n\n\nImportantQuestion 14\n\n\n\nBased on the ANCOVA model output, what is the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nTipAnswer question 14\n\n\n\nBy default, the reference level for the alcohol variable is set to \"None\" in R and to \"&gt;2 glasses/day\" in SPSS.\nLooking at the R output, the coefficient for \"&gt;2 glasses/day\" is 0.365 and the coefficient for \"1-2 glasses/day\" is -0.395. This means that individuals who consume \"&gt;2 glasses/day\" are expected to have a pocket depth that is 0.365 - -0.395 = 0.76 mm higher than those who consume \"1-2 glasses/day\", while controlling for age.\nLooking at the SPSS output, the coefficient for \"1-2 glasses/day\" is -0.759, which means that individuals who consume \"1-2 glasses/day\" are expected to have a pocket depth that is 0.759 mm lower than those who consume \"&gt;2 glasses/day\", while controlling for age.\nTherefore, the expected difference in pocket depth between individuals who consume \"1-2 glasses/day\" and those who consume \"&gt;2 glasses/day\" is 0.76 mm, independent of the reference level chosen.\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 118.337  1 236.8042 &lt; 2.2e-16 ***\nage           4.001  1   8.0055 0.0056789 ** \nalcohol       7.669  2   7.6729 0.0008104 ***\nResiduals    47.974 96                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantQuestion 15\n\n\n\nBased on the ANOVA table, is the alcohol variable significantly associated with pocketdepth after accounting for age?\n\n\n\n\n\n\n\n\nTipAnswer question 15\n\n\n\nThe p-value for the alcohol variable in the ANOVA table is less than 0.05, indicating that the alcohol variable is significantly associated with pocketdepth after accounting for age.\n\n\n\n\n\nModel Diagnostics\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model.\n\n\n\n\n\n\n\n\nTipAnswer question 15\n\n\n\n\nNormality of Residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histogram and normal Q-Q plot of residuals shows that the residuals are approximately normally distributed, indicating that the normality assumption is not violated.\n\n\nHomoscedasticity\n\n\n\n\n\n\n\n\n\nThe residuals vs. fitted plot shows that the residuals are randomly scattered around zero, indicating that the homoscedasticity assumption is not violated."
  },
  {
    "objectID": "lab6_answers.html#part-3-interactions-in-ancova",
    "href": "lab6_answers.html#part-3-interactions-in-ancova",
    "title": "Medical Statistics – Answers lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\n\nFitting the Interaction Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age * alcohol, data = pockets)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35020 -0.45321 -0.06748  0.39755  1.76684 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 4.161e+00  3.811e-01  10.921   &lt;2e-16 ***\nage                         1.494e-02  7.747e-03   1.928   0.0569 .  \nalcohol1–2 glasses/day     -3.862e-01  5.142e-01  -0.751   0.4544    \nalcohol&gt;2 glasses/day       3.690e-01  6.538e-01   0.564   0.5738    \nage:alcohol1–2 glasses/day -2.082e-04  1.214e-02  -0.017   0.9863    \nage:alcohol&gt;2 glasses/day  -9.585e-05  1.395e-02  -0.007   0.9945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7144 on 94 degrees of freedom\nMultiple R-squared:  0.257, Adjusted R-squared:  0.2175 \nF-statistic: 6.503 on 5 and 94 DF,  p-value: 3.102e-05\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n            Sum Sq Df  F value  Pr(&gt;F)    \n(Intercept) 60.865  1 119.2592 &lt; 2e-16 ***\nage          1.897  1   3.7169 0.05688 .  \nalcohol      0.784  2   0.7678 0.46691    \nage:alcohol  0.000  2   0.0001 0.99985    \nResiduals   47.974 94                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportantQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?\n\n\n\n\n\n\n\n\nTipAnswer question 16\n\n\n\nThe p-value for the interaction term age:alcohol in the ANOVA table is approximately equal to 1, indicating that there is no interaction between age and alcohol in predicting pocketdepth."
  },
  {
    "objectID": "lab6_answers.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "lab6_answers.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Medical Statistics – Answers lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions.\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nFitting the ANCOVA Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age + smoking, data = pockets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3995 -0.5308 -0.1453  0.5331  2.2462 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.89031    0.22660  17.168  &lt; 2e-16 ***\nage            0.01776    0.00534   3.326  0.00125 ** \nsmokingSmoker  0.32371    0.17709   1.828  0.07063 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7447 on 97 degrees of freedom\nMultiple R-squared:  0.1669,    Adjusted R-squared:  0.1497 \nF-statistic: 9.718 on 2 and 97 DF,  p-value: 0.0001423\n\n\nThe p-value for the smoking variable in the ANOVA table is 0.071, indicating that the smoking variable is not significantly associated with pocketdepth after accounting for age.\n\n\nModel Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots do not show any violations of the normality or homoscedasticity assumptions.\n\n\nFitting the Interaction Model\n\n\n\nCall:\nlm(formula = pocketdepth ~ age * smoking, data = pockets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4160 -0.5382 -0.1353  0.5514  2.2096 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.778052   0.264041  14.309  &lt; 2e-16 ***\nage                0.020622   0.006360   3.243  0.00163 ** \nsmokingSmoker      0.772509   0.567866   1.360  0.17690    \nage:smokingSmoker -0.009779   0.011755  -0.832  0.40751    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7459 on 96 degrees of freedom\nMultiple R-squared:  0.1729,    Adjusted R-squared:  0.147 \nF-statistic: 6.689 on 3 and 96 DF,  p-value: 0.0003787\n\n\nAnova Table (Type III tests)\n\nResponse: pocketdepth\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 113.894  1 204.7350 &lt; 2.2e-16 ***\nage           5.849  1  10.5145  0.001629 ** \nsmoking       1.029  1   1.8506  0.176898    \nage:smoking   0.385  1   0.6921  0.407507    \nResiduals    53.405 96                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for the interaction term age:smoking is 0.407, indicating that there is no significant interaction between age and smoking in predicting pocketdepth."
  },
  {
    "objectID": "lab8_answers.html",
    "href": "lab8_answers.html",
    "title": "Medical Statistics – Answers lab 8",
    "section": "",
    "text": "Create an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\n\n\n\nCall:\nlm(formula = los ~ age + gender + hr + sysbp + diasbp + bmi + \n    cvd + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.335 -2.653 -1.071  1.200 40.073 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2648144  2.1394896   1.526 0.127659    \nage           0.0031994  0.0168850   0.189 0.849792    \ngenderfemale  0.8575246  0.4489334   1.910 0.056698 .  \nhr            0.0190577  0.0090828   2.098 0.036396 *  \nsysbp        -0.0008358  0.0085656  -0.098 0.922304    \ndiasbp        0.0141799  0.0128884   1.100 0.271780    \nbmi          -0.0304532  0.0427613  -0.712 0.476700    \ncvdyes        0.3903925  0.4981468   0.784 0.433600    \nshoyes        3.5265170  1.0329265   3.414 0.000693 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.632 on 491 degrees of freedom\nMultiple R-squared:  0.04991,   Adjusted R-squared:  0.03443 \nF-statistic: 3.224 on 8 and 491 DF,  p-value: 0.001388\n\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: los\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)    50.0   1  2.3286 0.1276592    \nage             0.8   1  0.0359 0.8497924    \ngender         78.3   1  3.6486 0.0566978 .  \nhr             94.5   1  4.4025 0.0363962 *  \nsysbp           0.2   1  0.0095 0.9223042    \ndiasbp         26.0   1  1.2105 0.2717799    \nbmi            10.9   1  0.5072 0.4766997    \ncvd            13.2   1  0.6142 0.4336001    \nsho           250.1   1 11.6561 0.0006929 ***\nResiduals   10535.8 491                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that the predictor with the highest p-value is sysbp (\\(p = 0.92\\)). Systolic blood pressure is the least significant predictor and should be removed from the model.\n\n\n\nThe following variables are sequentially removed from the model (after initially removing sysbp):\n\nage: p-value = 0.86\ncvd: p-value = 0.41\nbmi: p-value = 0.44\ndiasbp: p-value = 0.22\n\n\n\n\n\nfinal_model &lt;- lm(los ~ gender + hr + sho, data = whas500)\nsummary(final_model)\n\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n\n# 95% CIs for regression coefficients\nconfint(final_model)\n\n                   2.5 %     97.5 %\n(Intercept)  2.226140295 5.36601012\ngenderfemale 0.075705855 1.74507053\nhr           0.003306032 0.03805437\nshoyes       1.567847029 5.53304829\n\n\nThe final model for hospital length of stay includes gender, hr, and sho. All predictors have \\(p &lt; 0.10\\).\n\n\n\nPredictor\nCoefficient\n95% CI\np-value\n\n\n\n\n(Intercept)\n3.80\n[2.23, 5.37]\n&lt; 0.001\n\n\nGender (Female)\n0.91\n[0.08, 1.75]\n0.033\n\n\nHeart rate\n0.02\n[0.003, 0.038]\n0.020\n\n\nCardiogenic shock\n3.55\n[1.57, 5.53]\n&lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe overall fit of the model appears reasonable, as the residuals are generally centered around zero with no major patterns suggesting severe violations of linearity. However, there are some outliers with very long lengths of stay (LOS) that are not adequately captured by the model. These outliers lead to a right skew in the residual distribution, as seen in the histogram, influencing model fit. While the current model seems to work reasonably well for most observations, further steps (e.g., transformations or robust regression techniques) could be considered to better account for these extreme cases."
  },
  {
    "objectID": "lab8_answers.html#part-1-building-prediction-models-using-backward-elimination",
    "href": "lab8_answers.html#part-1-building-prediction-models-using-backward-elimination",
    "title": "Medical Statistics – Answers lab 8",
    "section": "",
    "text": "Create an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\n\n\n\nCall:\nlm(formula = los ~ age + gender + hr + sysbp + diasbp + bmi + \n    cvd + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.335 -2.653 -1.071  1.200 40.073 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2648144  2.1394896   1.526 0.127659    \nage           0.0031994  0.0168850   0.189 0.849792    \ngenderfemale  0.8575246  0.4489334   1.910 0.056698 .  \nhr            0.0190577  0.0090828   2.098 0.036396 *  \nsysbp        -0.0008358  0.0085656  -0.098 0.922304    \ndiasbp        0.0141799  0.0128884   1.100 0.271780    \nbmi          -0.0304532  0.0427613  -0.712 0.476700    \ncvdyes        0.3903925  0.4981468   0.784 0.433600    \nshoyes        3.5265170  1.0329265   3.414 0.000693 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.632 on 491 degrees of freedom\nMultiple R-squared:  0.04991,   Adjusted R-squared:  0.03443 \nF-statistic: 3.224 on 8 and 491 DF,  p-value: 0.001388\n\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: los\n             Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)    50.0   1  2.3286 0.1276592    \nage             0.8   1  0.0359 0.8497924    \ngender         78.3   1  3.6486 0.0566978 .  \nhr             94.5   1  4.4025 0.0363962 *  \nsysbp           0.2   1  0.0095 0.9223042    \ndiasbp         26.0   1  1.2105 0.2717799    \nbmi            10.9   1  0.5072 0.4766997    \ncvd            13.2   1  0.6142 0.4336001    \nsho           250.1   1 11.6561 0.0006929 ***\nResiduals   10535.8 491                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that the predictor with the highest p-value is sysbp (\\(p = 0.92\\)). Systolic blood pressure is the least significant predictor and should be removed from the model.\n\n\n\nThe following variables are sequentially removed from the model (after initially removing sysbp):\n\nage: p-value = 0.86\ncvd: p-value = 0.41\nbmi: p-value = 0.44\ndiasbp: p-value = 0.22\n\n\n\n\n\nfinal_model &lt;- lm(los ~ gender + hr + sho, data = whas500)\nsummary(final_model)\n\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,   Adjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n\n# 95% CIs for regression coefficients\nconfint(final_model)\n\n                   2.5 %     97.5 %\n(Intercept)  2.226140295 5.36601012\ngenderfemale 0.075705855 1.74507053\nhr           0.003306032 0.03805437\nshoyes       1.567847029 5.53304829\n\n\nThe final model for hospital length of stay includes gender, hr, and sho. All predictors have \\(p &lt; 0.10\\).\n\n\n\nPredictor\nCoefficient\n95% CI\np-value\n\n\n\n\n(Intercept)\n3.80\n[2.23, 5.37]\n&lt; 0.001\n\n\nGender (Female)\n0.91\n[0.08, 1.75]\n0.033\n\n\nHeart rate\n0.02\n[0.003, 0.038]\n0.020\n\n\nCardiogenic shock\n3.55\n[1.57, 5.53]\n&lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe overall fit of the model appears reasonable, as the residuals are generally centered around zero with no major patterns suggesting severe violations of linearity. However, there are some outliers with very long lengths of stay (LOS) that are not adequately captured by the model. These outliers lead to a right skew in the residual distribution, as seen in the histogram, influencing model fit. While the current model seems to work reasonably well for most observations, further steps (e.g., transformations or robust regression techniques) could be considered to better account for these extreme cases."
  },
  {
    "objectID": "lab8_answers.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "href": "lab8_answers.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "title": "Medical Statistics – Answers lab 8",
    "section": "Part 2: Automated procedures for building prediction models (logistic regression)",
    "text": "Part 2: Automated procedures for building prediction models (logistic regression)\nIn this part, we explore automated procedures for predictor selection in logistic regression prediction models, focusing on predicting in-hospital death (dstat).\n\nR\n\n# Create a 0/1 outcome (1 = dead)\nwhas500 &lt;- whas500 |&gt;\n  mutate(dstat01 = ifelse(dstat == \"dead\", 1, 0))\n\nfit_full &lt;- glm(\n  dstat01 ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho,\n  family = binomial,\n  data = whas500\n)\n\nfit_step &lt;- stepAIC(fit_full, direction = \"backward\", trace = FALSE)\nsummary(fit_step)\n\n\nCall:\nglm(formula = dstat01 ~ age + hr + sysbp + sho, family = binomial, \n    data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.186164   1.687679  -3.665 0.000247 ***\nage          0.059272   0.016933   3.500 0.000464 ***\nhr           0.013961   0.007497   1.862 0.062572 .  \nsysbp       -0.017333   0.006212  -2.790 0.005271 ** \nshoyes       3.061710   0.525719   5.824 5.75e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 204.09  on 495  degrees of freedom\nAIC: 214.09\n\nNumber of Fisher Scoring iterations: 6\n\n# Odds ratios and 95% CIs\nexp(cbind(OR = coef(fit_step), confint.default(fit_step)))\n\n                      OR        2.5 %      97.5 %\n(Intercept)  0.002057705 7.530576e-05  0.05622609\nage          1.061063592 1.026428e+00  1.09686811\nhr           1.014058710 9.992675e-01  1.02906886\nsysbp        0.982816468 9.709221e-01  0.99485659\nshoyes      21.364054186 7.624145e+00 59.86543899\n\n\nQuestion: Which predictors are retained in the prediction model?\nAnswer:\nThe final logistic regression model obtained via backward elimination using AIC includes the following predictors: age, hr (heart rate), sysbp (systolic blood pressure), and sho (cardiogenic shock).\n\n\nSPSS\nIn SPSS, using the Backward: LR method (standard settings), the procedure also yields a final model with these four predictors.\n\n\n\nSPSS results of the backward selection for logistic regression"
  },
  {
    "objectID": "lab8_answers.html#part-3-causal-diagrams",
    "href": "lab8_answers.html#part-3-causal-diagrams",
    "title": "Medical Statistics – Answers lab 8",
    "section": "Part 3: Causal diagrams",
    "text": "Part 3: Causal diagrams\nFor each of the exercises below:\n\nTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\nCheck your answer using the DAGitty webtool\n\n\nExercise 1\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n\n\nDAG exercise 1\n\n\nAnswer:\nFollowing the recipe: after removing all arrows leaving E, there are several unblocked paths leading from E to O. Just like in the lecture, adjusting for v2 opens a backdoor path (E – v1 – v3 – O) This newly opened backdoor path needs to be closed by also conditioning on v1 or v3, or both. Hence, there are 3 options: (v1, v2, v3) ; (v1, v2) ; and finally, (v2, v3).\n\n\nExercise 2\nIn the graph depicted below, what happens when you additionally adjust for v5?\n\n\n\nDAG exercise 2\n\n\nAnswer: When adjusting for v5, we are blocking the effect through this indirect path from E to O (v5 is a mediator between E and O). Instead of the total effect of E on O, we will be estimating the direct effect.\nIn DAGitty, when you set v5 to ‘adjusted’, the algorithm will say the following: “The total effect cannot be estimated due to adjustment for an intermediate or a descendant of an intermediate.”\n\n\nExercise 3\nThis diagram is slightly different: v1 now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of v1 on O?\n\n\n\nDAG exercise 3\n\n\nAnswer: No adjustment is needed: there are no backdoor paths (removing all arrows leaving v1 reveals no remaining unblocked path from v1 to O).\n\n\nExercise 4\nNow, v2 is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of v2 on O?\n\n\n\nDAG exercise 4\n\n\nAnswer: Following the recipe, there are three unblocked paths left after removing the arrows leaving v2:\n\nv2 – v3 – O and\nv2 – v1 – E – O\nv2 - v1 – E -v5 - O\n\nBackdoor path a) can be closed by conditioning on v3.\nBackdoor path b) can be closed by conditioning on v1 (but not by conditioning on E, as you would no longer be estimating the total effect by blocking the paths from v2 to O mediated by E).\nIn this case, you should therefore condition on v1 and v3.\n\n\nExercise 5\nBack to the first DAG. However, v2 is now unmeasured. Can we still obtain an unconfounded estimate of the effect of E on O?\n\n\n\nDAG exercise 5\n\n\nAnswer: No, we cannot close the backdoor path between E and O since v2 is unmeasured and cannot be corrected for.\n\n\nExercise 6\nSee the DAG below: you adjusted for v5. What would be the consequence of this action?\n\n\n\nDAG exercise 6\n\n\nAnswer: There is no consequence: conditioning on v5 cannot alter any of the estimated effects in the DAG (it is neither a confounder, collider, nor a mediator in the E-O relationship)."
  },
  {
    "objectID": "R_Assignment_1.html",
    "href": "R_Assignment_1.html",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "",
    "text": "In this assignment, you will work with an individualized dataset derived from the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "R_Assignment_1.html#introduction",
    "href": "R_Assignment_1.html#introduction",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "",
    "text": "In this assignment, you will work with an individualized dataset derived from the Cleveland heart disease dataset. This dataset contains information on patients with suspected heart disease and includes various demographic, clinical, and diagnostic variables. Your task is to perform a series of analyses to explore the dataset and investigate the relationship between different variables and the presence of heart disease."
  },
  {
    "objectID": "R_Assignment_1.html#dataset-description",
    "href": "R_Assignment_1.html#dataset-description",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Dataset description",
    "text": "Dataset description\nThe Cleveland heart disease dataset originates from the Cleveland Clinic Foundation and focuses on heart disease diagnosis. It includes data from 303 patients on the following variables:\n\nage: Age in years\nsex: Sex (1 = female; 2 = male)\ncp: Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\ntrestbps: Resting blood pressure (mm Hg at hospital admission)\nchol: Serum cholesterol in mg/dl\nfbs: Fasting blood sugar &gt; 120 mg/dl\nrestecg: Resting electrocardiographic results (1 = normal; 2 = ST-T wave abnormality; 3 = left ventricular hypertrophy)\nthalach: Maximum heart rate achieved\nexang: Exercise-induced angina (1 = no; 2 = yes)\noldpeak: ST depression induced by exercise relative to rest\nslope: Slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\nca: Number of major vessels (0-3) colored by fluoroscopy\nthal: Thallium heart scan results (1 = normal; 2 = fixed defect; 3 = reversible defect)\ntarget: Diagnosis of heart disease (1 = heart disease; 2 = no heart disease)"
  },
  {
    "objectID": "R_Assignment_1.html#objectives",
    "href": "R_Assignment_1.html#objectives",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Objectives",
    "text": "Objectives\nIn this assignment, you will explore the Cleveland heart disease dataset to answer two personalized research questions. Your specific research questions are determined by the last two digits of your student (or staff) number (see the lookup table below).\nResearch Question 1 (ANOVA/Kruskal-Wallis): Does maximum heart rate differ across [Grouping variable] categories?\nResearch Question 2 (Chi-Square/Fisher’s Exact): Is the presence of heart disease associated with [Binary predictor]?"
  },
  {
    "objectID": "R_Assignment_1.html#steps-to-complete-the-assignment",
    "href": "R_Assignment_1.html#steps-to-complete-the-assignment",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Steps to complete the assignment",
    "text": "Steps to complete the assignment\n\nStep 1: Create an individualized dataset\nDownload the Cleveland heart disease dataset (see Downloads section at the end of this document) and follow the steps below to create an individualized dataset with 200 patients:\n\n\n\n\n\n\nImportant\n\n\n\nYou must use your individualized dataset for all analyses in this assignment. Each dataset is uniquely sampled based on your student or staff number, ensuring that every student works independently with a unique dataset. This approach also ensures that results remain reproducible and can be individually verified by the instructor.\n\n\n\nLoad the dataset into R\nSet a random seed using your student or staff number\n\nRemove any letters (e.g., S or P) and use the numeric part\nExample: set.seed(123456) for student number S123456\n\nRandomly sample 200 patients from the dataset\n\nUse: heart_data &lt;- heart_data[sample(nrow(heart_data), 200), ]\n\nVerify the dataset contains exactly 200 rows\n\nExample: nrow(heart_data) should return 200\n\n\n\n# Load required packages for importing and cleaning the data\n# You may load additional packages as needed for your analyses\nlibrary(haven)\nlibrary(dplyr)\n\n# Import the SPSS file\nheart_data &lt;- read_sav(\"datasets/heart_disease_cleveland.sav\")\n\n# Convert all categorical variables to factors\nheart_data &lt;- heart_data |&gt; mutate(across(where(is.labelled), as_factor))\n\n# Set the random seed\n# (replace 123456 with your student or staff number without S or P)\nset.seed(123456)\n\n# Create an individualized dataset with 200 patients\nheart_data &lt;- heart_data[sample(nrow(heart_data), 200), ]\n\n# Verify the dataset contains exactly 200 rows\nnrow(heart_data)\n\n\n\nStep 2: Identify your personalized research questions\nUse the last two digits of your student number to find your personalized research questions in the table below:\n\n\n\nLast 2 digits\nRQ1: Grouping variable\nRQ2: Binary predictor\n\n\n\n\n00–24\nChest pain type (cp)\nSex (sex)\n\n\n25–49\nChest pain type (cp)\nFasting blood sugar (fbs)\n\n\n50–74\nThalassemia (thal)\nSex (sex)\n\n\n75–99\nThalassemia (thal)\nFasting blood sugar (fbs)\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA student with number S2734567 has last two digits 67, which falls in the 50–74 range. Their research questions are:\n\nRQ1: Does maximum heart rate differ across thalassemia categories?\nRQ2: Is the presence of heart disease associated with sex?\n\n\n\n\n\nStep 3: Create a baseline characteristics table\n\nInclude all variables in the dataset apart from the outcome variable target\n\nSummarize demographic variables (e.g., age, sex)\nSummarize clinical variables (e.g., chol, trestbps, thalach, cp)\n\nDecide on suitable summary measures for each variable\n\nUse appropriate measures for continuous variables (e.g., mean, standard deviation, median, interquartile range)\nUse frequency counts and percentages for categorical variables\n\nPresent your table clearly\n\nUse meaningful labels, headings, and clear formatting\n\n\n\n\nStep 4: Perform the analysis for your first research question\nUsing your grouping variable from the lookup table:\n\nVisualize the data\n\nCreate a boxplot to compare the distribution of maximum heart rate across the categories of your grouping variable\n\nCalculate the estimated population means and 95% confidence intervals for maximum heart rate for each category of the grouping variable\nSelect and perform an appropriate test\n\nUse one-way ANOVA if normality and equal variances are met\nUse Kruskal-Wallis test if assumptions are violated\nPerform post-hoc comparisons using Bonferroni adjusted p-values if significant differences are found\n\n\n\n\nStep 5: Perform the analysis for your second research question\nUsing the binary predictor from the lookup table:\n\nSummarize the data\n\nCalculate and report the prevalence of heart disease for each category of your binary predictor\nInclude percentages and 95% confidence intervals for each group\n\nSelect and perform an appropriate test\n\nUse a Chi-Square test of homogeneity or Fisher’s Exact Test, depending on the expected cell counts\n\n\n\n\nStep 6: Write a report\nYour report should be structured in the form of Methods and Results sections, as typically encountered in scientific papers.\n\nMethods\n\nState your personalized research questions (based on your student number)\nOutline the steps taken to analyze the data\nDescribe statistical tests performed, assumptions checked, and adjustments applied\n\nResults\n\nInclude the baseline characteristics table\nPresent key findings for each research question\nInclude visualizations (e.g., boxplots) to support your findings where applicable\n\nFormatting guidelines\n\nProperly label all tables and figures\nLimit the report to 2–3 pages, including visuals and tables"
  },
  {
    "objectID": "R_Assignment_1.html#reporting-examples",
    "href": "R_Assignment_1.html#reporting-examples",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Reporting examples",
    "text": "Reporting examples\nWhen presenting your analysis results, ensure clarity and adherence to proper reporting conventions. Use the following examples as a guide:\n\nThe mean cholesterol levels (95% CI) for the four chest pain types were as follows: typical angina, 245.3 mg/dL (95% CI: 230.1, 260.5); atypical angina, 220.4 mg/dL (95% CI: 205.7, 235.1); non-anginal pain, 230.2 mg/dL (95% CI: 215.6, 244.8); and asymptomatic chest pain, 200.1 mg/dL (95% CI: 185.4, 214.8). A one-way ANOVA was conducted to compare cholesterol levels across these groups, revealing a significant difference, F(3, 299) = 4.32, p = 0.006. Post-hoc pairwise comparisons using Bonferroni-adjusted p-values indicated that patients with typical angina had significantly higher cholesterol levels compared to those with asymptomatic chest pain (adjusted p = 0.015). No other pairwise differences were statistically significant after adjustment.\n\n\nThe prevalence of heart disease was higher among patients older than 65 (68.5%, 95% CI: 60.2%, 76.8%) compared to those 65 or younger (47.2%, 95% CI: 35.6%, 58.8%). Fisher’s Exact Test indicated a significant difference between these groups (p = 0.028)."
  },
  {
    "objectID": "R_Assignment_1.html#submission-instructions",
    "href": "R_Assignment_1.html#submission-instructions",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Submission instructions",
    "text": "Submission instructions\nSubmit the following files as part of your assignment:\n\nThe report: Provide your report in Word or PDF format\nThe R script: Include your R script (.R file) with all analysis code"
  },
  {
    "objectID": "R_Assignment_1.html#downloads",
    "href": "R_Assignment_1.html#downloads",
    "title": "Assignment part 1: Cleveland heart disease dataset",
    "section": "Downloads",
    "text": "Downloads\nCleveland heart disease dataset (SPSS format)"
  },
  {
    "objectID": "R_lab2.html",
    "href": "R_lab2.html",
    "title": "Medical Statistics – Lab 2",
    "section": "",
    "text": "Welcome to lab 2 in the medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset.\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Load the dataset\nlowbwt &lt;- read_sav(\"lowbwt.sav\")\n\n# Convert alll labelled variables into factor variables \nlowbwt &lt;- lowbwt %&gt;% mutate(across(where(is.labelled), as_factor))\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "href": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-means",
    "title": "Medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Means",
    "text": "Point Estimates and 95% Confidence Intervals for Population Means\nWe will start by analyzing the variable ‘birth weight in grams’ (bwt), which is the main outcome of this study.\nTo calculate the mean, standard deviation, and standard error of the mean, you can use the following R code:\n\nbwt_summary &lt;- lowbwt |&gt;\n  summarise(\n    n = sum(!is.na(bwt)),\n    mean = mean(bwt, na.rm = TRUE),\n    sd = sd(bwt, na.rm = TRUE),\n    se = sd / sqrt(n)\n  )\n\n# Convert to data frame to display more decimal places\nas.data.frame(bwt_summary)\n\n\n\n\n\n\n\nNoteSequential Calculation in summarise()\n\n\n\nNotice that in the summarise() function above, the se calculation references both sd and n that were defined earlier in the same call. The summarise() function evaluates expressions sequentially from top to bottom, making newly created variables available for use in subsequent calculations within the same function call. This sequential evaluation also applies to other dplyr functions like mutate(), which we will use later in this lab.\n\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on these summary statistics, what is the estimated mean birth weight for the population?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nCalculate the corresponding 95% confidence interval based on the normal approximation.\n\n\nYou can also use R to calculate the 95% confidence interval by using the qt() function to determine the appropriate t-value. This approach uses the t-distribution, which provides a more accurate confidence interval when the population standard deviation is unknown and the sample size is small.\n\nbwt_ci &lt;- bwt_summary |&gt;\n  mutate(\n    t_value = qt(0.975, df = n - 1),\n    lower_ci = mean - t_value * se,\n    upper_ci = mean + t_value * se\n  )\n\nas.data.frame(bwt_ci)\n\nExplanation:\nThe qt() function in R is used to obtain the critical value from the t-distribution. In this case, we use qt(0.975, df = n - 1) to get the t-value for a 95% confidence interval, where 0.975 corresponds to the upper tail probability for a two-sided confidence level of 95%, and df = n - 1 represents the degrees of freedom (sample size minus one).\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nHow does the 95% confidence interval based on the t-distribution compare to the 95% confidence interval based on the normal approximation that you manually computed?"
  },
  {
    "objectID": "R_lab2.html#one-sample-t-test",
    "href": "R_lab2.html#one-sample-t-test",
    "title": "Medical Statistics – Lab 2",
    "section": "One-Sample t-Test",
    "text": "One-Sample t-Test\nTo determine whether the population mean birth weight differs significantly from a hypothesized value of 3000 grams, we use the t.test() function to conduct a one-sample t-test:\n\nt_test_result &lt;- t.test(lowbwt$bwt, mu = 3000)\nt_test_result\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nYou see that the test has 188 degrees of freedom. Why?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nBased on the results of the test, does the population mean significantly differ from 3000?\n\n\nOne of the assumptions underlying the one-sample t-test is that the data are normally distributed. We can check this assumption by creating a histogram:\n\nggplot(lowbwt, aes(x = bwt)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Birth Weights\", \n    x = \"Birth Weight (grams)\",\n    y = \"Frequency\")\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nLooking at the histogram, would you say that the data are normally distributed?"
  },
  {
    "objectID": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "href": "R_lab2.html#point-estimates-and-95-confidence-intervals-for-population-proportions",
    "title": "Medical Statistics – Lab 2",
    "section": "Point Estimates and 95% Confidence Intervals for Population Proportions",
    "text": "Point Estimates and 95% Confidence Intervals for Population Proportions\nNext, we will explore the variable ‘low birth weight’ (low), which is a dichotomous variable with the value ‘yes (bwt &lt; 2500)’ if the baby had a low birth weight (defined as a birth weight &lt; 2500g) and a value of ‘no (bwt &gt;= 2500 g)’ otherwise.\nWe start by using the table() function to calculate the frequency of each category of the ‘low’ variable:\n\nfreq_table &lt;- table(lowbwt$low)\nfreq_table\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on these frequencies, what is the estimated proportion of low birth weight babies in the population?\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nCalculate the corresponding 95% confidence interval based on the Normal approximation."
  },
  {
    "objectID": "R_lab2.html#binomial-test",
    "href": "R_lab2.html#binomial-test",
    "title": "Medical Statistics – Lab 2",
    "section": "Binomial Test",
    "text": "Binomial Test\nSubsequently, we perform an exact binomial test to assess whether the proportion of low birth weight babies in the population differs significantly from a hypothesized value of 30%. The binom.test() function requires two key pieces of information:\n\nx: the number of “successes” (in this case, babies with low birth weight)\nn: the total number of observations\n\nWe can extract these values from the frequency table we created earlier:\n\n# Extract the number of low birth weight babies (k)\nk &lt;- freq_table[\"yes (bwt &lt; 2500)\"]\n\n# Extract the total number of observations (n)\nn &lt;- sum(freq_table)\n\n# Perform the binomial test\nbinom_test_result &lt;- binom.test(x = k, n = n, p = 0.30)\nbinom_test_result\n\nExplanation:\n\nfreq_table[\"yes (bwt &lt; 2500)\"] extracts the count of babies with low birth weight from our frequency table\nsum(freq_table) calculates the total number of observations by summing all frequencies\nbinom.test(x = k, n = n, p = 0.30) performs the exact binomial test, where p = 0.30 specifies the hypothesized population proportion (30%)\n\nThe output displays, among other statistics, the two-sided p-value and an exact 95% confidence interval calculated using the Clopper and Pearson procedure.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDoes the proportion of low birth weight babies differ significantly from 30%?\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nThe Dutch government intends to start a campaign against drinking alcoholic beverages if over 50% of the adolescents drink alcoholic beverages regularly (at least once a week). A random sample of 200 adolescents is taken and 128 admit that they drink alcohol regularly (we assume all 200 speak the truth). Test the null hypothesis that 50% of the Dutch adolescents drink alcohol, using a significance level of 5%. Use the exact binomial test for this question.\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nRather than using an exact binomial test, we can also use the normal approximation of the binomial distribution to obtain an approximate p-value for the above hypothesis test. Manually calculate this approximate p-value and compare it to the p-value obtained from the binomial test. Is the use of the normal approximation appropriate in this case?\n\n\n\n\n\n\n\n\nNoteDifferences in Two-Sided P-Value Calculation Between SPSS and R\n\n\n\nWhen conducting statistical tests, it is important to understand that different software packages can calculate two-sided p-values in slightly different ways, which may lead to variations in results. A key difference exists between how SPSS and base R handle this calculation:\n\nSPSS often calculates two-sided p-values by doubling the one-sided p-value. Specifically, SPSS determines the probability of the observed outcome in one direction (greater or less than a given value) and then multiplies this value by 2. This approach assumes that the distribution of the test statistic is symmetric under the null hypothesis. While this method is straightforward, it can be misleading if the distribution is skewed or the sample size is small, as it may not fully account for the asymmetry in the data.\nBase R (e.g., the binom.test() function) uses a more exact method for calculating two-sided p-values. R’s approach sums the probabilities of observing outcomes that are as extreme as, or more extreme than, the observed value in both directions (both tails of the distribution). This method does not assume symmetry and provides a more accurate p-value, particularly for small samples or skewed distributions."
  },
  {
    "objectID": "R_lab4.html",
    "href": "R_lab4.html",
    "title": "Medical Statistics – Lab 4",
    "section": "",
    "text": "Welcome to lab 4 in the medical statistics course. In this lab, we will focus on the analysis of categorical data and the comparison of proportions between groups. We will also perform several statistical tests for the analysis of paired data.\nWe will use the following R packages: haven, ggplot2, and rcompanion. The first two packages were also used in the previous labs and therefore already installed on your computer. For rcompanion (used to perform pairwise chi-square tests) you may have to download and install it first by running install.packages(\"rcompanion\")."
  },
  {
    "objectID": "R_lab4.html#smoking-and-post-surgical-complications",
    "href": "R_lab4.html#smoking-and-post-surgical-complications",
    "title": "Medical Statistics – Lab 4",
    "section": "Smoking and post-surgical complications",
    "text": "Smoking and post-surgical complications\nA study was conducted to investigate whether smoking is associated with an increased risk of post-surgical complications. The relationship between smoking status (smoker or non-smoker) and the occurrence of complications following surgery was examined. The outcome of interest was whether or not a complication occurred (yes or no), with smoking status serving as the explanatory variable to compare complication rates between the two groups.\nThe data from the study are summarized in the following 2x2 contingency table:\n\n\n\n\nComplication\nNo Complication\nTotal\n\n\n\n\nSmokers\n8\n12\n20\n\n\nNon-smokers\n10\n50\n60\n\n\nTotal\n18\n62\n80\n\n\n\n\nConfidence intervals and hypothesis testing for the difference in proportions using the normal approximation\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nUsing the data provided in the table, calculate an approximate 95% confidence interval for the difference in proportions of post-surgical complications between smokers and non-smokers.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the 95% confidence interval, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\nWe can also use the normal approximation to test the hypothesis that the proportion of complications is the same for smokers and non-smokers. This test is known as the two-sample Z test for equality of proportions.\nAs explained in the syllabus, the two-sample Z test uses the pooled population proportion \\(\\hat{p}\\), which is calculated as the total number of events divided by the total sample size. This pooled proportion is used under the null hypothesis, which assumes that the two groups share the same underlying proportion. The standard error of the difference in proportions is then calculated as \\(\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}\\), where \\(n_1\\) and \\(n_2\\) are the sample sizes in the two groups.\nIn contrast, the 95% confidence interval for the difference in proportions does not rely on the pooled proportion. Instead, it calculates the standard error separately for each group using the observed proportions, resulting in an unpooled standard error: \\(\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\), where \\(p_1\\) and \\(p_2\\) are the sample proportions for each group. This approach provides an interval that better reflects the variability in the observed data, independent of the null hypothesis assumption.\nTo conduct the two-sample Z test, we start by creating a contingency table:\n\n# Create a contingency table\ncomplications &lt;- matrix(c(8, 12, 10, 50), nrow = 2, byrow = TRUE)\ncolnames(complications) &lt;- c(\"Complication\", \"No Complication\")\nrownames(complications) &lt;- c(\"Smokers\", \"Non-smokers\")\ncomplications &lt;- as.table(complications)\ncomplications\n\nNext, we supply this table to the prop.test() function to calculate the test statistic and p-value:\n\nprop.test(complications)\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIn addition to the p-value, output of the prop.test() function also provides an approximate 95% confidence interval for the difference in proportions. How does this confidence interval compare to the one you calculated manually?\n\n\n\n\nChecking of assumptions\nFor the use of the normal approximation to be valid, the expected number of events and non-events in each group should be at least 5.\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck this assumption by calculating the expected counts for each cell in the contingency table.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nIs it reasonable to use the normal approximation in this case?\n\n\n\n\nFisher’s exact test\nWhen the expected cell counts are small, the normal approximation may not be appropriate. In such cases, Fisher’s exact test is recommended for testing the association between two categorical variables.\nIn R, we can perform Fisher’s exact test using the fisher.test() function. The test is based on the hypergeometric distribution and provides an exact p-value for the association between the two variables:\n\nfisher.test(complications)\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nBased on the results of Fisher’s exact test, can we conclude that there is a statistically significant difference in the proportion of post-surgical complications between smokers and non-smokers?"
  },
  {
    "objectID": "R_lab4.html#vaccine-side-effects-across-age-groups",
    "href": "R_lab4.html#vaccine-side-effects-across-age-groups",
    "title": "Medical Statistics – Lab 4",
    "section": "Vaccine side effects across age groups",
    "text": "Vaccine side effects across age groups\nA study was conducted to investigate whether the occurrence of vaccine side effects differs across age groups. Researchers categorized side effects into three types: none, mild, and severe. The study participants were divided into three age groups: 18–39, 40–59, and 60+, and data was collected on the type of side effect experienced by individuals in each group.\nThe research objective was to determine whether the distribution of side effects is consistent across these age groups.\nThe data is summarized in the following contingency table:\n\n\n\nAge Group\nNone\nMild\nSevere\nTotal\n\n\n\n\n18–39\n50\n30\n10\n90\n\n\n40–59\n40\n40\n20\n100\n\n\n60+\n30\n50\n40\n120\n\n\nTotal\n120\n120\n70\n310\n\n\n\n\nChi-square test of homogeneity\nTo conduct a chi-square test of homogeneity, we start by creating a contingency table:\n\n# Create a contingency table\nside_effects &lt;- matrix(c(50, 30, 10, 40, 40, 20, 30, 50, 40), nrow = 3, byrow = TRUE)\ncolnames(side_effects) &lt;- c(\"None\", \"Mild\", \"Severe\")\nrownames(side_effects) &lt;- c(\"18–39\", \"40–59\", \"60+\")\nside_effects &lt;- as.table(side_effects)\nside_effects\n\nNext, we supply this table to the chisq.test() function to calculate the test statistic and p-value:\n\nchisq_test_overall &lt;- chisq.test(side_effects)\nprint(chisq_test_overall)\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the results of the chi-square test, can we conclude that the distribution of vaccine side effects is consistent across the three age groups?\n\n\n\nChecking of assumptions\nTo use the chi-square test, the expected cell counts should be at least 5 for most cells. To check this assumption, we retrieve the table of expected counts from the output of the chisq.test() function, which we stored in the chisq_test_overall object:\n\n# Retrieve the table of expected counts\nchisq_test_overall$expected\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nAre the expected cell counts greater than 5 for the different cells in the contingency table?\n\n\n\n\nPost-hoc pairwise comparisons\nFinally, we are interested in determining which age groups have significantly different distributions of side effects. For a \\(n \\times 2\\) table, this can be achieved easily using the pairwise.prop.test() function, which performs pairwise comparisons of proportions between groups with adjustments for multiple testing. However, in our case, we are working with a \\(3 \\times 3\\) contingency table, where the outcomes have more than two categories (None, Mild, Severe).\nFor contingency tables with more than two outcome categories, we can use the pairwiseNominalIndependence() function from the rcompanion package. This function performs pairwise chi-square tests between all pairs of rows (or columns) and applies a correction for multiple testing:\n\nlibrary(rcompanion)\npairwiseNominalIndependence(side_effects,\n                            compare = \"row\",\n                            fisher = FALSE,\n                            method = \"bonferroni\")\n\nThe output shows the results of pairwise comparisons between all age groups. The key columns to focus on are:\n\nComparison: The pair of age groups being compared\np.Chisq: The unadjusted p-value from the chi-square test\np.adj.Chisq: The Bonferroni-adjusted p-value for multiple comparisons\n\nWe use the adjusted p-values (p.adj.Chisq) to determine statistical significance, as these account for the increased risk of Type I errors when performing multiple comparisons. At a significance level of 0.05, we can see that the distribution of side effects differs significantly between the 18–39 and 60+ age groups (adjusted p &lt; 0.001), while the other pairwise comparisons are not statistically significant after correction.\n\n\n\n\n\n\nNote\n\n\n\nThe output also includes p-values from the G-test (p.Gtest and p.adj.Gtest), which is an alternative to the chi-square test based on the likelihood ratio. In most cases, the chi-square and G-test give similar results.\n\n\nAlternatively, we can perform the pairwise comparisons manually to better understand the underlying process. We start by comparing the first two age groups (18–39 and 40–59). First, we set up the contingency table for these two groups:\n\ntable_12 &lt;- side_effects[c(\"18–39\", \"40–59\"), ]\ntable_12\n\nNext, we perform the chi-square test for this subset of the data and adjust the p-value by applying the Bonferroni correction:\n\n# Perform chi-square test for the subset of data\nchisq_test_12 &lt;- chisq.test(table_12)\nprint(chisq_test_12)\n\n# Adjust the p-value for multiple testing\np_adjusted_12 &lt;- 3*chisq_test_12$p.value\np_adjusted_12\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nPerform the pairwise comparison between the other two pairs of age groups (40–59 and 60+, 18–39 and 60+) using the same approach.\n\n\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the pairwise comparisons, which age groups have significantly different distributions of side effects?"
  },
  {
    "objectID": "R_lab4.html#introduction",
    "href": "R_lab4.html#introduction",
    "title": "Medical Statistics – Lab 4",
    "section": "Introduction",
    "text": "Introduction\nIn this part of the lab, we will analyze paired data on pocket depth before and after an intervention. Pocket depth refers to the depth of the gum pockets around teeth, measured using a periodontal probe. It is an important indicator of periodontal health. Healthy gums typically have pocket depths less than 3 mm, while deeper pockets may indicate conditions such as gingivitis or periodontitis.\nThe dataset pockets_paired.sav, available from the Datasets menu, contains the following columns:\n\nsubjectID: Unique identifier for each participant\npocket_depth_before: Average pocket depth (in mm) measured before the intervention\npocket_depth_after: Average pocket depth (in mm) measured after the intervention\n\nThe objective is to determine whether the intervention significantly reduces pocket depth. We will apply three statistical methods to analyze the paired data:\n\nPaired t-test\nSign test\nWilcoxon signed-rank test\n\n\nPaired t-test\nFirst, we load the data from the provided SPSS file pockets_paired.sav:\n\nlibrary(haven)\npockets &lt;- read_sav(\"pockets_paired.sav\")\nhead(pockets)\n\nNext, we perform a paired t-test to compare the average pocket depth before and after the intervention:\n\n# Perform paired t-test\nt_test &lt;- t.test(pockets$pocket_depth_before,\n                 pockets$pocket_depth_after,\n                 paired = TRUE)\nprint(t_test)\n\nThe t.test() function with the argument paired = TRUE conducts a paired t-test; setting paired = FALSE (default) would perform an independent samples t-test. The output includes the test statistic, degrees of freedom, and the p-value.\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nBased on the results of the paired t-test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\nChecking of assumptions\nTo determine whether it is appropriate to apply the paired t-test to these data, we need to verify that the differences in pocket depth before and after the intervention are normally distributed. We can visually inspect the distribution of differences using a histogram:\n\n# Calculate the differences in pocket depth\npockets$diff &lt;- pockets$pocket_depth_after -\n                pockets$pocket_depth_before\n\n# Create a histogram of the differences\nlibrary(ggplot2)\nggplot(pockets, aes(x = diff)) +\n  geom_histogram(binwidth = 0.05,\n                 fill = \"skyblue\",\n                 color = \"black\") +\n  labs(title = \"Distribution of differences in pocket depth\",\n       x = \"Difference in pocket depth (mm)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nBased on the histogram, do the differences in pocket depth appear to be approximately normally distributed?\n\n\n\n\n\nSign test\nThe sign test is a non-parametric test used to compare two related samples. It is based on the signs of the differences between the pairs of observations. We will apply the sign test to the pocket depth data to determine whether the intervention has a significant effect.\nFirst, we calculate the total number of positive and negative signs in the differences:\n\n# Calculate the number of positive and negative signs\nsigns_positive &lt;- sum(pockets$diff &gt; 0)\nsigns_negative &lt;- sum(pockets$diff &lt; 0)\n\nNext, we perform the sign test using the binom.test() function, which calculates the exact p-value for the sign test:\n\n# Perform the sign test\nsign_test &lt;- binom.test(signs_positive,\n                        signs_positive + signs_negative,\n                        p = 0.5)\nprint(sign_test)\n\n\n\n\n\n\n\nImportantQuestion 12\n\n\n\nBased on the results of the sign test, can we conclude that the intervention significantly reduces pocket depth?\n\n\n\n\nWilcoxon signed-rank test\nThe Wilcoxon signed-rank test is another non-parametric test used to compare two related samples. It is based on the ranks of the absolute differences between the pairs of observations. In this case, the sign test is more appropriate because the Wilcoxon signed-rank test requires the assumption of symmetry in the distribution of differences, whereas the previously constructed histogram suggests that the distribution of these differences is left-skewed.\nTo perform the Wilcoxon signed-rank test, we use the wilcox.test() function:\n\n# Perform the Wilcoxon signed-rank test\nwilcox_test &lt;- wilcox.test(pockets$pocket_depth_before,\n                           pockets$pocket_depth_after,\n                           paired = TRUE)\nprint(wilcox_test)\n\n\n\n\n\n\n\nImportantQuestion 13\n\n\n\nBased on the results of the Wilcoxon signed-rank test, can we conclude that the intervention significantly reduces pocket depth?"
  },
  {
    "objectID": "R_lab7.html",
    "href": "R_lab7.html",
    "title": "Medical Statistics – Lab 7",
    "section": "",
    "text": "In part 1 of the lab, we are going to analyze the risk of in-hospital death in patients hospitalized because of acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001 (file whas500.sav from the Datasets menu). The outcome of interest is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\nWe will use the following R packages: haven, dplyr, and DescTools. The first two packages were also used in the previous labs and therefore already installed on your computer. For DescTools (used to perform Hosmer-Lemeshow Goodness of Fit Tests) you may have to download and install it first by running install.packages(\"DescTools\").\n\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(DescTools)  # for performing Hosmer-Lemeshow Goodness of Fit Tests\n\n# Load the dataset\nwhas500 &lt;- read_sav(\"whas500.sav\")\n\n# Convert labeled variables to factors\nwhas500 &lt;- whas500 %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\n\ncontingencyTable &lt;- table(whas500$gender, whas500$dstat) # create 2 x 2 table\ncontingencyTable\n\nprop.table(contingencyTable, margin = 1) # Calulate the row proportions (margin = 1)\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. In R, this can be achieved using the glm() function:\n\n# Create a 0/1 numeric version of the dependent variable, where a value of 1 means \"success\" (i.e., occurrence of the event of interest)\nwhas500$dstat_numeric &lt;- ifelse(whas500$dstat==\"dead\", 1, 0)\n\n# Fit logistic regression model \nmodel.sex &lt;- glm(dstat_numeric ~ gender, family = binomial, data = whas500)\nsummary(model.sex)\n\nExplanation: glm stands for “generalized linear model,” an extension of linear regression that accommodates different types of outcome distributions and includes a link function to model the relationship between predictors and the outcome. The argument family = binomial specifies that the outcome distribution is Bernoulli/binomial. By default, the link function for this model is “logit,” which corresponds to logistic regression.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\n\n# Fit logistic regression model \nmodel.sex.age &lt;- glm(dstat_numeric ~ gender + age, family = binomial, data = whas500)\nsummary(model.sex.age)\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To achieve this, we first need to fit the reduced model that only includes age and then perform the LRT using the anova() function:\n\n# Fit reduced model\nmodel.age &lt;- glm(dstat_numeric ~ age, family = binomial, data = whas500)\n\n# Perform likelihood ratio test\nanova(model.age, model.sex.age, test=\"LRT\")\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In R, this test can be performed using the HosmerLemeshowTest() function from the DescTools package:\n\nHosmerLemeshowTest(fit=fitted(model.sex.age), obs=model.sex.age$y, ngr = 10, verbose = TRUE)\n\nExplanation:\n\nfitted(model.sex.age) returns the caclulated in-hospital death probabilities based on the fitted logistic regression model for each patient in the dataset. These are the\nmodel.sex.age$y retrieves the outcome variable from the fitted logistic regression model.\nngr = 10 specifies the number of groups to be used in the test, which is set to 10 by default.\nverbose = TRUE not only prints the test-statistic, degrees of freedom, and p-value but also the groups used to perform the test.\n\nThe Hosmer-Lemeshow goodness-of-fit test has two variations: the C statistic and the H statistic, which differ in how they group predicted probabilities for comparison. The C statistic groups predicted probabilities into deciles (10 equal-sized groups based on the range of probabilities), while the H statistic uses fixed cutoffs (e.g., evenly spaced intervals between 0 and 1). The C statistic is the most commonly used version as it adapts to the data distribution, ensuring well-populated groups, making it suitable for general model fit evaluation. In this lab, we therefore focus on this latter statistics (with matches the explanation in the syllabus).\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab7.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "href": "R_lab7.html#part-1-risk-of-in-hospital-death-in-patients-with-acute-myocardial-infarction",
    "title": "Medical Statistics – Lab 7",
    "section": "",
    "text": "In part 1 of the lab, we are going to analyze the risk of in-hospital death in patients hospitalized because of acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001 (file whas500.sav from the Datasets menu). The outcome of interest is in-hospital death, measured by the variable “discharge status from hospital” (dstat) with values alive and death.\nWe will use the following R packages: haven, dplyr, and DescTools. The first two packages were also used in the previous labs and therefore already installed on your computer. For DescTools (used to perform Hosmer-Lemeshow Goodness of Fit Tests) you may have to download and install it first by running install.packages(\"DescTools\").\n\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(DescTools)  # for performing Hosmer-Lemeshow Goodness of Fit Tests\n\n# Load the dataset\nwhas500 &lt;- read_sav(\"whas500.sav\")\n\n# Convert labeled variables to factors\nwhas500 &lt;- whas500 %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n\n\nTo explore whether gender has an effect on the risk of in-hospital death, we start by creating a contingency table and use the table to calculate the proportion in-hospital death in the two gender subgroups:\n\ncontingencyTable &lt;- table(whas500$gender, whas500$dstat) # create 2 x 2 table\ncontingencyTable\n\nprop.table(contingencyTable, margin = 1) # Calulate the row proportions (margin = 1)\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the group proportions, do you expect gender to have an effect on the risk of in-hospital death?\n\n\n\n\n\nTo determine whether gender is significantly associated with in-hospital death, we can conduct several statistical tests. As a recap of lab 4, we start by performing a chi-square test of homogeneity.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nPerform the chi-square test of homogeneity (see instructions in lab 4 if needed). What conclusion can be drawn from the test?\n\n\nAnother option is to perform logistic regression. In R, this can be achieved using the glm() function:\n\n# Create a 0/1 numeric version of the dependent variable, where a value of 1 means \"success\" (i.e., occurrence of the event of interest)\nwhas500$dstat_numeric &lt;- ifelse(whas500$dstat==\"dead\", 1, 0)\n\n# Fit logistic regression model \nmodel.sex &lt;- glm(dstat_numeric ~ gender, family = binomial, data = whas500)\nsummary(model.sex)\n\nExplanation: glm stands for “generalized linear model,” an extension of linear regression that accommodates different types of outcome distributions and includes a link function to model the relationship between predictors and the outcome. The argument family = binomial specifies that the outcome distribution is Bernoulli/binomial. By default, the link function for this model is “logit,” which corresponds to logistic regression.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the odds ratio for in-hospital death for females compared to males? How should this odds ratio be interpreted in the context of the study?\n\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nBased on the estimated regression coefficients (ignoring p-values), what are the predicted proportions of in-hospital deaths for male and female patients? Compare the predicted proportions to the observed proportions from the previously constructed contingency table. Do they match?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nWhat conclusion can be drawn from the logistic regression analysis regarding the association between gender and in-hospital death? Is this in line with the conclusion drawn from the chi-square test?\n\n\n\n\n\nTo assess the extent to which the effect of gender is confounded by age, we will fit a multiple regression model with in-hospital death as the dependent variable and gender and age as the independent variables:\n\n# Fit logistic regression model \nmodel.sex.age &lt;- glm(dstat_numeric ~ gender + age, family = binomial, data = whas500)\nsummary(model.sex.age)\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nHow does adjusting for age affect the estimated odds ratio for in-hospital death for females compared to males?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nCalculate the odds ratio for in-hospital death corresponding to a 10-year increase in age and interpret its meaning.\n\n\n\n\n\nAs explained in the syllabus, the p-values in the table of estimated regression coefficients are derived from Wald tests, which test the null hypothesis that the corresponding regression coefficient is equal to 0\nInstead of the Wald tests, we can also obtain p-values using likelihood ratio tests, which compare the goodness of fit of the full model (including the predictor of interest) to a reduced model (excluding the predictor) to test the null hypothesis that the predictor has no effect on the outcome. This approach is particularly useful for testing predictors with non-linear or complex effects, as it evaluates their contribution to the model as a whole. Examples include categorical variables with three or more categories (requiring the creation of multiple dummy variables) and relationships modeled using multiple terms, such as including both a linear and a quadratic term to capture a quadratic relationship.\nFor example, we can use a likelihood ratio test (LRT) to compare a full model including both sex and age as predictors to a reduced model including only age. To achieve this, we first need to fit the reduced model that only includes age and then perform the LRT using the anova() function:\n\n# Fit reduced model\nmodel.age &lt;- glm(dstat_numeric ~ age, family = binomial, data = whas500)\n\n# Perform likelihood ratio test\nanova(model.age, model.sex.age, test=\"LRT\")\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow does the p-value from the likelihood ratio test compare to the one from the Wald test?\n\n\n\n\n\nOne way to examine the fit of the logistic regression model is the Hosmer-Lemeshow Goodness of Fit test. In R, this test can be performed using the HosmerLemeshowTest() function from the DescTools package:\n\nHosmerLemeshowTest(fit=fitted(model.sex.age), obs=model.sex.age$y, ngr = 10, verbose = TRUE)\n\nExplanation:\n\nfitted(model.sex.age) returns the caclulated in-hospital death probabilities based on the fitted logistic regression model for each patient in the dataset. These are the\nmodel.sex.age$y retrieves the outcome variable from the fitted logistic regression model.\nngr = 10 specifies the number of groups to be used in the test, which is set to 10 by default.\nverbose = TRUE not only prints the test-statistic, degrees of freedom, and p-value but also the groups used to perform the test.\n\nThe Hosmer-Lemeshow goodness-of-fit test has two variations: the C statistic and the H statistic, which differ in how they group predicted probabilities for comparison. The C statistic groups predicted probabilities into deciles (10 equal-sized groups based on the range of probabilities), while the H statistic uses fixed cutoffs (e.g., evenly spaced intervals between 0 and 1). The C statistic is the most commonly used version as it adapts to the data distribution, ensuring well-populated groups, making it suitable for general model fit evaluation. In this lab, we therefore focus on this latter statistics (with matches the explanation in the syllabus).\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nBased on the results of the Hosmer-Lemeshow goodness-of-fit test, does our model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab7.html#part-2-unguided-exercises",
    "href": "R_lab7.html#part-2-unguided-exercises",
    "title": "Medical Statistics – Lab 7",
    "section": "Part 2: unguided exercises",
    "text": "Part 2: unguided exercises\n\nExercise 1\nMultiple logistic regression was used to construct a prognostic index to predict coronary artery disease from data on 348 patients with valvular heart disease who had undergone routine coronary arteriography before valve replacement (Ramsdale et al. 1982). The estimated equation was:\n\\[logit(p) = ln(p/(1-p)) = b_{0} + 1.167 \\times x{1} + 0.0106 \\times x_{2} + \\textrm{other terms}\\]\nwhere \\(x_{1}\\) stands for the family history of ischaemic disease (0=no, 1=yes) and \\(x_{2}\\) is the estimated total number of cigarettes ever smoked in terms of thousand cigarettes, calculated as the average number smoked annually times the number of years smoking.\n\nWhat is the estimated odds ratio for having coronary artery disease for subjects with a positive family history relative to subjects with a negative family history?\nWhat total number of cigarettes ever smoked carries the same risk as a positive family history? Convert the result into years of smoking 20 cigarettes per day.\nWhat is the odds ratio for coronary artery disease for someone with a positive family history who had smoked 20 cigarettes a day for 30 years compared to a non smoker with no family history?\n\n\n\nExercise 2\nData from 37 patients receiving a non-depleted allogenic bone marrow transplant were examined to see which variables were associated with the occurrence of acute graft-versus-host disease (GvHD: 0=no, 1=yes) (Bagot et al., 1988). Possible predictors are TYPE (type of leukemia: 1=AML, acute myeloid leukaemia; 2=ALL, acute lymphocytic leukaemia; 3=CML, chronic myeloid leukemia), PREG (donor pregnancy: 0= no, 1=yes), and LOGIND (the logarithm of an index of mixed epidermal cell-lymphocyte reactions). The data are in the file GvHD.sav available from the Downloads menu.\n\nPerform a likelihood ratio test to determine whether there is a significant association between the type of leukemia and the occurrence of GvHD after adjusting for donor pregnancy and the logarithm of an index of mixed epidermal cell-lymphocyte reactions.\nIn the adjusted model, What is the estimated odds ratio for the occurrence of GvHD for patients with ALL compared to those with ALM?\nUse the Hosmer-Lemeshow goodness-of-fit test to evaluate the fit of the model. Based on the results, does the model provide a satisfactory fit to the data?"
  },
  {
    "objectID": "R_lab9.html",
    "href": "R_lab9.html",
    "title": "Medical Statistics – Lab 9",
    "section": "",
    "text": "In this section, we are going to continue analyzing the Worcester Heart Attack Study (WHAS) dataset (file whas500.sav). The outcome of interest for today’s analysis is overall survival, defined as the time from hospital admission to death from any cause. This information is captured in the variable lenfol (length of follow-up in days) and the variable fstat (follow-up status; dead or censored).\n\nlibrary(haven)   # for reading SPSS files\nlibrary(dplyr)   # for data manipulation\nlibrary(survival)  # for performing survival analysis\n\n# Load the dataset\nwhas500 &lt;- read_sav(\"datasets/whas500.sav\")\n\n# Convert labeled variables to factors\nwhas500 &lt;- whas500 %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n# Create a numerical variable for follow-up status (1 = dead, 0 = censored)\nwhas500$fstat_numeric &lt;- ifelse(whas500$fstat==\"dead\", 1, 0)\n\n\n\nThe variable miord represents the sequence of myocardial infarction (MI) events, categorized as either a first MI or a recurrent MI. Our aim is to analyze the relationship between MI order and overall survival outcomes.\n\n\nWe start by constructing a Kaplan-Meier survival curve to compare the survival probabilities between patients with a first MI and those with a recurrent MI:\n\n# Kaplan-Meier by MI order\ngroup_fit &lt;- survfit(Surv(lenfol, fstat_numeric) ~ miord, \n                     data = whas500)\nsummary(group_fit)\n\nplot(group_fit, col = c(\"blue\", \"red\"), \n     xlab = \"Follow-up Time (days)\", \n     ylab = \"Survival Probability\", \n     main = \"Survival by MI Order\")\nlegend(\"topleft\", legend = c(\"First MI\", \"Recurrent MI\"), \n       col = c(\"blue\", \"red\"), lty = 1)\n\nExplanation of the code:\n\nThe survfit() function is used to fit a Kaplan-Meier survival curve for the two groups defined by the miord variable.\nThe formula Surv(lenfol, fstat) ~ miord specifies the survival time and event status as the response variables and the MI order as the predictor.\nsummary(group_fit) provides the survival probabilities at different time points for each group.\nThe plot() function is used to visualize the Kaplan-Meier curves for the two groups.\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\nTo formally test the difference in survival between the two groups, we can use the logrank test:\n\n# Logrank test\nsurvdiff(Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\nNext, we will perform a Cox proportional hazards regression analysis to assess the association between MI order and overall survival while adjusting for potential confounders. We will start with the unadjusted model:\n\n# Unadjusted Cox regression\ncoxph_model_unadj &lt;- coxph(Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\nsummary(coxph_model_unadj)\n\nExplanation of the code:\n\nThe coxph() function is used to fit a Cox proportional hazards regression model with the survival time and event status as the response variables and the MI order as the predictor.\nThe summary() function provides the estimated hazard ratio (HR) and its significance.\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\nNow, let’s adjust the Cox regression model using age and gender as potential confounders:\n\n# Adjusted Cox regression\ncoxph_model_adj &lt;- coxph(Surv(lenfol, fstat_numeric) ~ \n                           miord + age + gender, \n                         data = whas500)\nsummary(coxph_model_adj)\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?"
  },
  {
    "objectID": "R_lab9.html#association-between-mi-order-and-overall-survival",
    "href": "R_lab9.html#association-between-mi-order-and-overall-survival",
    "title": "Medical Statistics – Lab 9",
    "section": "",
    "text": "The variable miord represents the sequence of myocardial infarction (MI) events, categorized as either a first MI or a recurrent MI. Our aim is to analyze the relationship between MI order and overall survival outcomes.\n\n\nWe start by constructing a Kaplan-Meier survival curve to compare the survival probabilities between patients with a first MI and those with a recurrent MI:\n\n# Kaplan-Meier by MI order\ngroup_fit &lt;- survfit(Surv(lenfol, fstat_numeric) ~ miord, \n                     data = whas500)\nsummary(group_fit)\n\nplot(group_fit, col = c(\"blue\", \"red\"), \n     xlab = \"Follow-up Time (days)\", \n     ylab = \"Survival Probability\", \n     main = \"Survival by MI Order\")\nlegend(\"topleft\", legend = c(\"First MI\", \"Recurrent MI\"), \n       col = c(\"blue\", \"red\"), lty = 1)\n\nExplanation of the code:\n\nThe survfit() function is used to fit a Kaplan-Meier survival curve for the two groups defined by the miord variable.\nThe formula Surv(lenfol, fstat) ~ miord specifies the survival time and event status as the response variables and the MI order as the predictor.\nsummary(group_fit) provides the survival probabilities at different time points for each group.\nThe plot() function is used to visualize the Kaplan-Meier curves for the two groups.\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the Kaplan-Meier table, what are the estimated survival probabilities at 3 years for patients with a first MI and those with a recurrent MI?\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Kaplan-Meier curves, do you observe any differences in survival times between patients with a first MI and those with a recurrent MI?\n\n\nTo formally test the difference in survival between the two groups, we can use the logrank test:\n\n# Logrank test\nsurvdiff(Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nBased on the results of the logrank test, is there a significant difference in overall survival between patients with a first MI and those with a recurrent MI?\n\n\n\n\n\nNext, we will perform a Cox proportional hazards regression analysis to assess the association between MI order and overall survival while adjusting for potential confounders. We will start with the unadjusted model:\n\n# Unadjusted Cox regression\ncoxph_model_unadj &lt;- coxph(Surv(lenfol, fstat_numeric) ~ miord, data = whas500)\nsummary(coxph_model_unadj)\n\nExplanation of the code:\n\nThe coxph() function is used to fit a Cox proportional hazards regression model with the survival time and event status as the response variables and the MI order as the predictor.\nThe summary() function provides the estimated hazard ratio (HR) and its significance.\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nWhat is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI based on the unadjusted Cox regression model?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes the result of the Cox regression model support the findings from the logrank test regarding the association between MI order and overall survival?\n\n\nNow, let’s adjust the Cox regression model using age and gender as potential confounders:\n\n# Adjusted Cox regression\ncoxph_model_adj &lt;- coxph(Surv(lenfol, fstat_numeric) ~ \n                           miord + age + gender, \n                         data = whas500)\nsummary(coxph_model_adj)\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nAfter adjusting for age and gender, what is the hazard ratio (HR) for patients with a recurrent MI compared to those with a first MI? How does this compare to the unadjusted HR? Can you explain the change in the HR after adjusting for these variables?"
  },
  {
    "objectID": "SPSS_lab1.html",
    "href": "SPSS_lab1.html",
    "title": "Medical Statistics – Lab 1",
    "section": "",
    "text": "Welcome to lab 1 in the Medical statistics course. In this lab, we will explore descriptive statistics and probability calculations for random variables. We will use an example dataset to practice summarizing continuous and categorical variables, and introduce some basic concepts of probability distributions."
  },
  {
    "objectID": "SPSS_lab1.html#descriptive-analysis-of-continuous-variables",
    "href": "SPSS_lab1.html#descriptive-analysis-of-continuous-variables",
    "title": "Medical Statistics – Lab 1",
    "section": "Descriptive analysis of continuous variables",
    "text": "Descriptive analysis of continuous variables\nLet’s start by calculating the summary statistics for the continuous variable age.\n\nMean and Standard Deviation: Use the “Analyze &gt; Descriptive Statistics &gt; Descriptives…” menu in SPSS to calculate the mean age of the mothers. Select age as the variable, and ensure that the mean and standard deviation are checked in the “Options” dialog.\nMedian and Interquartile Range (IQR): To calculate the median and IQR for age, use the “Analyze &gt; Descriptive Statistics &gt; Frequencies…” menu. Select age as the variable and click on “Statistics…” to choose the median and quartiles.\n\nTo decide which summary measures (mean and standard deviation, or median and IQR) are appropriate to report, we need to understand the shape of the distribution of the age variable. Create a histogram by using “Graphs &gt; Legacy Dialogs &gt; Histogram…” and selecting age as the variable.\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the shape of the histogram, determine which summary statistics are more appropriate to report.\n\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nCalculate the mean, standard deviation, median, and IQR for the variable lwt. Additionally, create a histogram to determine the shape of its distribution and decide which summary measures are most appropriate to report."
  },
  {
    "objectID": "SPSS_lab1.html#descriptive-analysis-of-categorical-variables",
    "href": "SPSS_lab1.html#descriptive-analysis-of-categorical-variables",
    "title": "Medical Statistics – Lab 1",
    "section": "Descriptive analysis of categorical variables",
    "text": "Descriptive analysis of categorical variables\nLet’s move on to analyzing the categorical variables. We will start by calculating the frequency and percentage of mothers who smoked during pregnancy (smoke):\n\nFrequency Table: Use the “Analyze &gt; Descriptive Statistics &gt; Frequencies…” menu to calculate the frequency of each category for smoke.\nPercentage Calculation: SPSS will automatically calculate the percentages for each category in the frequency table output.\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nCalculate the frequencies and percentages for the variable ht (history of hypertension).\n\n\nIn addition to calculating frequencies and percentages, it can also be helpful to visualize categorical data. One common way to do this is by creating a bar chart. To create a bar chart for the smoke variable, use “Graphs &gt; Legacy Dialogs &gt; Bar…” and select “Simple” and “Summaries for Groups of Cases.” Then select smoke as the Category Axis.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nCreate a bar chart for the variable ht to visualize the frequency of mothers with a history of hypertension."
  },
  {
    "objectID": "SPSS_lab1.html#binomial-distribution",
    "href": "SPSS_lab1.html#binomial-distribution",
    "title": "Medical Statistics – Lab 1",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial distribution represents the number of successes in a fixed number of independent trials, each with the same probability of success. For example, if we have 10 patients and we want to know the probability that exactly 3 of them respond to a given treatment, where the response rate is known to be 40%, we can use SPSS to calculate this probability.\nTo perform this calculation in SPSS, go to “Transform &gt; Compute Variable…” and create a new variable called p_response_3. Use the function PDF.BINOM(x, n, p) to calculate the cumulative probability:\n\nx: The number of successes we are interested in (in this example, 3 patients responding).\nn: The number of trials (in this example, 10 patients).\np: The probability of success in each trial (in this example, 0.4 or 40% response rate).\n\nCumulative probabilities, such as the probability that 3 or fewer patients out of 10 respond to the treatment, can similarly be calculated by creating a new variable with the CDF.BINOM(x, n, p) function.\nNote: Unlike other statistical software, SPSS requires that calculations performed using the “Transform &gt; Compute Variable…” tool always have a target variable specified. This means you cannot perform calculations without creating an output variable in your dataset, which can make one-time calculations cumbersome. This is a limitation of SPSS, especially when you just want to explore different scenarios without cluttering your dataset with temporary variables.\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nThe probability of being blood group B is 0.08. What is the probability that if 500 ml of blood is taken from each of 100 unrelated blood donors fewer than 1,500 ml of group B blood will be obtained?\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nIn a clinical trial in which a total of 100 patients are allocated to two treatments A and B by simple randomization (tossing a coin for each new patient). What is the probability that the difference between the numbers of patients in the two treatment groups exceeds 20? (Hint: the number of individuals in one treatment group (for example A) follows a Binomial distribution)."
  },
  {
    "objectID": "SPSS_lab1.html#normal-distribution",
    "href": "SPSS_lab1.html#normal-distribution",
    "title": "Medical Statistics – Lab 1",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nSuppose that we want to calculate the probability that a randomly selected individual has a weight less than or equal to 80 kg, assuming that the distribution of weight in the population follows a normal distribution with mean 72 kg and standard deviation 10 kg.\nTo calculate this probability, we first need to standardize the value using the formula:\n\\[\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{80-72}{10} = 0.8\n\\]\nwhere:\n\nx is the value we want to standardize (in this case, 80 kg).\nμ is the mean of the distribution (in this case, 72 kg).\nσ is the standard deviation of the distribution (in this case, 10 kg).\n\nUsing SPSS, we can then use the “Transform &gt; Compute Variable…” menu and the CDF.NORMAL(z, mean, sd) function to find the corresponding cumulative probability from the standard normal distribution by setting z = 0.8, mean = 0 and sd = 1.\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nOver a 25 year period the mean height of adult males increased from 175.8 cm to 179.1 cm, but the standard deviation stayed at 5.84 cm. The minimum height requirement for men to join the police force is 172 cm. What proportion of men would be too short to become policemen at the beginning and end of the 25 year period, assuming that the height of adult males has a Normal distribution?"
  },
  {
    "objectID": "SPSS_lab3.html",
    "href": "SPSS_lab3.html",
    "title": "Medical Statistics – Lab 3",
    "section": "",
    "text": "Welcome to lab 3 in the medical statistics course. For today’s exercises, we will continue exploring the lowbwt.sav dataset, which you can downloaded from the Dataset section of the menu.\nAs a reminder, the dataset includes the following variables (see the previous lab for more details):"
  },
  {
    "objectID": "SPSS_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "href": "SPSS_lab3.html#part-1-independent-samples-t-test-and-mann-whitney-u-test",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 1: Independent Samples t-test and Mann-Whitney U Test",
    "text": "Part 1: Independent Samples t-test and Mann-Whitney U Test\nIn this part of the lab, we will examine the effect of smoking during pregnancy on birth weight.\n\nExploratory data analysis\nTo get a sense of the data and the extent of a possible effect of smoking, we start by creating a boxplot. In SPSS, this can be achieved by following these steps:\n\nCreate Boxplot: Go to Graphs &gt; Chart Builder.\nSelect Boxplot: In the Chart Builder, select Boxplot from the gallery.\nSelect Variables: Drag bwt to the y-axis and smoke to the x-axis to create a boxplot comparing the birth weights for mothers who smoked and those who did not.\nRun the Graph: Click OK to generate the boxplot.\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the boxplot, do you expect the smoking status to have an effect on birth weight?\n\n\n\n\nIndependent samples t-test\nNext, we will perform an independent samples t-test to compare the mean birth weights between mothers who smoked and those who did not. To conduct this analysis in SPSS, follow these steps:\n\nPerform the Test: Go to Analyze &gt; Compare Means &gt; Independent-Samples t Test.\nSelect Test Variable and Grouping Variable: Move bwt to the Test Variable box and smoke to the Grouping Variable box.\nDefine Groups: Click on Define Groups and specify 0 and 1 for mothers who did not smoke and mothers who smoked, respectively.\nRun the Test: Click OK to run the analysis.\n\nSPSS will generate several tables. Here we focus on the table Independent Samples Test, which includes:\n\nLevene’s Test for Equality of Variances: Tests the assumption of equal variances between the two groups. The F and Sig. (p-value) columns represent the results of this test. If the p-value is greater than 0.05, the assumption of equal variances holds, and you should use the row labeled “Equal variances assumed” for interpreting the t-test results. If the p-value is less than or equal to 0.05, the assumption of equal variances is violated, and you should use the row labeled “Equal variances not assumed” for interpreting the t-test results.\nt-test for Equality of Means: Provides the t-test results, including the t-value, degrees of freedom, and p-value. It also includes the Mean Difference and 95% Confidence Interval of the Difference. It is important to choose the appropriate row (“Equal variances assumed” or “Equal variances not assumed”) based on the result of Levene’s test.\n\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nBased on the Levene test, does the assumption of equal variances hold?\n\n\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nIs there a significant difference in birth weights between mothers who smoked and those who did not?\n\n\n\n\nChecking of assumptions\nTo assess whether the assumption of normality holds for the outcome variable in both groups, follow these steps to create histograms:\n\nCreate the Graph: Go to Graphs &gt; Chart Builder.\nCreate Histogram: In the Chart Builder, select Histogram from the gallery.\nSelect Variables: Drag bwt into the x-axis box.\nSplit by Group: In the Chart Builder, click on the Groups/Points ID tab. Check the Rows panel variable checkbox to add a panel field to the chart preview. Then, drag the smoke variable into this field to split the histograms by group.\nRun the Graph: Click OK to generate the histograms.\n\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nDo the histograms indicate that the birth weight data are approximately normally distributed for both groups?\n\n\n\n\n95% Confidence Interval for the mean difference\nIn addition to performing hypothesis tests, it is often informative to estimate the effect size and its uncertainty. One way to do this is by calculating a confidence interval for the mean difference in birth weight between the two groups. The 95% confidence interval is included in the output provided by the independent samples t-test procedure, so in principle, we could extract it from there. As an exercise, we are also going to calculate it manually based on the formulas provided in the lecture/course syllabus.\nTo calculate the summary statistics required for the manual calculation, follow these steps:\n\nNavigate to Explore Analysis:\n\nGo to Analyze &gt; Descriptive Statistics &gt; Explore.\n\nSelect Variables:\n\nMove bwt to the Dependent List box.\nMove smoke to the Factor List box.\n\nSet Display Options:\n\nCheck the option Statistics to prevent SPSS from cluttering the output with a range of plots.\n\nRun the Analysis:\n\nClick OK to run the analysis. The output will provide group-wise means, standard deviations, and sample sizes, which can be used for manual calculations.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nBased on these summary statistics, calculate the pooled standard deviation and the standard error of the mean difference. Then compute the 95% confidence interval for the mean difference in birth weight between mothers who smoked and those who did not.\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nDoes your manually calculated 95% confidence interval for the mean difference in birth weight between the two groups agree with the one provided in the output of the independent samples t-test procedure?\n\n\n\n\nMann-Whitney U Test\nIn case the assumption of normality is violated, we can use the Mann-Whitney U test as a non-parametric alternative to the independent samples t-test. To perform this test in SPSS, follow these steps:\n\nNavigate to the Mann-Whitney U Test:\n\nGo to Analyze &gt; Nonparametric Tests &gt; Legacy Dialogs &gt; 2 Independent Samples.\n\nSelect Variables:\n\nMove bwt to the Test Variable List.\nMove smoke to the Grouping Variable.\nClick Define Groups and specify the values of the smoke variable (e.g., 0 = No, 1 = Yes). Click Continue.\n\nSelect the Mann-Whitney U Test:\n\nUnder Test Type, ensure that Mann-Whitney U is selected.\n\nRun the Test:\n\nClick OK to run the analysis. The output will include the test statistic and associated p-value.\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat are the null and alternative hypotheses for the Mann-Whitney U test, and what does the p-value indicate about the difference in birth weight between mothers who smoked and those who did not?"
  },
  {
    "objectID": "SPSS_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "href": "SPSS_lab3.html#part-2-one-way-anova-and-kruskal-wallis-test",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 2: One-Way ANOVA and Kruskal-Wallis Test",
    "text": "Part 2: One-Way ANOVA and Kruskal-Wallis Test\nIn this part of the lab, we are going to examine the effect of ethnicity on birth weight.\n\nExploratory data analysis\nTo get a sense of the differences in birth weights across the different ethnic groups, we will create a boxplot. In SPSS, follow these steps:\n\nCreate Boxplot:\n\nGo to Graphs &gt; Chart Builder.\nSelect Boxplot from the gallery.\n\nAssign Variables:\n\nDrag bwt to the y-axis.\nDrag ethnicity to the x-axis.\n\nRun the Graph:\n\nClick OK to generate the boxplot.\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nWhat does the boxplot suggest about the distribution of birth weights across different ethnic groups?\n\n\n\n\nOne-way ANOVA\nTo test whether the mean birth weights differ across ethnic groups, we will perform a one-way ANOVA. Follow these steps:\n\nPerform the Test:\n\nGo to Analyze &gt; Compare Means &gt; One-Way ANOVA.\n\nSelect Variables:\n\nSet Dependent Variable: Birth Weight (bwt).\nSet Factor: Ethnicity (ethnicity).\n\nRun the Test:\n\nClick OK to run the analysis.\n\n\nSPSS will generate the following key output:\n\nANOVA Table: Contains the F-statistic and the associated p-value. These values indicate whether there is a statistically significant difference in mean birth weights across ethnic groups.\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nWhat conclusions can be drawn from the results of the one-way ANOVA?\n\n\n\n\nPost-hoc tests\nIf the one-way ANOVA indicates a statistically significant difference, we will perform post-hoc tests to determine which specific groups differ from each other. To do this in SPSS:\n\nEnable Post-Hoc Tests:\n\nIn the one-way ANOVA dialog, click on the Post Hoc button.\nSelect Bonferroni as the method.\n\nRun the Test:\n\nClick Continue and then OK to run the analysis.\n\n\nThe post-hoc output will include pairwise comparisons of the means between all groups, with adjusted p-values.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nWhich groups differ significantly, and what do the results suggest about the differences in birth weights across ethnic groups?\n\n\n\n\nChecking of assumptions\nTo assess whether the results of the one-way ANOVA are valid, we need to check the assumptions of normality and homogeneity of variances. This step is analogous to the previous examples, and is left as an exercise.\n\n\nKruskal-Wallis Test\nIf the assumptions of the one-way ANOVA are not satisfied, we can use the Kruskal-Wallis test as a non-parametric alternative. To perform this test in SPSS, follow these steps:\n\nNavigate to the Kruskal-Wallis Test:\n\nGo to Analyze &gt; Nonparametric Tests &gt; Independent Samples.\n\nSelect Variables (Fields tab):\n\nMove bwt to the Test Fields box.\nMove ethnicity to the Groups box.\n\nConfigure Settings (Settings tab):\n\nSelect Customize tests.\nCheck the box for Kruskal-Wallis 1-way ANOVA (k samples).\nCheck the box for All pairwise under the multiple comparisons options to enable Dunn’s post-hoc test.\n\nRun the Test:\n\nClick Run to run the analysis. The output will include the test statistic and associated p-value for the overall Kruskal-Wallis test.\n\n\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nAre the results of the Kruskal-Wallis test consistent with the one-way ANOVA results?\n\n\n\n\nPost-hoc tests for Kruskal-Wallis\nIf the Kruskal-Wallis test indicates a statistically significant difference between groups, we can examine the pairwise comparisons to determine which specific groups differ from each other. SPSS performs Dunn’s test for these pairwise comparisons.\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nWhat conclusions can be drawn from Dunn’s test? Are these consistent with the post-hoc comparisons from the one-way ANOVA?"
  },
  {
    "objectID": "SPSS_lab3.html#part-3-unguided-exercises",
    "href": "SPSS_lab3.html#part-3-unguided-exercises",
    "title": "Medical Statistics – Lab 3",
    "section": "Part 3: Unguided exercises",
    "text": "Part 3: Unguided exercises\n\nEffect of hypertension on birth weight\nExamine the effect of history of hypertension on birth weight by performing the following steps:\n\nCreate a boxplot to visualize the distribution of birth weights by history of hypertension\nPerform an independent samples t-test to compare the mean birth weights between mothers with and without a history of hypertension\nCheck the assumptions of the t-test, including normality and homogeneity of variances\nIf the assumptions of the t-test are violated, perform a Mann-Whitney U test as a non-parametric alternative\n\n\n\nComparing red cell folate levels across ventilation strategies in cardiac bypass patients\nTwenty-two patients undergoing cardiac bypass surgery were randomized to one of three ventilation groups:\n\nGroup I: Received a 50% nitrous oxide and 50% oxygen mixture continuously for 24 hours\nGroup II: Received a 50% nitrous oxide and 50% oxygen mixture only during the operation\nGroup III: Received no nitrous oxide and a 35-50% oxygen mixture continuously for 24 hours\n\nThe data file ex5_6.sav contains the red cell folate levels for the three groups after 24 hours of ventilation. The aim of this study is to compare the three groups and test whether they have the same red cell folate levels.\n\nTasks\n\nExploratory data analysis Create a boxplot to visualize the distribution of red cell folate levels by ventilation group. Based on this plot:\n\nWhat are your first conclusions regarding the means and variances of the different groups?\n\nPerform a one-way ANOVA:\n\nInterpret the results\nAre the assumptions satisfied?\n\nTry a log transformation on the data:\n\nPerform another one-way ANOVA\nAre the assumptions satisfied after the transformation?\n\nDetermine which means differ:\n\nWhich means do you think differ?\nExplain your reasoning.\n\nTry a non-parametric approach:\n\nWhat are your conclusions from this method?"
  },
  {
    "objectID": "SPSS_lab6.html",
    "href": "SPSS_lab6.html",
    "title": "Medical Statistics – Lab 6",
    "section": "",
    "text": "Welcome to lab 6 on correlation and linear regression. In today’s exercises, we will be analyzing a dataset named pockets.sav, which you can download from the Datasets menu. This dataset contains measurements of periodontal pocket depth for a group of individuals, along with several demographic and lifestyle variables.\nBelow is an overview of the variables we will be working with:"
  },
  {
    "objectID": "SPSS_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "href": "SPSS_lab6.html#part-1-pearsons-correlation-coefficient-and-simple-linear-regression",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 1: Pearson’s Correlation Coefficient and Simple Linear Regression",
    "text": "Part 1: Pearson’s Correlation Coefficient and Simple Linear Regression\nIn this section, we will investigate whether age is associated with pocket depth. We begin by creating a scatterplot to visualize the relationship between pocketdepth and age.\n\nOpen pockets.sav in SPSS.\n\nGo to Graphs → Legacy Dialogs → Scatter/Dot.\n\nChoose Simple Scatter and click Define.\n\nPlace age on the X Axis and pocketdepth on the Y Axis.\n\nClick OK to generate the scatterplot.\n\n\n\n\n\n\n\nImportantQuestion 1\n\n\n\nBased on the scatterplot, is there an indication of a linear association between age and pocketdepth? If so, is this association positive or negative?\n\n\n\nPearson’s Correlation Coefficient\nTo quantify the strength and direction of the linear relationship between age and pocketdepth, we will calculate Pearson’s correlation coefficient:\n\nGo to Analyze → Correlate → Bivariate.\n\nMove age and pocketdepth into the Variables list.\n\nMake sure Pearson is checked under Correlation Coefficients.\n\nClick OK.\n\nSPSS will output the correlation coefficient and a p-value testing whether the correlation is different from zero.\n\n\n\n\n\n\nImportantQuestion 2\n\n\n\nWhat does the correlation coefficient tell us about the relationship between age and pocketdepth? Does this align with your interpretation of the scatterplot?\n\n\nYou will also see a Sig. (2-tailed) value in the SPSS output for the Pearson correlation, which corresponds to the test that the correlation coefficient is significantly different from zero.\n\n\n\n\n\n\nImportantQuestion 3\n\n\n\nWhat is the p-value for the correlation coefficient test? Based on this p-value, do we have sufficient evidence to reject the null hypothesis (that the correlation is zero)?\n\n\n\n\nFitting a Simple Linear Regression Model\nNext, we fit a simple linear regression model to quantify the relationship between age and pocketdepth.\n\nGo to Analyze → Regression → Linear.\n\nPut pocketdepth in the Dependent box.\n\nPut age in the Independent(s) box.\n\nClick OK to run the analysis.\n\nSPSS will produce output tables, including Model Summary, ANOVA, and Coefficients.\n\n\n\n\n\n\nImportantQuestion 4\n\n\n\nIs the relationship between age and pocketdepth statistically significant (at α = 0.05) according to the regression output?\n\n\n\n\n\n\n\n\nImportantQuestion 5\n\n\n\nHow does the p-value for age in the regression output compare to the p-value for the correlation coefficient test? Are they consistent?\n\n\n\n\n\n\n\n\nImportantQuestion 6\n\n\n\nWhat is the interpretation of the intercept and the slope coefficient for age in the regression output?\n\n\n\n\n\n\n\n\nImportantQuestion 7\n\n\n\nBased on the fitted model, what is the expected pocket depth for a person who is 40 years old?\n\n\n\n\n\n\n\n\nImportantQuestion 8\n\n\n\nHow much of the variation in pocket depth is explained by age in this model?\n\n\n\n\nAssumption Checking\nTo assess assumptions (normality of residuals, homoscedasticity, etc.), we inspect residual plots.\n\nIn the Linear Regression dialog, click Plots.\n\nMove ZRESID (standardized residuals) to the Y: box and ZPRED (standardized predicted values) to the X: box under Scatter.\n\nAlso check Histogram and Normal probability plot.\n\nClick Continue → OK.\n\nSPSS will generate:\n\nA Histogram of the residuals (for checking normality).\nA Normal P-P plot of the residuals (another way to check normality).\nA scatterplot of standardized residuals vs. standardized predicted values (for checking homoscedasticity and linearity).\n\n\nNormality of Residuals\nInspect the Histogram and Normal P-P plot in the output.\n\n\n\n\n\n\nImportantQuestion 9\n\n\n\nDo the histogram and Normal P-P plot suggest that the residuals are reasonably normally distributed?\n\n\n\n\nHomoscedasticity and Linearity\nLook at the standardized residuals versus standardized predicted values scatterplot.\n\n\n\n\n\n\nImportantQuestion 10\n\n\n\nDoes the plot suggest constant variance?\n\n\n\n\n\n\n\n\nImportantQuestion 11\n\n\n\nIs there any strong curvature or systematic pattern that would indicate the model is misspecified (i.e., not truly linear)?"
  },
  {
    "objectID": "SPSS_lab6.html#part-2-ancova-analysis-of-covariance",
    "href": "SPSS_lab6.html#part-2-ancova-analysis-of-covariance",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 2: ANCOVA (Analysis of Covariance)",
    "text": "Part 2: ANCOVA (Analysis of Covariance)\nIn this section, we will fit an ANCOVA model to determine whether alcohol consumption is associated with pocket depth, controlling for age.\n\nExploratory Data Analysis\nWe start by creating a scatterplot to visualize the relationship between age and pocketdepth, using different colors to represent the levels of alcohol:\n\nGo to Graphs → Legacy Dialogs → Scatter/Dot.\n\nChoose Simple Scatter and click Define.\n\nPlace age on the X Axis, pocketdepth on the Y Axis, and alcohol in Set Markers by.\n\nClick OK to generate the scatterplot.\n\n\n\n\n\n\n\nImportantQuestion 12\n\n\n\nWhat can you infer from the scatterplot about the relationship between age, pocketdepth, and alcohol consumption?\n\n\n\n\nFitting the ANCOVA Model\nIn SPSS, there are multiple ways to fit linear regression models. If all your explanatory variables are continuous, you can use Analyze → Regression → Linear. However, if your model includes categorical predictors, it is often more convenient to use Analyze → General Linear Model, as SPSS will automatically handle the dummy coding for categorical variables in that procedure.\nIn this case, we have both continuous (age) and categorical (alcohol) explanatory variables, so we will use the General Linear Model procedure.\n\nGo to Analyze → General Linear Model → Univariate.\n\nPlace pocketdepth in the Dependent Variable box.\n\nPlace age under Covariate(s).\n\nPlace alcohol under Fixed Factor(s).\n\nClick Options, and select Parameter estimates.\nClick OK to run the analysis.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that SPSS uses the last category (highest numerical code) as the default reference category in the General Linear Model procedure. In this dataset, “&gt;2 glasses/day” is the reference category.\n\n\n\n\n\n\n\n\nImportantQuestion 13\n\n\n\nFrom the table with the estimated regression coefficients, what is the estimated difference in pocket depth between individuals who consume \"None\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\n\n\n\n\n\n\nImportantQuestion 14\n\n\n\nFrom the same table, what is the estimated difference in pocket depth between individuals who consume \"1–2 glasses/day\" and those who consume \"&gt;2 glasses/day\", while controlling for age?\n\n\nTo test the overall significance of the alcohol variable as a predictor of pocketdepth, we construct an analysis of variance (ANOVA) table. The ANOVA table summarizes how much each term in a linear regression model contributes to explaining the overall variation in the response variable. There are different ways to construct this table depending on how the sum of squares is partitioned among model terms. A common approach is Type III ANOVA, which evaluates each variable or interaction after all other terms have been accounted for. Each effect is tested as if it were entered last, so its sum of squares reflects the unique contribution of that variable or interaction beyond what is already explained by the remaining terms.\nIn SPSS, the type III ANOVA table is automatically generated when you fit a linear model using the General Linear Model procedure. This table is displayed under the heading Tests of Between-Subjects Effects in the output.\n\n\n\n\n\n\nImportantQuestion 15\n\n\n\nBased on the ANOVA table, is there a significant association between alcohol consumption and pocketdepth after accounting for age?\n\n\n\n\nModel Diagnostics\nSimilar to the simple regression case, you can request certain diagnostic plots in the Options menu of the Univariate dialog. However, the options are limited compared to the linear regression dialog. If you want more control over the diagnostic plots, you can save the residuals and predicted values to your dataset and then create the plots manually:\n\nIn the Univariate dialog, click Save.\nSelect Unstandardized predicted values and Standardized residuals.\nClick Continue to go back to the main dialog, and Click OK to run the analysis.\nAfter the analysis finishes, go to Graphs → Legacy Dialogs → Scatter/Dot (or Histogram) to plot the new residual and predicted-value columns, explore their relationship, or check for normality.\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nCheck the normality of residuals and homoscedasticity assumptions for the ANCOVA model. Do you see any notable violations?"
  },
  {
    "objectID": "SPSS_lab6.html#part-3-interactions-in-ancova",
    "href": "SPSS_lab6.html#part-3-interactions-in-ancova",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 3: Interactions in ANCOVA",
    "text": "Part 3: Interactions in ANCOVA\nIn some cases, the relationship between the outcome variable and a predictor may depend on the level of another predictor. This is known as an interaction effect. In the context of ANCOVA, we can test for interactions between the continuous predictor (age) and the categorical predictor (alcohol).\n\nFitting the Interaction Model\n\nGo to Analyze → General Linear Model → Univariate.\n\nPlace pocketdepth in the Dependent Variable box.\n\nPlace age under Covariate(s).\n\nPlace alcohol under Fixed Factor(s).\n\nClick Model, and select Build terms under Specify Model.\nUnder Build Terms, set the type to Main effects.\nIn the Factors & Covariates box, select alcohol and age and move the two variables to the Model box.\nUnder Build Terms, set the type to Interaction.\nIn the Factors & Covariates box, select alcohol and age and move the two variables to the Model box.\nClick Continue to go back to the main dialog.\nClick OK to run the analysis.\n\n\n\n\n\n\n\nImportantQuestion 16\n\n\n\nBased on the output in the ANOVA table, is there a significant interaction between age and alcohol in predicting pocketdepth?"
  },
  {
    "objectID": "SPSS_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "href": "SPSS_lab6.html#part-4-relationship-between-smoking-and-pocket-depth",
    "title": "Medical Statistics – Lab 6",
    "section": "Part 4: Relationship Between Smoking and Pocket Depth",
    "text": "Part 4: Relationship Between Smoking and Pocket Depth\nIn addition to information about alcohol consumption, the dataset also contains information about smoking habits. Explore the relationship between smoking and pocket depth, and how it interacts with age. You can use the same approach as in the previous sections to fit models, test for significance, and check assumptions."
  },
  {
    "objectID": "SPSS_lab8.html",
    "href": "SPSS_lab8.html",
    "title": "Medical Statistics – Lab 8",
    "section": "",
    "text": "Welcome to lab 8. In this lab, we will build prediction models using backward elimination and automated procedures, and we will practice reasoning with causal diagrams (DAGs)."
  },
  {
    "objectID": "SPSS_lab8.html#part-1-building-prediction-models-using-backward-elimination",
    "href": "SPSS_lab8.html#part-1-building-prediction-models-using-backward-elimination",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 1: Building prediction models using backward elimination",
    "text": "Part 1: Building prediction models using backward elimination\nIn this part of the lab, we will build a prediction model for hospital length of stay (los) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\nKey variables in the dataset include:\n\nlos: Length of hospital stay (days, continuous outcome)\nage: Age at hospital admission (years)\ngender: Gender (0 = Male, 1 = Female)\nhr: Initial heart rate (beats per minute)\nsysbp and diasbp: Initial systolic and diastolic blood pressure (mmHg)\nbmi: Body mass index (kg/m^2)\ncvd: Presence of cardiovascular disease (0 = No, 1 = Yes)\nsho: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n\nStep 1: Fit the initial linear regression model\nDownload the dataset from the Datasets menu (whas500.sav) and open it in SPSS.\nCreate an initial model for hospital length of stay (los) using the following predictors: age, gender, hr, sysbp, diasbp, bmi, cvd, sho. Run/summarize the model to inspect coefficients and p-values.\nUse the General Linear Model procedure: Analyze → General Linear Model → Univariate.\n\n\nStep 2: Eliminate the least significant predictor\nTo identify the least significant predictor, we use the Type III ANOVA table:\n\nSignificance threshold: \\(p &gt; 0.10\\)\nRemove the predictor with the largest p-value above this threshold.\n\nThe Type III ANOVA table is generated in the General Linear Model output.\n\n\nStep 3: Repeat the steps\nIteratively remove the least significant predictor until all predictors have \\(p &lt; 0.10\\). At each step:\n\nRerun the regression model\nGenerate the Type III ANOVA table\nRemove the least significant predictor\n\n\n\nStep 4: Final model\nPresent the final linear regression model:\n\nReport the final model, including regression coefficients and 95% CIs.\nCreate residual plots to assess the model assumptions (normality, homoscedasticity, linearity).\n\n\n\n\n\n\n\nTip95% CIs for regression coefficients\n\n\n\nIn Analyze → General Linear Model → Univariate, click Options and request Parameter estimates (95% CIs are included automatically)."
  },
  {
    "objectID": "SPSS_lab8.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "href": "SPSS_lab8.html#part-2-automated-procedures-for-building-prediction-models-logistic-regression",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 2: Automated procedures for building prediction models (logistic regression)",
    "text": "Part 2: Automated procedures for building prediction models (logistic regression)\nIn this part, we explore automated procedures for predictor selection in logistic regression prediction models. We use the same WHAS dataset but now focus on predicting in-hospital death (dstat: alive/dead) from candidate predictors.\n\nSPSS: Backward selection (LR tests)\n\nGo to Analyze → Regression → Binary Logistic.\nPut dstat in Dependent.\nAdd your continuous candidate predictors (e.g., age, hr, sysbp, diasbp, bmi) to Covariates.\nAlso add any categorical candidate predictors (e.g., gender, cvd, sho) to Covariates, then click Categorical… and move them into Categorical Covariates so SPSS treats them as factors.\nUnder Method, select Backward: LR (backward selection based on likelihood ratio tests).\nClick Options… and tick CI for exp(B) to obtain 95% CIs for the odds ratios.\n\nNote (categorical predictors): Make sure you correctly specify categorical predictors via Categorical… (otherwise SPSS may treat their numeric codes as continuous). If you include categorical predictors with more than two categories and specify them via Categorical…, SPSS will handle the dummy coding internally and the Backward: LR procedure evaluates the predictor as a factor (i.e., removal is based on the overall significance of the factor, not separately for each dummy coefficient).\n\n\n\n\n\n\nNoteRelated note (linear regression)\n\n\n\nFor stepwise selection with a continuous outcome, SPSS implements this under Analyze → Regression → Linear (Method: Forward/Backward/Stepwise), but this procedure does not have a dedicated “factors” box—categorical predictors must be dummy-coded manually. In that situation, it is possible for some dummy variables to enter/leave the model while others do not. The General Linear Model → Univariate procedure does not offer the same automated stepwise selection.\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nInspect your final selected logistic regression model. Which predictors are retained in the prediction model?"
  },
  {
    "objectID": "SPSS_lab8.html#part-3-causal-diagrams",
    "href": "SPSS_lab8.html#part-3-causal-diagrams",
    "title": "Medical Statistics – Lab 8",
    "section": "Part 3: Causal diagrams",
    "text": "Part 3: Causal diagrams\nFor each of the exercises below:\n\nTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\nCheck your answer using the DAGitty webtool\n\n\nExercise 1\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n\n\nDAG exercise 1\n\n\n\n\nExercise 2\nIn the graph depicted below, what happens when you additionally adjust for v5?\n\n\n\nDAG exercise 2\n\n\n\n\nExercise 3\nThis diagram is slightly different: v1 now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of v1 on O?\n\n\n\nDAG exercise 3\n\n\n\n\nExercise 4\nNow, v2 is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of v2 on O?\n\n\n\nDAG exercise 4\n\n\n\n\nExercise 5\nBack to the first DAG. However, v2 is now unmeasured. Can we still obtain an unconfounded estimate of the effect of E on O?\n\n\n\nDAG exercise 5\n\n\n\n\nExercise 6\nSee the DAG below: you adjusted for v5. What would be the consequence of this action?\n\n\n\nDAG exercise 6"
  }
]