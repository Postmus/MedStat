{
  "hash": "da0d019f58e1350996228084a56c3fe6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Medical Statistics – Answers lab 8\"\nformat:\n  html:\n    toc: true       \n    toc-depth: 3    \n    toc-location: right\n    code-overflow: wrap \n  pdf:\n    toc: false\nexecute:\n  warning: false\n  message: false\n  error: false\n  eval: true\n  echo: false\n---\n\n## Part 1: Building prediction models using backward elimination\n\n### Step 1: Fit the initial linear regression model\n\n\n::: {.cell}\n\n:::\n\n\nCreate an initial model for hospital length of stay (`los`) using the following predictors: `age`, `gender`, `hr`, `sysbp`, `diasbp`, `bmi`, `cvd`, `sho`. Run/summarize the model to inspect coefficients and p-values.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = los ~ age + gender + hr + sysbp + diasbp + bmi + \n    cvd + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.335 -2.653 -1.071  1.200 40.073 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.2648144  2.1394896   1.526 0.127659    \nage           0.0031994  0.0168850   0.189 0.849792    \ngenderfemale  0.8575246  0.4489334   1.910 0.056698 .  \nhr            0.0190577  0.0090828   2.098 0.036396 *  \nsysbp        -0.0008358  0.0085656  -0.098 0.922304    \ndiasbp        0.0141799  0.0128884   1.100 0.271780    \nbmi          -0.0304532  0.0427613  -0.712 0.476700    \ncvdyes        0.3903925  0.4981468   0.784 0.433600    \nshoyes        3.5265170  1.0329265   3.414 0.000693 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.632 on 491 degrees of freedom\nMultiple R-squared:  0.04991,\tAdjusted R-squared:  0.03443 \nF-statistic: 3.224 on 8 and 491 DF,  p-value: 0.001388\n```\n\n\n:::\n:::\n\n\n### Step 2: Eliminate the least significant predictor\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nAnova Table (Type III tests)\n\nResponse: los\n             Sum Sq  Df F value    Pr(>F)    \n(Intercept)    50.0   1  2.3286 0.1276592    \nage             0.8   1  0.0359 0.8497924    \ngender         78.3   1  3.6486 0.0566978 .  \nhr             94.5   1  4.4025 0.0363962 *  \nsysbp           0.2   1  0.0095 0.9223042    \ndiasbp         26.0   1  1.2105 0.2717799    \nbmi            10.9   1  0.5072 0.4766997    \ncvd            13.2   1  0.6142 0.4336001    \nsho           250.1   1 11.6561 0.0006929 ***\nResiduals   10535.8 491                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe ANOVA table shows that the predictor with the highest p-value is `sysbp` ($p = 0.92$). Systolic blood pressure is the least significant predictor and should be removed from the model.\n\n### Step 3: Repeat the steps\n\nThe following variables are sequentially removed from the model (after initially removing `sysbp`):\n\n1. `age`: p-value = 0.86\n2. `cvd`: p-value = 0.41\n3. `bmi`: p-value = 0.44\n4. `diasbp`: p-value = 0.22\n\n### Step 4: Final model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model <- lm(los ~ gender + hr + sho, data = whas500)\nsummary(final_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = los ~ gender + hr + sho, data = whas500)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.663 -2.661 -1.064  1.136 40.826 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.796075   0.799047   4.751 2.66e-06 ***\ngenderfemale 0.910388   0.424827   2.143 0.032602 *  \nhr           0.020680   0.008843   2.339 0.019751 *  \nshoyes       3.550448   1.009081   3.518 0.000474 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.622 on 496 degrees of freedom\nMultiple R-squared:  0.04443,\tAdjusted R-squared:  0.03865 \nF-statistic: 7.687 on 3 and 496 DF,  p-value: 4.976e-05\n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% CIs for regression coefficients\nconfint(final_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   2.5 %     97.5 %\n(Intercept)  2.226140295 5.36601012\ngenderfemale 0.075705855 1.74507053\nhr           0.003306032 0.03805437\nshoyes       1.567847029 5.53304829\n```\n\n\n:::\n:::\n\n\nThe final model for hospital length of stay includes `gender`, `hr`, and `sho`. All predictors have $p < 0.10$. \n\n| Predictor | Coefficient | 95% CI | p-value |\n| :--- | :--- | :--- | :--- |\n| (Intercept) | 3.80 | [2.23, 5.37] | < 0.001 |\n| Gender (Female) | 0.91 | [0.08, 1.75] | 0.033 |\n| Heart rate | 0.02 | [0.003, 0.038] | 0.020 |\n| Cardiogenic shock | 3.55 | [1.57, 5.53] | < 0.001 |\n\n#### Residual plots\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab8_answers_files/figure-pdf/unnamed-chunk-5-1.pdf)\n:::\n\n::: {.cell-output-display}\n![](lab8_answers_files/figure-pdf/unnamed-chunk-5-2.pdf)\n:::\n:::\n\n  \nThe overall fit of the model appears reasonable, as the residuals are generally centered around zero with no major patterns suggesting severe violations of linearity. However, there are some outliers with very long lengths of stay (LOS) that are not adequately captured by the model. These outliers lead to a right skew in the residual distribution, as seen in the histogram, influencing model fit. While the current model seems to work reasonably well for most observations, further steps (e.g., transformations or robust regression techniques) could be considered to better account for these extreme cases.  \n\n## Part 2: Automated procedures for building prediction models (logistic regression)\n\nIn this part, we explore automated procedures for predictor selection in **logistic regression** prediction models, focusing on predicting in-hospital death (`dstat`).\n\n#### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 0/1 outcome (1 = dead)\nwhas500 <- whas500 |>\n  mutate(dstat01 = ifelse(dstat == \"dead\", 1, 0))\n\nfit_full <- glm(\n  dstat01 ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho,\n  family = binomial,\n  data = whas500\n)\n\nfit_step <- stepAIC(fit_full, direction = \"backward\", trace = FALSE)\nsummary(fit_step)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = dstat01 ~ age + hr + sysbp + sho, family = binomial, \n    data = whas500)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.186164   1.687679  -3.665 0.000247 ***\nage          0.059272   0.016933   3.500 0.000464 ***\nhr           0.013961   0.007497   1.862 0.062572 .  \nsysbp       -0.017333   0.006212  -2.790 0.005271 ** \nshoyes       3.061710   0.525719   5.824 5.75e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.86  on 499  degrees of freedom\nResidual deviance: 204.09  on 495  degrees of freedom\nAIC: 214.09\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Odds ratios and 95% CIs\nexp(cbind(OR = coef(fit_step), confint.default(fit_step)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      OR        2.5 %      97.5 %\n(Intercept)  0.002057705 7.530576e-05  0.05622609\nage          1.061063592 1.026428e+00  1.09686811\nhr           1.014058710 9.992675e-01  1.02906886\nsysbp        0.982816468 9.709221e-01  0.99485659\nshoyes      21.364054186 7.624145e+00 59.86543899\n```\n\n\n:::\n:::\n\n\n**Question:** Which predictors are retained in the prediction model?\n\n**Answer:**  \nThe final logistic regression model obtained via backward elimination using AIC includes the following predictors: **age**, **hr** (heart rate), **sysbp** (systolic blood pressure), and **sho** (cardiogenic shock).\n\n#### SPSS\n\nIn SPSS, using the **Backward: LR** method (standard settings), the procedure also yields a final model with these four predictors.\n\n![SPSS results of the backward selection for logistic regression](images/lab7_SPSS_BW_logistic.png)\n\n## Part 3: Causal diagrams\n\nFor each of the exercises below: \n\n-\tTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\n-\tCheck your answer using the [DAGitty webtool](http://www.dagitty.net/dags.html)\n\n### Exercise 1\n\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n![DAG exercise 1](images/lab7_DAG1.PNG)\n\n**Answer:**  \nFollowing the recipe: after removing all arrows leaving E, there are several unblocked paths leading from E to O. Just like in the lecture, adjusting for v2 opens a backdoor path (E – v1 – v3 – O) This newly opened backdoor path needs to be closed by also conditioning on v1 or v3, or both. \nHence, there are 3 options: (v1, v2, v3) ; (v1, v2) ; and finally, (v2, v3).\n\n### Exercise 2\n\nIn the graph depicted below, what happens when you additionally adjust for **v5**?\n\n![DAG exercise 2](images/lab7_DAG2.PNG)\n\n**Answer:**\nWhen adjusting for v5, we are blocking the effect through this indirect path from E to O (v5 is a mediator between E and O). Instead of the total effect of E on O, we will be estimating the direct effect. \n\nIn DAGitty, when you set v5 to ‘adjusted’, the algorithm will say the following:  “The total effect cannot be estimated due to adjustment for an intermediate or a descendant of an intermediate.”\n\n### Exercise 3\n\nThis diagram is slightly different: **v1** now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of **v1** on **O**?\n\n![DAG exercise 3](images/lab7_DAG3.PNG)\n\n**Answer:**\nNo adjustment is needed: there are no backdoor paths (removing all arrows leaving v1 reveals no remaining unblocked path from v1 to O).\n\n### Exercise 4\n\nNow, **v2** is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of **v2** on **O**?\n\n![DAG exercise 4](images/lab7_DAG4.PNG)\n\n**Answer:**\nFollowing the recipe, there are three unblocked paths left after removing the arrows leaving v2: \n\na) v2 – v3 – O and \nb) v2 – v1 – E – O \nc) v2  - v1 – E -v5 - O\n\nBackdoor path a) can be closed by conditioning on v3. \n\nBackdoor path b) can be closed by conditioning on v1 (but not by conditioning on E, as you would no longer be estimating the total effect by blocking the paths from v2 to O mediated by E).\n\nIn this case, you should therefore condition on v1 and v3.\n\n### Exercise 5\n\nBack to the first DAG. However, **v2** is now unmeasured. Can we still obtain an unconfounded estimate of the effect of **E** on **O**?\n\n![DAG exercise 5](images/lab7_DAG5.PNG)\n\n**Answer:**\nNo, we cannot close the backdoor path between E and O since v2 is unmeasured and cannot be corrected for.\n\n### Exercise 6\n\nSee the DAG below: you adjusted for **v5**. What would be the consequence of this action?\n\n![DAG exercise 6](images/lab7_DAG6.PNG)\n\n**Answer:**\nThere is no consequence: conditioning on v5 cannot alter any of the estimated effects in the DAG (it is neither a confounder, collider, nor a mediator in the E-O relationship).\n",
    "supporting": [
      "lab8_answers_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}