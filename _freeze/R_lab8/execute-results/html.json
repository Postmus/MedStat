{
  "hash": "7e856e1bb3f214ec560f41c5c456e317",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Medical Statistics â€“ Lab 8\"\nsubtitle: R version\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-location: right\n    code-overflow: wrap\n  pdf:\n    toc: false\nexecute:\n  warning: false\n  message: false\n  error: false\n  eval: false\n---\n\nWelcome to lab 8. In this lab, we will build prediction models using backward elimination and automated procedures, and we will practice reasoning with causal diagrams (DAGs).\n\nWe will use the Worcester Heart Attack Study dataset (`whas500.sav`, available from the Datasets menu) and the following R packages: `haven`, `dplyr`, `ggplot2`, `car`, and `MASS`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(haven)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(MASS)\n\nwhas500 <- read_sav(\"whas500.sav\")\nwhas500 <- whas500 |> mutate(across(where(is.labelled), as_factor))\n```\n:::\n\n\n\n\n## Part 1: Building prediction models using backward elimination\n\nIn this part of the lab, we will build a prediction model for hospital length of stay (`los`) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\n\nKey variables in the dataset include:\n\n  - `los`: Length of hospital stay (days, continuous outcome)\n  - `age`: Age at hospital admission (years)\n  - `gender`: Gender (0 = Male, 1 = Female)\n  - `hr`: Initial heart rate (beats per minute)\n  - `sysbp` and `diasbp`: Initial systolic and diastolic blood pressure (mmHg)\n  - `bmi`: Body mass index (kg/m^2)\n  - `cvd`: Presence of cardiovascular disease (0 = No, 1 = Yes)\n  - `sho`: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n\n### Step 1: Fit the initial linear regression model\n\nFit an initial linear regression model for hospital length of stay (`los`) using `lm()` with predictors `age`, `gender`, `hr`, `sysbp`, `diasbp`, `bmi`, `cvd`, and `sho`. Summarize the model output to inspect coefficients and p-values.\n\n### Step 2: Eliminate the least significant predictor\n\nTo identify the least significant predictor, we use the Type III ANOVA table:\n\n* Significance threshold: $p > 0.10$\n* Remove the predictor with the largest p-value above this threshold.\n\nUse the `Anova()` function from the `car` package to obtain the Type III ANOVA table (see lab week 6).\n\n### Step 3: Repeat the steps\n\nIteratively remove the least significant predictor until all predictors have $p < 0.10$. At each step:\n\n* Rerun the regression model\n* Generate the Type III ANOVA table\n* Remove the least significant predictor\n\n### Step 4: Final model\n\nPresent the final linear regression model:\n\n* Report the final model, including regression coefficients and 95% CIs.\n* Create residual plots to assess the model assumptions (normality, homoscedasticity, linearity).\n\n::: {.callout-tip title=\"95% CIs for regression coefficients\"}\nTo obtain 95% CIs for the regression coefficients after fitting your final model (e.g., `fit <- lm(...)`), use:\n\n```r\nconfint(fit)\n```\n:::\n\n## Part 2: Automated procedures for building prediction models (logistic regression)\n\nIn this part, we explore automated procedures for predictor selection in **logistic regression** prediction models. We use the same WHAS dataset but now focus on predicting in-hospital death (`dstat`: alive/dead) from candidate predictors.\n\n### Automated model selection (AIC)\n\n`stepAIC()` from the `MASS` package can be used for automated selection based on AIC. Because AIC compares overall model fit (not individual p-values), the selected model may differ from manual backward elimination.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 0/1 outcome (1 = dead)\nwhas500 <- whas500 |>\n  mutate(dstat01 = ifelse(dstat == \"dead\", 1, 0))\n\nfit_full <- glm(\n  dstat01 ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho,\n  family = binomial,\n  data = whas500\n)\n\nfit_step <- stepAIC(fit_full, direction = \"backward\", trace = FALSE)\nsummary(fit_step)\n```\n:::\n\n\n::: {.callout-important icon=false title=\"Question\"}\nInspect your final selected logistic regression model. Which predictors are retained in the prediction model?\n:::\n\n::: {.callout-tip title=\"Reporting odds ratios (ORs) and 95% CIs\"}\nFor logistic regression, `summary()` reports regression coefficients on the **log-odds** scale. For reporting, it is often more useful to report **odds ratios (ORs)** with **95% confidence intervals (CIs)**.\n\nYou can compute these from the `summary()` output (Wald CI):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- summary(fit_step)$coefficients\n\nOR <- exp(s[, \"Estimate\"])\nCI_lower <- exp(s[, \"Estimate\"] - 1.96 * s[, \"Std. Error\"])\nCI_upper <- exp(s[, \"Estimate\"] + 1.96 * s[, \"Std. Error\"])\n\ncbind(OR = OR, CI_lower = CI_lower, CI_upper = CI_upper)\n```\n:::\n\n\nOr compute them more directly by exponentiating the coefficient confidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(cbind(OR = coef(fit_step), confint.default(fit_step)))\n```\n:::\n\n:::\n\n## Part 3: Causal diagrams\n\nFor each of the exercises below:\n\n-\tTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\n-\tCheck your answer using the [DAGitty webtool](http://www.dagitty.net/dags.html)\n\n### Exercise 1\n\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n![DAG exercise 1](images/lab7_DAG1.PNG)\n\n### Exercise 2\n\nIn the graph depicted below, what happens when you additionally adjust for **v5**?\n\n![DAG exercise 2](images/lab7_DAG2.PNG)\n\n### Exercise 3\n\nThis diagram is slightly different: **v1** now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of **v1** on **O**?\n\n![DAG exercise 3](images/lab7_DAG3.PNG)\n\n### Exercise 4\n\nNow, **v2** is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of **v2** on **O**?\n\n![DAG exercise 4](images/lab7_DAG4.PNG)\n\n### Exercise 5\n\nBack to the first DAG. However, **v2** is now unmeasured. Can we still obtain an unconfounded estimate of the effect of **E** on **O**?\n\n![DAG exercise 5](images/lab7_DAG5.PNG)\n\n### Exercise 6\n\nSee the DAG below: you adjusted for **v5**. What would be the consequence of this action?\n\n![DAG exercise 6](images/lab7_DAG6.PNG)\n",
    "supporting": [
      "R_lab8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}