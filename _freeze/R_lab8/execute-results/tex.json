{
  "hash": "bb1e4fed8cba022cfaf6ba6c555cc644",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Medical Statistics â€“ Lab 8\"\nsubtitle: R version\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-location: right\n    code-overflow: wrap\n  pdf:\n    toc: false\nexecute:\n  warning: false\n  message: false\n  error: false\n  eval: false\n---\n\n## Part 1: Building prediction models using backward elimination (linear regression)\n\nIn this part of the lab, we will build a prediction model for hospital length of stay (`los`) in patients with acute myocardial infarction. The dataset comes from the Worcester Heart Attack Study (WHAS) and includes data from 500 patients admitted in Worcester, Massachusetts in 1997, 1999, and 2001.\n\nKey variables in the dataset include:\n\n  - `los`: Length of hospital stay (days, continuous outcome)\n  - `age`: Age at hospital admission (years)\n  - `gender`: Gender (0 = Male, 1 = Female)\n  - `hr`: Initial heart rate (beats per minute)\n  - `sysbp` and `diasbp`: Initial systolic and diastolic blood pressure (mmHg)\n  - `bmi`: Body mass index (kg/m^2)\n  - `cvd`: Presence of cardiovascular disease (0 = No, 1 = Yes)\n  - `sho`: Presence of cardiogenic shock (0 = No, 1 = Yes)\n\n### Step 1: Fit the initial linear regression model\n\nDownload the dataset from the Datasets menu (`whas500.sav`) and open it in R. Make sure that the categorical variables are correctly coded as factors.\n\nCreate an initial model for hospital length of stay (`los`) using the following predictors: `age`, `gender`, `hr`, `sysbp`, `diasbp`, `bmi`, `cvd`, `sho`. Run/summarize the model to inspect coefficients and p-values.\n\n**R instructions:** use the `lm()` function to fit the model.\n\n### Step 2: Eliminate the least significant predictor\n\nTo identify the least significant predictor, use the Type III ANOVA table:\n\n* Significance threshold: $p > 0.10$\n* Remove the predictor with the largest p-value above this threshold.\n\n**R instructions:** use the `Anova()` function from the `car` package to obtain the Type III ANOVA table (see previous lab).\n\n### Step 3: Repeat the steps\n\nIteratively remove the least significant predictor until all predictors have $p < 0.10$. At each step:\n\n* Rerun the regression model\n* Generate the Type III ANOVA table\n* Remove the least significant predictor\n\n### Step 4: Final model\n\nPresent the final linear regression model:\n\n* Summarize the remaining predictors and their coefficients\n* Report 95% confidence intervals (CIs) for the regression coefficients\n* Discuss how each variable contributes to predicting hospital length of stay\n\nCreate residual plots to assess the model assumptions (normality, homoscedasticity, linearity).\n\n**R tip (95% CIs):** after fitting your final model (e.g., `fit <- lm(...)`), use:\n\n```r\nconfint(fit)\n```\n\n## Part 2: Automated procedures for building prediction models (logistic regression)\n\nIn this part, we explore automated procedures for predictor selection in **logistic regression** prediction models. We use the same WHAS dataset but now focus on predicting in-hospital death (`dstat`: alive/dead) from candidate predictors.\n\n### R: Automated model selection (AIC)\n\nIn R, the `stepAIC()` function from the `MASS` package allows for automated selection based on AIC (Akaike Information Criterion). Backward, forward, or stepwise selection can be specified using the `direction` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(haven)\nlibrary(dplyr)\nlibrary(MASS)\n\nwhas500 <- read_sav(\"datasets/whas500.sav\")\nwhas500 <- whas500 |> mutate(across(where(is.labelled), as_factor))\n\n# Create a 0/1 outcome (1 = dead)\nwhas500 <- whas500 |>\n  mutate(dstat01 = ifelse(dstat == \"dead\", 1, 0))\n\nfit_full <- glm(\n  dstat01 ~ age + gender + hr + sysbp + diasbp + bmi + cvd + sho,\n  family = binomial,\n  data = whas500\n)\n\nfit_step <- stepAIC(fit_full, direction = \"backward\", trace = FALSE)\nsummary(fit_step)\n```\n:::\n\n\n### R: Odds ratios and 95% CIs\n\nTo report odds ratios with 95% confidence intervals, exponentiate the coefficient estimates and their confidence intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(cbind(OR = coef(fit_step), confint.default(fit_step)))\n```\n:::\n\n\n::: {.callout-important icon=false title=\"Question\"}\nInspect your final selected logistic regression model. Which predictors are retained in the prediction model?\n:::\n\n## Part 3: Causal diagrams\n\nFor each of the exercises below:\n\n-\tTry solving the diagrams by hand by using the recipe from the lecture (see lecture slides on Brightspace)\n-\tCheck your answer using the [DAGitty webtool](http://www.dagitty.net/dags.html)\n\n### Exercise 1\n\nIn the graph depicted below, for which variables do you need to adjust to assess the unconfounded effect of E on O (there may be several possibilities)?\n\n![DAG exercise 1](images/lab7_DAG1.PNG)\n\n### Exercise 2\n\nIn the graph depicted below, what happens when you additionally adjust for **v5**?\n\n![DAG exercise 2](images/lab7_DAG2.PNG)\n\n### Exercise 3\n\nThis diagram is slightly different: **v1** now is the exposure. For which variables do you need toadjust to assess the unconfounded effect of **v1** on **O**?\n\n![DAG exercise 3](images/lab7_DAG3.PNG)\n\n### Exercise 4\n\nNow, **v2** is the exposure. For which variables do you need to adjust to assess the total unconfounded effect of **v2** on **O**?\n\n![DAG exercise 4](images/lab7_DAG4.PNG)\n\n### Exercise 5\n\nBack to the first DAG. However, **v2** is now unmeasured. Can we still obtain an unconfounded estimate of the effect of **E** on **O**?\n\n![DAG exercise 5](images/lab7_DAG5.PNG)\n\n### Exercise 6\n\nSee the DAG below: you adjusted for **v5**. What would be the consequence of this action?\n\n![DAG exercise 6](images/lab7_DAG6.PNG)\n\n",
    "supporting": [
      "R_lab8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}